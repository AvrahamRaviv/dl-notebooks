{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Attention Mechanism\n",
    "\n",
    "This notebook covers **Chapter 8** of the Deep Learning in Hebrew book, focusing on the **Attention Mechanism** - one of the most revolutionary concepts in deep learning that has transformed natural language processing and many other domains.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **Attention Mechanism** is a fundamental breakthrough that allows models to focus on relevant parts of the input when producing each part of the output. Unlike traditional sequence-to-sequence models that compress all information into a single fixed-size vector, attention enables dynamic alignment between input and output sequences.\n",
    "\n",
    "The attention mechanism was first introduced in the context of sequence-to-sequence models to address the bottleneck problem of encoding long sequences. However, its true power was revealed in 2017 with the paper **\"Attention is All You Need\"**, which introduced the **Transformer** architecture - a model that uses attention mechanisms exclusively, without any recurrent or convolutional layers.\n",
    "\n",
    "This chapter will take you through:\n",
    "1. **Sequence-to-Sequence Learning with Attention** - How attention solves the bottleneck problem\n",
    "2. **Bahdanau and Luong Attention** - Early attention mechanisms\n",
    "3. **Transformer Architecture** - The revolutionary attention-only model\n",
    "4. **Self-Attention** - Attention within a single sequence\n",
    "5. **Multi-Head Attention** - Parallel attention mechanisms\n",
    "6. **Positional Encoding** - Representing sequence order without recurrence\n",
    "7. **Transformer Applications** - BERT, GPT, and more\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 8.1 Sequence to Sequence Learning and Attention\n",
    "- 8.1.1 [Attention in Seq2Seq Models](#811-attention-in-seq2seq-models)\n",
    "- 8.1.2 [Bahdanau Attention and Luong Attention](#812-bahdanau-attention-and-luong-attention)\n",
    "\n",
    "### 8.2 Transformer\n",
    "- 8.2.1 [Positional Encoding](#821-positional-encoding)\n",
    "- 8.2.2 [Self-Attention Layer](#822-self-attention-layer)\n",
    "- 8.2.3 [Multi-Head Attention](#823-multi-head-attention)\n",
    "- 8.2.4 [Transformer End to End](#824-transformer-end-to-end)\n",
    "- 8.2.5 [Transformer Applications](#825-transformer-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Sequence to Sequence Learning and Attention\n",
    "\n",
    "## 8.1.1 Attention in Seq2Seq Models\n",
    "\n",
    "Traditional sequence-to-sequence (Seq2Seq) models, as we saw in Chapter 6, use RNNs with encoder-decoder architecture. The input sequence passes through an **encoder** that creates a vector of known size representing the original sequence, including the order of elements and relationships between them. Then this vector passes through a **decoder** to decode the information in the vector and generate output in another domain.\n",
    "\n",
    "For example, a Seq2Seq model can encode a sentence in one language into a certain vector and then decode the vector into a sentence in a second language.\n",
    "\n",
    "### The Bottleneck Problem\n",
    "\n",
    "The conventional way to create the vector and decode it was using different architectures of RNNs, such as deep networks containing memory components like LSTM and GRU. These models encountered a problem with long sequences, because the vector is limited in its ability to contain relationships between many elements.\n",
    "\n",
    "The problem is that **all information from the input sequence must be compressed into a single fixed-size vector**. This creates a bottleneck:\n",
    "- For short sequences, the vector may be sufficient\n",
    "- For long sequences, information gets lost or diluted\n",
    "- The decoder must generate the entire output from this single vector\n",
    "\n",
    "### The Attention Solution\n",
    "\n",
    "To deal with this problem, we can use a different approach - instead of creating a vector at the output of the encoder, we can use the hidden states of the encoder in combination with the hidden states of the decoder, and thus find dependencies between elements of the input sequence and elements of the output sequence (**general attention**) and relationships between elements of the input sequence itself (**self-attention**).\n",
    "\n",
    "### How Attention Works\n",
    "\n",
    "For example, in translation of the sentence \"How was your day\" to another language - in this case, the attention mechanism generates a new vector for each word in the output sequence, where each vector quantifies how much the current word in the output is related to each of the words in the original sentence.\n",
    "\n",
    "In this way, each element of the output sequence can access each element of the input sequence. This mechanism is called **attention**.\n",
    "\n",
    "### Key Benefits of Attention\n",
    "\n",
    "1. **Dynamic Alignment**: Each output element can focus on different parts of the input\n",
    "2. **No Information Bottleneck**: Direct access to all encoder hidden states\n",
    "3. **Interpretability**: Attention weights show which input parts are important\n",
    "4. **Handles Long Sequences**: No need to compress everything into one vector\n",
    "\n",
    "### Attention Mechanism Overview\n",
    "\n",
    "The attention mechanism computes a **context vector** for each output time step by:\n",
    "1. Computing **alignment scores** between the current decoder state and all encoder states\n",
    "2. Converting scores to **attention weights** using softmax\n",
    "3. Computing a **weighted sum** of encoder states (context vector)\n",
    "4. Using the context vector along with the decoder state to generate output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize Attention Mechanism Concept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Traditional Seq2Seq (without attention)\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-0.5, 6)\n",
    "ax1.set_ylim(-0.5, 4)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Traditional Seq2Seq (No Attention)', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "input_words = ['How', 'was', 'your', 'day']\n",
    "for i, word in enumerate(input_words):\n",
    "    ax1.text(0, 3-i*0.8, word, fontsize=11, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "    if i < len(input_words) - 1:\n",
    "        ax1.arrow(0.3, 3-i*0.8, 0.2, -0.4, head_width=0.08, head_length=0.06, fc='black', ec='black')\n",
    "\n",
    "# Encoder\n",
    "ax1.text(2, 2, 'Encoder\\n(LSTM/GRU)', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(0.8, 2, 0.8, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Bottleneck vector\n",
    "ax1.text(3.5, 2, 'Context\\nVector', fontsize=10, weight='bold', ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "ax1.text(3.5, 1.2, '(Fixed Size)', fontsize=8, ha='center', style='italic', color='red')\n",
    "\n",
    "# Decoder\n",
    "ax1.text(5, 2, 'Decoder\\n(LSTM/GRU)', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(4.2, 2, 0.5, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Output sequence\n",
    "output_words = ['Comment', 'était', 'ta', 'journée']\n",
    "for i, word in enumerate(output_words):\n",
    "    ax1.text(5, 0.5-i*0.3, word, fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "    if i < len(output_words) - 1:\n",
    "        ax1.arrow(5, 0.5-i*0.3-0.15, 0, -0.15, head_width=0.1, head_length=0.05, fc='black', ec='black')\n",
    "\n",
    "# Problem annotation\n",
    "ax1.annotate('Information\\nBottleneck!', xy=(3.5, 1.2), xytext=(3.5, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, weight='bold', color='red', ha='center')\n",
    "\n",
    "# Seq2Seq with Attention\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-0.5, 7)\n",
    "ax2.set_ylim(-0.5, 4.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Seq2Seq with Attention', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "for i, word in enumerate(input_words):\n",
    "    ax2.text(0, 3.5-i*0.7, word, fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Encoder with hidden states\n",
    "ax2.text(2, 3.5, 'Encoder', fontsize=10, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "for i in range(len(input_words)):\n",
    "    ax2.text(2, 2.8-i*0.7, f'h_{i+1}', fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', alpha=0.7))\n",
    "\n",
    "# Attention mechanism\n",
    "ax2.text(4, 3.5, 'Attention', fontsize=10, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "ax2.text(4, 2.8, 'Context', fontsize=9, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', edgecolor='black'))\n",
    "ax2.text(4, 2.1, 'Vector', fontsize=9, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', edgecolor='black'))\n",
    "\n",
    "# Attention connections (example for first output word)\n",
    "for i in range(len(input_words)):\n",
    "    # Connection from encoder hidden state to attention\n",
    "    ax2.plot([2.3, 3.5], [2.8-i*0.7, 2.45], 'b-', linewidth=1.5, alpha=0.4)\n",
    "    # Connection from attention to decoder\n",
    "    ax2.plot([4.5, 5.2], [2.45, 2.1], 'b-', linewidth=1.5, alpha=0.4)\n",
    "\n",
    "# Decoder\n",
    "ax2.text(6, 2.1, 'Decoder', fontsize=10, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Output sequence\n",
    "for i, word in enumerate(output_words):\n",
    "    ax2.text(6, 0.5-i*0.3, word, fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "\n",
    "# Annotation\n",
    "ax2.text(3.5, 0.2, 'Dynamic context for each output!', fontsize=9, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='green', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Differences:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Traditional Seq2Seq:\")\n",
    "print(\"  - Single fixed-size context vector\")\n",
    "print(\"  - Information bottleneck for long sequences\")\n",
    "print(\"  - All input information compressed into one vector\")\n",
    "print(\"\\nSeq2Seq with Attention:\")\n",
    "print(\"  - Dynamic context vector for each output step\")\n",
    "print(\"  - Direct access to all encoder hidden states\")\n",
    "print(\"  - Attention weights show which inputs are important\")\n",
    "print(\"  - No information bottleneck\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bahdanau Attention Implementation\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau (Additive) Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention weights\n",
    "        self.W_encoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_decoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Compute attention weights and context vector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        decoder_hidden : tensor, shape (batch, hidden_size)\n",
    "            Previous decoder hidden state\n",
    "        encoder_outputs : tensor, shape (seq_len, batch, hidden_size)\n",
    "            All encoder hidden states\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        context : tensor, shape (batch, hidden_size)\n",
    "            Context vector\n",
    "        attention_weights : tensor, shape (batch, seq_len)\n",
    "            Attention weights for visualization\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = encoder_outputs.shape\n",
    "        \n",
    "        # Reshape for computation\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)  # (1, batch, hidden_size)\n",
    "        \n",
    "        # Compute alignment scores\n",
    "        # score = v^T * tanh(W_e * h_e + W_d * h_d)\n",
    "        encoder_proj = self.W_encoder(encoder_outputs)  # (seq_len, batch, hidden_size)\n",
    "        decoder_proj = self.W_decoder(decoder_hidden)  # (1, batch, hidden_size)\n",
    "        \n",
    "        # Add and apply tanh\n",
    "        combined = torch.tanh(encoder_proj + decoder_proj)  # (seq_len, batch, hidden_size)\n",
    "        \n",
    "        # Compute scores\n",
    "        scores = self.v(combined).squeeze(-1)  # (seq_len, batch)\n",
    "        scores = scores.transpose(0, 1)  # (batch, seq_len)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)  # (batch, seq_len)\n",
    "        \n",
    "        # Compute context vector as weighted sum\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # (batch, seq_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden_size)\n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Luong Attention Implementation\n",
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Luong (Multiplicative) Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, method='general'):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.method = method\n",
    "        \n",
    "        if method == 'general':\n",
    "            self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        elif method == 'concat':\n",
    "            self.W = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n",
    "            self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Compute attention weights and context vector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        decoder_hidden : tensor, shape (batch, hidden_size)\n",
    "            Current decoder hidden state\n",
    "        encoder_outputs : tensor, shape (seq_len, batch, hidden_size)\n",
    "            All encoder hidden states\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        context : tensor, shape (batch, hidden_size)\n",
    "            Context vector\n",
    "        attention_weights : tensor, shape (batch, seq_len)\n",
    "            Attention weights\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = encoder_outputs.shape\n",
    "        \n",
    "        # Transpose encoder_outputs once at the beginning: (seq_len, batch, hidden_size) -> (batch, seq_len, hidden_size)\n",
    "        encoder_outputs_batch = encoder_outputs.transpose(0, 1)  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            # Dot product: score = h_t^T * h_s\n",
    "            decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "            scores = torch.bmm(decoder_hidden, encoder_outputs_batch.transpose(1, 2))  # (batch, 1, seq_len)\n",
    "            scores = scores.squeeze(1)  # (batch, seq_len)\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # General: score = h_t^T * W * h_s\n",
    "            decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "            encoder_proj = self.W(encoder_outputs_batch)  # (batch, seq_len, hidden_size)\n",
    "            scores = torch.bmm(decoder_hidden, encoder_proj.transpose(1, 2))  # (batch, 1, seq_len)\n",
    "            scores = scores.squeeze(1)  # (batch, seq_len)\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            # Concat: score = v^T * tanh(W * [h_t; h_s])\n",
    "            decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)  # (batch, seq_len, hidden_size)\n",
    "            combined = torch.cat([decoder_hidden, encoder_outputs_batch], dim=2)  # (batch, seq_len, 2*hidden_size)\n",
    "            scores = self.v(torch.tanh(self.W(combined)))  # (batch, seq_len, 1)\n",
    "            scores = scores.squeeze(2)  # (batch, seq_len)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)  # (batch, seq_len)\n",
    "        \n",
    "        # Compute context vector (encoder_outputs_batch is already in correct shape: batch, seq_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs_batch)  # (batch, 1, hidden_size)\n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Test both attention mechanisms\n",
    "hidden_size = 64\n",
    "seq_len = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Create dummy encoder outputs and decoder hidden state\n",
    "encoder_outputs = torch.randn(seq_len, batch_size, hidden_size)\n",
    "decoder_hidden_bahdanau = torch.randn(batch_size, hidden_size)\n",
    "decoder_hidden_luong = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "# Test Bahdanau Attention\n",
    "bahdanau_attn = BahdanauAttention(hidden_size)\n",
    "context_bahdanau, weights_bahdanau = bahdanau_attn(decoder_hidden_bahdanau, encoder_outputs)\n",
    "\n",
    "# Test Luong Attention (general method)\n",
    "luong_attn = LuongAttention(hidden_size, method='general')\n",
    "context_luong, weights_luong = luong_attn(decoder_hidden_luong, encoder_outputs)\n",
    "\n",
    "print(\"Attention Mechanisms Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"\\nBahdanau Attention:\")\n",
    "print(f\"  Context shape: {context_bahdanau.shape}\")\n",
    "print(f\"  Attention weights shape: {weights_bahdanau.shape}\")\n",
    "print(f\"  Attention weights sum (should be 1.0): {weights_bahdanau.sum(dim=1)}\")\n",
    "print(f\"\\nLuong Attention (general):\")\n",
    "print(f\"  Context shape: {context_luong.shape}\")\n",
    "print(f\"  Attention weights shape: {weights_luong.shape}\")\n",
    "print(f\"  Attention weights sum (should be 1.0): {weights_luong.sum(dim=1)}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bahdanau attention weights\n",
    "im1 = axes[0].imshow(weights_bahdanau.detach().numpy(), cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Bahdanau Attention Weights', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Encoder Position', fontsize=11)\n",
    "axes[0].set_ylabel('Batch Item', fontsize=11)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Luong attention weights\n",
    "im2 = axes[1].imshow(weights_luong.detach().numpy(), cmap='Reds', aspect='auto')\n",
    "axes[1].set_title('Luong Attention Weights', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Encoder Position', fontsize=11)\n",
    "axes[1].set_ylabel('Batch Item', fontsize=11)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Bahdanau (Additive):\")\n",
    "print(\"  - Uses previous decoder hidden state h_{t-1}\")\n",
    "print(\"  - Score: v^T * tanh(W_e*h_e + W_d*h_{t-1})\")\n",
    "print(\"  - More parameters, more expressive\")\n",
    "print(\"\\nLuong (Multiplicative):\")\n",
    "print(\"  - Uses current decoder hidden state h_t\")\n",
    "print(\"  - Score: h_t^T * W * h_s (general) or h_t^T * h_s (dot)\")\n",
    "print(\"  - Fewer parameters, more efficient\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Transformer\n",
    "\n",
    "After the attention mechanism began to gain momentum, an architecture based on attention was invented that does not use memory components at all. This architecture is called **Transformer**.\n",
    "\n",
    "The Transformer offers two new elements in order to find relationships between elements in a certain sequence - **positional encoding** and **self-attention**.\n",
    "\n",
    "The Transformer architecture, introduced in the paper \"Attention is All You Need\" (Vaswani et al., 2017), revolutionized deep learning by showing that attention mechanisms alone, without any recurrent or convolutional layers, can achieve state-of-the-art results on many tasks.\n",
    "\n",
    "## 8.2.1 Positional Encoding\n",
    "\n",
    "RNN-based architectures inherently use the order of elements in the sequence. Another approach to representing the order between sequence elements is called **positional encoding**, in which we add to each element of the input a piece of information about its position in the sequence, and thus we can use architectures without RNN.\n",
    "\n",
    "### Why Positional Encoding?\n",
    "\n",
    "Since the Transformer uses only attention mechanisms (which are permutation-invariant), we need a way to inject information about the order of elements in the sequence. Positional encoding provides this information.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Formally, for an input sequence $x \\in \\mathbb{R}^d$, we compute a vector of dimension $d \\times 1$ as follows:\n",
    "\n",
    "For position $t$ and dimension $i$:\n",
    "\n",
    "$$p_t(i) = \\begin{cases}\n",
    "\\sin(\\omega_i t), & \\text{if } i \\text{ is even} \\\\\n",
    "\\cos(\\omega_i t), & \\text{if } i \\text{ is odd}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\omega_i = \\frac{1}{10000^{2i/d}}$\n",
    "\n",
    "The full positional encoding vector for position $t$ is:\n",
    "\n",
    "$$p_t = \\begin{bmatrix}\n",
    "\\sin(\\omega_1 t) \\\\\n",
    "\\cos(\\omega_1 t) \\\\\n",
    "\\sin(\\omega_2 t) \\\\\n",
    "\\cos(\\omega_2 t) \\\\\n",
    "\\vdots \\\\\n",
    "\\sin(\\omega_{d/2} t) \\\\\n",
    "\\cos(\\omega_{d/2} t)\n",
    "\\end{bmatrix}_{d \\times 1}$$\n",
    "\n",
    "### Understanding Positional Encoding\n",
    "\n",
    "To understand how this vector contains meaning of order between things, we can think of it as a continuous version of binary representation. If we want to take a sequence of numbers and represent them in binary form, we can see that the higher the bit weight, the less frequently it changes, and in fact the frequency of bit change is an indication of its position.\n",
    "\n",
    "The MSB (Most Significant Bit) changes at the lowest frequency, while the LSB (Least Significant Bit) changes at the highest frequency.\n",
    "\n",
    "Similarly, we can use trigonometric functions with frequencies that decrease and increase - this is essentially the vector $p$ - it contains many trigonometric functions with decreasing frequencies, and according to the frequency added to each element in the sequence, we can encode the position of the element.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Relative Position Information**: For any pair of functions with the same frequency $[\\sin(\\omega_i t), \\cos(\\omega_i t)]$, we can linearly transform them to represent a different position:\n",
    "\n",
    "$$M \\cdot \\begin{bmatrix} \\sin(\\omega_i t) \\\\ \\cos(\\omega_i t) \\end{bmatrix} = \\begin{bmatrix} \\sin(\\omega_i t + \\phi) \\\\ \\cos(\\omega_i t + \\phi) \\end{bmatrix}$$\n",
    "\n",
    "where $M = \\begin{bmatrix} \\cos(\\omega_i \\phi) & \\sin(\\omega_i \\phi) \\\\ -\\sin(\\omega_i \\phi) & \\cos(\\omega_i \\phi) \\end{bmatrix}$\n",
    "\n",
    "This allows the model to easily learn relative positions between any two positions.\n",
    "\n",
    "2. **Extrapolation**: The sinusoidal nature allows the model to extrapolate to sequence lengths longer than those seen during training.\n",
    "\n",
    "3. **Deterministic**: Positional encodings are fixed and not learned (though learned positional embeddings are also used in some models)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Positional Encoding Implementation\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute div_term: 1 / (10000^(2i/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of model state)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (batch, seq_len, d_model)\n",
    "            Input embeddings\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        x + positional_encoding : tensor, shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # x is (batch, seq_len, d_model)\n",
    "        # pe is (1, max_len, d_model)\n",
    "        # We need to slice pe to match seq_len\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Test Positional Encoding\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Create dummy input embeddings\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Add positional encoding\n",
    "x_with_pos = pos_encoding(x)\n",
    "\n",
    "print(\"Positional Encoding:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model dimension (d_model): {d_model}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {x_with_pos.shape}\")\n",
    "print(f\"Positional encoding shape: {pos_encoding.pe.shape}\")\n",
    "\n",
    "# Visualize positional encoding\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Positional encoding for different positions\n",
    "ax1 = axes[0, 0]\n",
    "positions = [0, 10, 20, 50]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "for pos, color in zip(positions, colors):\n",
    "    ax1.plot(pos_encoding.pe[0, pos, :64].numpy(), label=f'Position {pos}', color=color, linewidth=2)\n",
    "ax1.set_title('Positional Encoding Values (First 64 dims)', fontsize=12, weight='bold')\n",
    "ax1.set_xlabel('Dimension', fontsize=11)\n",
    "ax1.set_ylabel('Encoding Value', fontsize=11)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Heatmap of positional encoding\n",
    "ax2 = axes[0, 1]\n",
    "pe_matrix = pos_encoding.pe[0, :50, :].numpy()\n",
    "im = ax2.imshow(pe_matrix.T, cmap='coolwarm', aspect='auto', interpolation='nearest')\n",
    "ax2.set_title('Positional Encoding Heatmap (50 positions, all dims)', fontsize=12, weight='bold')\n",
    "ax2.set_xlabel('Position', fontsize=11)\n",
    "ax2.set_ylabel('Dimension', fontsize=11)\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Plot 3: Frequency analysis - how different frequencies change\n",
    "ax3 = axes[1, 0]\n",
    "positions = torch.arange(0, 50)\n",
    "# Plot different frequency components\n",
    "for i in [0, 2, 4, 8, 16]:\n",
    "    freq_component = pos_encoding.pe[0, :50, i].numpy()\n",
    "    ax3.plot(positions, freq_component, label=f'Dim {i} (freq {i//2})', linewidth=2)\n",
    "ax3.set_title('Different Frequency Components', fontsize=12, weight='bold')\n",
    "ax3.set_xlabel('Position', fontsize=11)\n",
    "ax3.set_ylabel('Encoding Value', fontsize=11)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Relative position encoding (demonstration)\n",
    "ax4 = axes[1, 1]\n",
    "# Show how position 10 can be transformed to position 20\n",
    "pos_10 = pos_encoding.pe[0, 10, :8].numpy()\n",
    "pos_20 = pos_encoding.pe[0, 20, :8].numpy()\n",
    "x_pos = np.arange(8)\n",
    "width = 0.35\n",
    "ax4.bar(x_pos - width/2, pos_10, width, label='Position 10', alpha=0.7)\n",
    "ax4.bar(x_pos + width/2, pos_20, width, label='Position 20', alpha=0.7)\n",
    "ax4.set_title('Positional Encoding Comparison (First 8 dims)', fontsize=12, weight='bold')\n",
    "ax4.set_xlabel('Dimension', fontsize=11)\n",
    "ax4.set_ylabel('Encoding Value', fontsize=11)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Properties of Positional Encoding:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Sinusoidal functions with different frequencies\")\n",
    "print(\"2. Lower dimensions have higher frequencies (change more rapidly)\")\n",
    "print(\"3. Higher dimensions have lower frequencies (change more slowly)\")\n",
    "print(\"4. Allows model to learn relative positions\")\n",
    "print(\"5. Can extrapolate to longer sequences than seen in training\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Self-Attention Layer\n",
    "\n",
    "In addition to positional encoding, the idea arose to perform attention not only between input elements and output elements, but also between input elements themselves. This is called **self-attention**.\n",
    "\n",
    "### Motivation for Self-Attention\n",
    "\n",
    "For each element in the sequence, we want to create a new representation that will represent an element in the original sequence plus information about its relationship to the other elements. The idea is to take each element in the sequence, and calculate its similarity to all other elements in the sequence.\n",
    "\n",
    "Similar elements (close) in the sequence will receive high similarity values, while different elements (distant) in the sequence will give low values. In NLP, this could be words that are likely to appear together, and in images, this could be similar pixels.\n",
    "\n",
    "The relationship between elements that have a connection between them is calculated using an inner product between representation vectors of the elements. Each inner product between two elements gives a coefficient that is a real number, and thus we can sum the product of all coefficients with the original elements, and get a new representation of the original element that also contains a relationship between the current element and similar elements in the sequence.\n",
    "\n",
    "In other words, we can look at the vector containing the relationships of an element in the sequence as its new representation that reflects its relationships with the rest of the sequence elements.\n",
    "\n",
    "### Query, Key, and Value\n",
    "\n",
    "Formally, to calculate self-attention, we create three matrices of coefficients for the input sequence. These matrices are called **Value**, **Key**, and **Query**, where each row in each matrix corresponds to an element of the input.\n",
    "\n",
    "Using these matrices, we calculate the attention score:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{SoftMax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right) \\cdot V$$\n",
    "\n",
    "where:\n",
    "- $Q$ (Query): What am I looking for?\n",
    "- $K$ (Key): What do I offer?\n",
    "- $V$ (Value): What information do I contain?\n",
    "\n",
    "### Step-by-Step Computation\n",
    "\n",
    "1. **Create Q, K, V matrices**: For input sequence $x$, we compute:\n",
    "   - $Q = x W_Q$ (Query matrix)\n",
    "   - $K = x W_K$ (Key matrix)\n",
    "   - $V = x W_V$ (Value matrix)\n",
    "   \n",
    "   where $W_Q, W_K, W_V$ are learned weight matrices.\n",
    "\n",
    "2. **Compute attention scores**: For each query $q_i$ and key $k_j$:\n",
    "   $$score_{ij} = \\frac{q_i^T k_j}{\\sqrt{d_k}}$$\n",
    "   \n",
    "   The scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large.\n",
    "\n",
    "3. **Apply softmax**: Convert scores to probabilities:\n",
    "   $$w_{ij} = \\text{SoftMax}(score_{ij}) = \\frac{\\exp(score_{ij})}{\\sum_{k=1}^{n} \\exp(score_{ik})}$$\n",
    "\n",
    "4. **Weighted sum**: Compute output for each position:\n",
    "   $$z_i = \\sum_{j=1}^{n} w_{ij} v_j$$\n",
    "\n",
    "### Understanding the Computation\n",
    "\n",
    "For an input sequence $x$, we get three matrices, where each element in the original sequence $x_i$ creates a row in each of the matrices. When we take the row $q_i = Q \\cdot x_i$ and multiply it by each of the rows in matrix $K$, we get a new vector, where each element $j$ says how much there is a relationship between elements $i, j$ in the original sequence.\n",
    "\n",
    "Performing this multiplication for the entire input sequence creates a new matrix where each row represents the relationship between a certain element and the rest of the sequence elements. This multiplication is essentially $Q \\cdot K^T$, where each $q_i^T k_j$ represents the relationship between element $i$ and element $j$.\n",
    "\n",
    "We scale the result by the dimension of the embedding to prevent large gradients, and normalize using SoftMax. In this way, we get a matrix of numbers in the range [0,1], representing as mentioned the relationship between every two elements in the original sequence.\n",
    "\n",
    "### The Output\n",
    "\n",
    "For each element in the sequence, we compute a new representation by weighting the vectors in $V$:\n",
    "\n",
    "$$z_i = \\sum_{j=1}^{n} w_{ij} v_j = \\sum_{j=1}^{n} \\frac{\\exp(q_i^T k_j)}{\\sum_{k=1}^{n} \\exp(q_i^T k_k)} v_j$$\n",
    "\n",
    "The resulting sequence $z$ is a new representation of the sequence, where each $z_i$ represents element $i$ in the original sequence together with information about the relationships between it and the rest of the sequence elements. The resulting sequence can be passed through a decoder or several additional layers, and thus perform various tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Self-Attention Implementation from Scratch\n",
    "def self_attention_naive(Q, K, V):\n",
    "    \"\"\"\n",
    "    Naive implementation of self-attention for understanding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q : tensor, shape (batch, seq_len, d_k)\n",
    "        Query matrix\n",
    "    K : tensor, shape (batch, seq_len, d_k)\n",
    "        Key matrix\n",
    "    V : tensor, shape (batch, seq_len, d_v)\n",
    "        Value matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : tensor, shape (batch, seq_len, d_v)\n",
    "        Self-attention output\n",
    "    attention_weights : tensor, shape (batch, seq_len, seq_len)\n",
    "        Attention weights for visualization\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    # Q @ K^T: (batch, seq_len, d_k) @ (batch, d_k, seq_len) -> (batch, seq_len, seq_len)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 3: Weighted sum of values\n",
    "    # attention_weights @ V: (batch, seq_len, seq_len) @ (batch, seq_len, d_v) -> (batch, seq_len, d_v)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# PyTorch Self-Attention Module\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention layer implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k if d_k is not None else d_model\n",
    "        self.d_v = d_v if d_v is not None else d_model\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, self.d_k)\n",
    "        self.W_K = nn.Linear(d_model, self.d_k)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through self-attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (batch, seq_len, d_model)\n",
    "            Input sequence\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : tensor, shape (batch, seq_len, d_v)\n",
    "            Self-attention output\n",
    "        attention_weights : tensor, shape (batch, seq_len, seq_len)\n",
    "            Attention weights\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_Q(x)  # (batch, seq_len, d_k)\n",
    "        K = self.W_K(x)  # (batch, seq_len, d_k)\n",
    "        V = self.W_V(x)  # (batch, seq_len, d_v)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test Self-Attention\n",
    "d_model = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Create input sequence\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Test naive implementation\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output_naive, weights_naive = self_attention_naive(Q, K, V)\n",
    "\n",
    "# Test PyTorch module\n",
    "self_attn = SelfAttention(d_model)\n",
    "output_module, weights_module = self_attn(x)\n",
    "\n",
    "print(\"Self-Attention Implementation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Model dimension (d_model): {d_model}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"\\nNaive Implementation:\")\n",
    "print(f\"  Output shape: {output_naive.shape}\")\n",
    "print(f\"  Attention weights shape: {weights_naive.shape}\")\n",
    "print(f\"  Attention weights sum (should be 1.0): {weights_naive.sum(dim=-1)[0, 0]}\")\n",
    "print(f\"\\nPyTorch Module:\")\n",
    "print(f\"  Output shape: {output_module.shape}\")\n",
    "print(f\"  Attention weights shape: {weights_module.shape}\")\n",
    "print(f\"  Attention weights sum (should be 1.0): {weights_module.sum(dim=-1)[0, 0]}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Attention weights for first batch item\n",
    "im1 = axes[0].imshow(weights_naive[0].detach().numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_title('Self-Attention Weights (Naive, First Batch)', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Key Position (j)', fontsize=11)\n",
    "axes[0].set_ylabel('Query Position (i)', fontsize=11)\n",
    "axes[0].set_xticks(range(seq_len))\n",
    "axes[0].set_yticks(range(seq_len))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(weights_module[0].detach().numpy(), cmap='Reds', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('Self-Attention Weights (Module, First Batch)', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Key Position (j)', fontsize=11)\n",
    "axes[1].set_ylabel('Query Position (i)', fontsize=11)\n",
    "axes[1].set_xticks(range(seq_len))\n",
    "axes[1].set_yticks(range(seq_len))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSelf-Attention Key Points:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Each position attends to all positions (including itself)\")\n",
    "print(\"2. Attention weights sum to 1.0 for each query position\")\n",
    "print(\"3. Higher weights indicate stronger relationships\")\n",
    "print(\"4. Self-attention is permutation-invariant (needs positional encoding)\")\n",
    "print(\"5. Complexity: O(n²) where n is sequence length\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Example: Self-Attention Computation\n",
    "\n",
    "Let's trace through a concrete example to understand how self-attention works:\n",
    "\n",
    "**Input**: Sequence of 3 words: [\"The\", \"cat\", \"sat\"]\n",
    "\n",
    "1. **Embedding**: Each word is converted to a vector (e.g., 4-dimensional for simplicity)\n",
    "   - \"The\" → [0.2, 0.1, 0.3, 0.1]\n",
    "   - \"cat\" → [0.5, 0.3, 0.2, 0.4]\n",
    "   - \"sat\" → [0.3, 0.2, 0.4, 0.3]\n",
    "\n",
    "2. **Create Q, K, V**: Apply learned transformations\n",
    "   - $Q = X W_Q$, $K = X W_K$, $V = X W_V$\n",
    "\n",
    "3. **Compute Scores**: For each word, compute similarity with all words\n",
    "   - \"The\" attends to [\"The\", \"cat\", \"sat\"]\n",
    "   - \"cat\" attends to [\"The\", \"cat\", \"sat\"]\n",
    "   - \"sat\" attends to [\"The\", \"cat\", \"sat\"]\n",
    "\n",
    "4. **Softmax**: Convert scores to probabilities\n",
    "   - Each row sums to 1.0\n",
    "\n",
    "5. **Weighted Sum**: Combine value vectors according to attention weights\n",
    "   - Output for \"cat\" might heavily weight \"sat\" (they often appear together)\n",
    "   - Output for \"The\" might weight all words more evenly (it's a common word)\n",
    "\n",
    "### Why Self-Attention Works\n",
    "\n",
    "1. **Parallel Computation**: Unlike RNNs, all positions can be computed in parallel\n",
    "2. **Long-Range Dependencies**: Direct connections between any two positions\n",
    "3. **Interpretability**: Attention weights show which words are related\n",
    "4. **Flexible Relationships**: Can learn different types of relationships (syntactic, semantic, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed Self-Attention Example\n",
    "def demonstrate_self_attention():\n",
    "    \"\"\"\n",
    "    Step-by-step demonstration of self-attention computation.\n",
    "    \"\"\"\n",
    "    # Simple example: 3 words, 4-dimensional embeddings\n",
    "    words = [\"The\", \"cat\", \"sat\"]\n",
    "    seq_len = len(words)\n",
    "    d_model = 4\n",
    "    \n",
    "    # Create simple embeddings (normally these would be learned)\n",
    "    embeddings = torch.tensor([\n",
    "        [0.2, 0.1, 0.3, 0.1],  # \"The\"\n",
    "        [0.5, 0.3, 0.2, 0.4],  # \"cat\"\n",
    "        [0.3, 0.2, 0.4, 0.3]   # \"sat\"\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    # Simple weight matrices (normally learned)\n",
    "    W_Q = torch.eye(d_model) * 0.5\n",
    "    W_K = torch.eye(d_model) * 0.5\n",
    "    W_V = torch.eye(d_model) * 0.5\n",
    "    \n",
    "    # Compute Q, K, V\n",
    "    Q = torch.matmul(embeddings, W_Q)\n",
    "    K = torch.matmul(embeddings, W_K)\n",
    "    V = torch.matmul(embeddings, W_V)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.T) / math.sqrt(d_model)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    print(\"Self-Attention Step-by-Step Example:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Words: {words}\")\n",
    "    print(f\"\\n1. Input Embeddings (shape {embeddings.shape}):\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(f\"   {word:5s}: {embeddings[i].numpy()}\")\n",
    "    \n",
    "    print(f\"\\n2. Attention Scores (before softmax, shape {scores.shape}):\")\n",
    "    print(scores.numpy())\n",
    "    \n",
    "    print(f\"\\n3. Attention Weights (after softmax, shape {attention_weights.shape}):\")\n",
    "    print(attention_weights.numpy())\n",
    "    print(\"\\n   Each row sums to 1.0 (probabilities)\")\n",
    "    \n",
    "    print(f\"\\n4. Output (shape {output.shape}):\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(f\"   {word:5s}: {output[i].numpy()}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Attention weights heatmap\n",
    "    im1 = axes[0].imshow(attention_weights.numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Attention Weights Matrix', fontsize=12, weight='bold')\n",
    "    axes[0].set_xlabel('Key Position (attended to)', fontsize=11)\n",
    "    axes[0].set_ylabel('Query Position (attending from)', fontsize=11)\n",
    "    axes[0].set_xticks(range(seq_len))\n",
    "    axes[0].set_yticks(range(seq_len))\n",
    "    axes[0].set_xticklabels(words)\n",
    "    axes[0].set_yticklabels(words)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            text = axes[0].text(j, i, f'{attention_weights[i, j].item():.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Bar chart showing attention for \"cat\"\n",
    "    axes[1].bar(words, attention_weights[1].numpy(), color=['blue', 'green', 'red'], alpha=0.7)\n",
    "    axes[1].set_title('Attention Weights for \"cat\"', fontsize=12, weight='bold')\n",
    "    axes[1].set_ylabel('Attention Weight', fontsize=11)\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_weights, output\n",
    "\n",
    "# Run demonstration\n",
    "attn_weights, output = demonstrate_self_attention()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3 Multi-Head Attention\n",
    "\n",
    "We can use the self-attention mechanism multiple times in parallel. Each time we get three matrices $(Q_h, K_h, V_h)$, and with them we calculate the new representations of the sequence elements (attention score). Each such mechanism is called an **attention head**, and the combination of multiple attention heads is called **Multi-Head Attention**.\n",
    "\n",
    "In this way, for each input element $x_i$, there are several different representations $z_i^h$, which can be multiplied by a weight matrix $W_O$ and get the weighted representation of the element using multiple attention heads.\n",
    "\n",
    "### Why Multi-Head Attention?\n",
    "\n",
    "Different attention heads can learn to focus on different aspects of the relationships:\n",
    "- **Head 1**: Might focus on syntactic relationships (subject-verb, etc.)\n",
    "- **Head 2**: Might focus on semantic relationships (synonyms, related concepts)\n",
    "- **Head 3**: Might focus on long-range dependencies\n",
    "- **Head 4**: Might focus on positional relationships\n",
    "\n",
    "By combining multiple heads, the model can capture richer, more nuanced relationships.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For $h$ attention heads:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O$$\n",
    "\n",
    "where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)$$\n",
    "\n",
    "and:\n",
    "- $W_Q^i, W_K^i, W_V^i$ are learned projections for head $i$\n",
    "- $W_O$ is the output projection matrix\n",
    "- Typically $d_k = d_v = d_{model} / h$ to keep total parameters similar\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "1. **Split dimensions**: Divide $d_{model}$ into $h$ heads\n",
    "2. **Parallel computation**: Compute attention for all heads simultaneously\n",
    "3. **Concatenate**: Combine all head outputs\n",
    "4. **Project**: Apply final linear transformation\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Diverse representations**: Each head can specialize\n",
    "- **Parallel computation**: All heads computed simultaneously\n",
    "- **Richer modeling**: Captures multiple relationship types\n",
    "- **Scalability**: Can increase model capacity by adding heads"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Implementation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V (for all heads)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through multi-head attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Q, K, V : tensor, shape (batch, seq_len, d_model)\n",
    "            Query, Key, Value matrices (can have different seq_len for encoder-decoder attention)\n",
    "        mask : tensor, optional, shape (batch, seq_len_q, seq_len_k)\n",
    "            Mask to prevent attention to certain positions\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : tensor, shape (batch, seq_len_q, d_model)\n",
    "            Multi-head attention output\n",
    "        attention_weights : tensor, shape (batch, num_heads, seq_len_q, seq_len_k)\n",
    "            Attention weights for all heads\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = Q.shape\n",
    "        _, seq_len_k, _ = K.shape\n",
    "        _, seq_len_v, _ = V.shape\n",
    "        \n",
    "        # Project and reshape for multi-head\n",
    "        # Q: (batch, seq_len_q, d_model) -> (batch, seq_len_q, num_heads, d_k) -> (batch, num_heads, seq_len_q, d_k)\n",
    "        Q = self.W_Q(Q).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # K: (batch, seq_len_k, d_model) -> (batch, seq_len_k, num_heads, d_k) -> (batch, num_heads, seq_len_k, d_k)\n",
    "        K = self.W_K(K).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # V: (batch, seq_len_v, d_model) -> (batch, seq_len_v, num_heads, d_k) -> (batch, num_heads, seq_len_v, d_k)\n",
    "        V = self.W_V(V).view(batch_size, seq_len_v, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        # Q: (batch, num_heads, seq_len_q, d_k)\n",
    "        # K: (batch, num_heads, seq_len_k, d_k)\n",
    "        # scores: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        # mask should be (batch, 1, seq_len_q, seq_len_k) or broadcastable\n",
    "        if mask is not None:\n",
    "            # Expand mask to match scores dimensions if needed\n",
    "            if mask.dim() == 4 and mask.size(1) == 1:\n",
    "                # mask is (batch, 1, seq_len_q, seq_len_k), expand to (batch, num_heads, seq_len_q, seq_len_k)\n",
    "                mask = mask.expand(batch_size, self.num_heads, seq_len_q, seq_len_k)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, d_k)\n",
    "        # -> (batch, num_heads, seq_len, d_k)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        # (batch, num_heads, seq_len_q, d_k) -> (batch, seq_len_q, num_heads, d_k)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # -> (batch, seq_len_q, d_model)\n",
    "        output = output.view(batch_size, seq_len_q, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_O(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Test multi-head attention\n",
    "multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
    "output, attention_weights = multi_head_attn(x, x, x)\n",
    "\n",
    "print(\"Multi-Head Attention:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Model dimension (d_model): {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head (d_k): {d_model // num_heads}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  - Batch: {attention_weights.shape[0]}\")\n",
    "print(f\"  - Heads: {attention_weights.shape[1]}\")\n",
    "print(f\"  - Query positions: {attention_weights.shape[2]}\")\n",
    "print(f\"  - Key positions: {attention_weights.shape[3]}\")\n",
    "\n",
    "# Visualize attention from different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "fig.suptitle('Attention Weights from Different Heads (First Batch Item)', \n",
    "              fontsize=14, weight='bold', y=1.02)\n",
    "\n",
    "for head in range(num_heads):\n",
    "    row = head // 4\n",
    "    col = head % 4\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    im = ax.imshow(attention_weights[0, head].detach().numpy(), \n",
    "                   cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {head+1}', fontsize=10, weight='bold')\n",
    "    ax.set_xlabel('Key Position', fontsize=9)\n",
    "    ax.set_ylabel('Query Position', fontsize=9)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-Head Attention Benefits:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Each head can learn different types of relationships\")\n",
    "print(\"2. Parallel computation of all heads\")\n",
    "print(\"3. Richer representation by combining multiple perspectives\")\n",
    "print(\"4. Total parameters similar to single-head (d_k = d_model / num_heads)\")\n",
    "print(\"5. Attention weights show what each head focuses on\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Complete Transformer Implementation\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network in Transformer.\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Masked self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        attn_output, _ = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer model.\"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, \n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, max_len=5000, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"Generate masks for encoder and decoder.\"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        src_len = src.size(1)\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Source mask for encoder self-attention: (batch, 1, 1, src_len)\n",
    "        # This prevents attending to padding tokens in the source\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, src_len)\n",
    "        \n",
    "        # Target mask for decoder self-attention: (batch, 1, tgt_len, tgt_len)\n",
    "        # This prevents attending to padding tokens and future tokens\n",
    "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)  # (batch, 1, tgt_len, 1)\n",
    "        look_ahead_mask = torch.triu(torch.ones(tgt_len, tgt_len, device=tgt.device), diagonal=1).bool()  # (tgt_len, tgt_len)\n",
    "        look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, tgt_len, tgt_len)\n",
    "        tgt_mask = tgt_padding_mask & ~look_ahead_mask  # (batch, 1, tgt_len, tgt_len)\n",
    "        \n",
    "        # Source mask for encoder-decoder attention: (batch, 1, tgt_len, src_len)\n",
    "        # This prevents attending to padding tokens in the source when querying from target\n",
    "        src_mask_enc_dec = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, src_len)\n",
    "        src_mask_enc_dec = src_mask_enc_dec.expand(batch_size, 1, tgt_len, src_len)  # (batch, 1, tgt_len, src_len)\n",
    "        \n",
    "        return src_mask, tgt_mask, src_mask_enc_dec\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Embeddings + positional encoding\n",
    "        src_emb = self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Generate masks\n",
    "        src_mask, tgt_mask, src_mask_enc_dec = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        enc_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask_enc_dec, tgt_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_proj(dec_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test Transformer\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
    "                         num_layers, num_layers, d_ff=512, max_len=100)\n",
    "\n",
    "# Create dummy sequences\n",
    "src_seq_len = 10\n",
    "tgt_seq_len = 12\n",
    "batch_size = 2\n",
    "\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(src, tgt)\n",
    "\n",
    "print(\"Complete Transformer Model:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Source vocabulary size: {src_vocab_size}\")\n",
    "print(f\"Target vocabulary size: {tgt_vocab_size}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Number of encoder/decoder layers: {num_layers}\")\n",
    "print(f\"\\nInput shapes:\")\n",
    "print(f\"  Source: {src.shape}\")\n",
    "print(f\"  Target: {tgt.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Visualize Transformer architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(-0.5, 8)\n",
    "ax.set_ylim(-0.5, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Transformer Architecture', fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "# Input embeddings\n",
    "ax.text(0, 9, 'Input\\nEmbeddings', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax.text(0, 7.5, 'Positional\\nEncoding', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Encoder stack\n",
    "ax.text(2, 8.5, 'Encoder', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "for i in range(3):\n",
    "    y_pos = 7 - i * 1.5\n",
    "    # Encoder layer\n",
    "    ax.text(2, y_pos, f'Encoder Layer {i+1}', fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', edgecolor='black'))\n",
    "    ax.text(2, y_pos-0.5, 'Self-Attn → FFN', fontsize=8, ha='center', style='italic')\n",
    "    if i < 2:\n",
    "        ax.arrow(2, y_pos-0.7, 0, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Encoder output\n",
    "ax.text(2, 1.5, 'Encoder\\nOutput', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Target embeddings\n",
    "ax.text(5, 9, 'Target\\nEmbeddings', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax.text(5, 7.5, 'Positional\\nEncoding', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Decoder stack\n",
    "ax.text(7, 8.5, 'Decoder', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "for i in range(3):\n",
    "    y_pos = 7 - i * 1.5\n",
    "    # Decoder layer\n",
    "    ax.text(7, y_pos, f'Decoder Layer {i+1}', fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='pink', edgecolor='black'))\n",
    "    ax.text(7, y_pos-0.5, 'Masked Self-Attn', fontsize=8, ha='center', style='italic')\n",
    "    ax.text(7, y_pos-0.7, '→ Enc-Dec Attn → FFN', fontsize=8, ha='center', style='italic')\n",
    "    if i < 2:\n",
    "        ax.arrow(7, y_pos-0.9, 0, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Decoder output\n",
    "ax.text(7, 1.5, 'Output\\nProjection', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrows\n",
    "ax.arrow(0.5, 8.25, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(0.5, 7.75, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(2.5, 8.25, 0, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.5, 8.25, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.5, 7.75, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 8.25, 0, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Encoder-decoder attention connection\n",
    "ax.plot([2.5, 2.5, 6.5, 6.5], [4, 3, 3, 4], 'g--', linewidth=2, alpha=0.7)\n",
    "ax.arrow(6.5, 3.5, 0, 0.3, head_width=0.15, head_length=0.1, fc='green', ec='green')\n",
    "ax.text(4.5, 3, 'Encoder-Decoder\\nAttention', fontsize=9, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='green', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.5 Transformer Applications\n",
    "\n",
    "The Transformer presented state-of-the-art performance on many tasks using attention alone. Besides the high level of performance, the training process of Transformer is much faster than convolutional networks or recurrent networks. Like other models, with Transformer we can also perform **transfer learning**, i.e., take a Transformer trained on one task and adapt it to another task.\n",
    "\n",
    "In practice, not all applications use the entire Transformer - some applications use only the encoder or only the decoder, depending on the task.\n",
    "\n",
    "### Machine Translation\n",
    "\n",
    "Translation of sentences between different languages is a trivial application of the Transformer. The task is to take a sentence and output a sentence in another language, and this is done by representing the original sentence in a new way using self-attention and then converting it using Encoder-Decoder Attention to another sentence.\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "A language model based on encoder only. A language model is a function that receives text as input and returns the distribution over words for the next word. The most familiar and intuitive language model is automatic completion, which suggests the most likely word or words given what the user has typed so far.\n",
    "\n",
    "Since self-attention is used together with the contexts between different words, the encoder in Transformer can function as a language model in an appropriate way. The developers of BERT used only the encoder - they trained the model on sentences where each time randomly they do masking of words in the sentence, and the task is to predict the masked words.\n",
    "\n",
    "**BERT Key Features:**\n",
    "- **Bidirectional**: Uses both left and right context\n",
    "- **Masked Language Modeling**: Predicts masked words in sentences\n",
    "- **Next Sentence Prediction**: Learns relationships between sentences\n",
    "- **Pre-training + Fine-tuning**: Trained on large corpus, then fine-tuned for specific tasks\n",
    "\n",
    "### Generative Pre-training (GPT)\n",
    "\n",
    "A model for predicting the next word in a sentence. We can take a sentence that is cut in the middle, and examine what word the decoder suggests. We input a cut sentence to the decoder and then go through many words and check which word is most likely to be the next word in the sentence.\n",
    "\n",
    "In fact, the Key is the sentence that was input, and the Query that enters is each time a different word in the dictionary, and thus using attention we examine which Query will fit best with the Key.\n",
    "\n",
    "**GPT Key Features:**\n",
    "- **Decoder-only**: Uses only the decoder part\n",
    "- **Autoregressive**: Generates text one token at a time\n",
    "- **Causal masking**: Prevents looking at future tokens\n",
    "- **Pre-training**: Trained on large text corpus\n",
    "- **Fine-tuning**: Adapted for specific tasks\n",
    "\n",
    "### Other Applications\n",
    "\n",
    "1. **DETR (Detection Transformer)**: Object detection in images using Transformer\n",
    "2. **Vision Transformer (ViT)**: Image classification using Transformer\n",
    "3. **Speech Recognition**: Transformer for speech-to-text\n",
    "4. **Text Summarization**: Using encoder-decoder architecture\n",
    "5. **Question Answering**: BERT-based models\n",
    "6. **Named Entity Recognition**: Sequence labeling with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: BERT-style Encoder-only Model\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT-style encoder-only model for language modeling.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_len=512):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        \n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embeddings + positional encoding\n",
    "        x = self.pos_encoding(self.token_embedding(x) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example: GPT-style Decoder-only Model\n",
    "class GPTDecoder(nn.Module):\n",
    "    \"\"\"GPT-style decoder-only model for text generation.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_len=512):\n",
    "        super(GPTDecoder, self).__init__()\n",
    "        \n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Decoder layers (without encoder-decoder attention)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embeddings + positional encoding\n",
    "        x = self.pos_encoding(self.token_embedding(x) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Pass through decoder layers (masked self-attention only)\n",
    "        # For GPT, we don't use encoder-decoder attention\n",
    "        for layer in self.decoder_layers:\n",
    "            # Use only masked self-attention, skip encoder-decoder attention\n",
    "            attn_output, _ = layer.self_attn(x, x, x, mask)\n",
    "            x = layer.norm1(x + layer.dropout1(attn_output))\n",
    "            ff_output = layer.feed_forward(x)\n",
    "            x = layer.norm3(x + layer.dropout3(ff_output))\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_proj(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test BERT and GPT\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "\n",
    "bert = BERTEncoder(vocab_size, d_model, num_heads, num_layers, d_ff=512)\n",
    "gpt = GPTDecoder(vocab_size, d_model, num_heads, num_layers, d_ff=512)\n",
    "\n",
    "# Create dummy input\n",
    "seq_len = 15\n",
    "batch_size = 2\n",
    "input_ids = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "bert_output = bert(input_ids)\n",
    "gpt_output = gpt(input_ids)\n",
    "\n",
    "print(\"Transformer Applications:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBERT (Encoder-only):\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")\n",
    "print(f\"  Output shape: {bert_output.shape}\")\n",
    "print(f\"  Use case: Language understanding, masked language modeling\")\n",
    "print(f\"\\nGPT (Decoder-only):\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")\n",
    "print(f\"  Output shape: {gpt_output.shape}\")\n",
    "print(f\"  Use case: Text generation, next token prediction\")\n",
    "\n",
    "# Visualize architecture differences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# BERT architecture\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-0.5, 4)\n",
    "ax1.set_ylim(-0.5, 6)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('BERT Architecture (Encoder-only)', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "ax1.text(0, 5, 'Input\\nTokens', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax1.text(0, 3.5, 'Embeddings\\n+ Position', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "ax1.text(2, 4.5, 'Encoder\\nStack', fontsize=11, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "for i in range(3):\n",
    "    y_pos = 3.5 - i * 0.8\n",
    "    ax1.text(2, y_pos, f'Layer {i+1}', fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', edgecolor='black'))\n",
    "    if i < 2:\n",
    "        ax1.arrow(2, y_pos-0.4, 0, -0.3, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "\n",
    "ax1.text(2, 0.5, 'Contextualized\\nRepresentations', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "\n",
    "ax1.arrow(0.5, 4.25, 1, 0, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax1.arrow(0.5, 3.75, 1, 0, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax1.arrow(2.5, 4.5, 0, -0.5, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax1.arrow(2.5, 0.5, 0, 0.3, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "\n",
    "# GPT architecture\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-0.5, 4)\n",
    "ax2.set_ylim(-0.5, 6)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('GPT Architecture (Decoder-only)', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "ax2.text(0, 5, 'Input\\nTokens', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax2.text(0, 3.5, 'Embeddings\\n+ Position', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "ax2.text(2, 4.5, 'Decoder\\nStack', fontsize=11, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "for i in range(3):\n",
    "    y_pos = 3.5 - i * 0.8\n",
    "    ax2.text(2, y_pos, f'Layer {i+1}', fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='pink', edgecolor='black'))\n",
    "    ax2.text(2, y_pos-0.25, 'Masked\\nSelf-Attn', fontsize=8, ha='center', style='italic')\n",
    "    if i < 2:\n",
    "        ax2.arrow(2, y_pos-0.4, 0, -0.3, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "\n",
    "ax2.text(2, 0.5, 'Next Token\\nPrediction', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "\n",
    "ax2.arrow(0.5, 4.25, 1, 0, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax2.arrow(0.5, 3.75, 1, 0, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax2.arrow(2.5, 4.5, 0, -0.5, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "ax2.arrow(2.5, 0.5, 0, 0.3, head_width=0.1, head_length=0.08, fc='black', ec='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we've covered the **Attention Mechanism** - one of the most important breakthroughs in deep learning:\n",
    "\n",
    "### 8.1 Sequence to Sequence Learning and Attention\n",
    "- **8.1.1 Attention in Seq2Seq Models**: Solved the information bottleneck problem, dynamic alignment, interpretability\n",
    "- **8.1.2 Bahdanau and Luong Attention**: Early attention mechanisms, additive vs multiplicative attention\n",
    "\n",
    "### 8.2 Transformer\n",
    "- **8.2.1 Positional Encoding**: Sinusoidal encoding to represent sequence order without recurrence\n",
    "- **8.2.2 Self-Attention Layer**: Query, Key, Value matrices, attention scores, weighted aggregation\n",
    "- **8.2.3 Multi-Head Attention**: Parallel attention heads, diverse relationship modeling\n",
    "- **8.2.4 Transformer End to End**: Complete encoder-decoder architecture, residual connections, layer normalization\n",
    "- **8.2.5 Transformer Applications**: BERT, GPT, machine translation, and more\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Attention** allows models to focus on relevant parts of input when generating output\n",
    "- **Self-attention** finds relationships within a single sequence\n",
    "- **Multi-head attention** captures diverse types of relationships in parallel\n",
    "- **Positional encoding** injects order information without recurrence\n",
    "- **Transformer** uses only attention mechanisms, no RNNs or CNNs\n",
    "- **BERT** uses encoder-only architecture for language understanding\n",
    "- **GPT** uses decoder-only architecture for text generation\n",
    "\n",
    "The Transformer architecture has revolutionized NLP and many other domains, enabling models like BERT, GPT, and modern LLMs that have transformed the field of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
