{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Bonus: Hands-On Linear and Logistic Regression\n",
    "\n",
    "This bonus notebook extends **Chapter 3 â€“ Regression** with more hands-on, from-scratch implementations. We will:\n",
    "\n",
    "1. Derive and implement **Gradient Descent for Linear Regression** using NumPy\n",
    "2. Implement **Logistic Regression by hand** (no autograd)\n",
    "3. Visualize **decision boundaries** for a 2D classification problem\n",
    "4. Explore the effect of **L2 regularization** on logistic regression\n",
    "5. Summarize when linear models work well and why we need multi-layer networks\n",
    "\n",
    "The goal is to deepen your intuition for linear models and prepare you for **multi-layer perceptrons** in Chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent for Linear Regression (from scratch)\n",
    "\n",
    "In Chapter 3 we saw linear regression in vector form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = Xw + b, \\quad X \\in \\mathbb{R}^{n \\times d},\\ w \\in \\mathbb{R}^d,\\ b \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "For a dataset $(x_i, y_i)_{i=1}^n$, the **Mean Squared Error (MSE)** loss is:\n",
    "\n",
    "$$\n",
    "L(w, b) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2.\n",
    "$$\n",
    "\n",
    "Writing this in matrix form with $\\hat{y} = Xw + b\\mathbf{1}$:\n",
    "\n",
    "$$\n",
    "L(w, b) = \\frac{1}{2n} \\lVert Xw + b\\mathbf{1} - y \\rVert^2.\n",
    "$$\n",
    "\n",
    "The gradients are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(Xw + b\\mathbf{1} - y), \\quad\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\mathbf{1}^T(Xw + b\\mathbf{1} - y).\n",
    "$$\n",
    "\n",
    "**Gradient Descent update rule:**\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}, \\quad\n",
    "b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b},\n",
    "$$\n",
    "\n",
    "where $\\eta > 0$ is the learning rate.\n",
    "\n",
    "We will now:\n",
    "\n",
    "1. Generate synthetic 1D data from a true line\n",
    "2. Compute the **closed-form solution** for linear regression\n",
    "3. Train with **gradient descent** using only NumPy\n",
    "4. Compare the learned parameters and visualize the fit and the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 1D data\n",
    "def generate_linear_data(n_samples=100, noise_std=2.0):\n",
    "    \"\"\"y = 3x + 5 + noise\"\"\"\n",
    "    X = np.linspace(-5, 5, n_samples).reshape(-1, 1)\n",
    "    true_w = np.array([3.0])\n",
    "    true_b = 5.0\n",
    "    noise = np.random.randn(n_samples, 1) * noise_std\n",
    "    y = X @ true_w.reshape(-1, 1) + true_b + noise\n",
    "    return X, y.ravel(), true_w, true_b\n",
    "\n",
    "X, y, true_w, true_b = generate_linear_data()\n",
    "\n",
    "# Closed-form solution: w* = (X^T X)^{-1} X^T y\n",
    "X_bias = np.c_[X, np.ones_like(X)]  # [x, 1]\n",
    "XtX = X_bias.T @ X_bias\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "XtY = X_bias.T @ y\n",
    "w_closed = XtX_inv @ XtY  # [w, b]\n",
    "\n",
    "print(\"True w, b:\", true_w[0], true_b)\n",
    "print(\"Closed-form w, b:\", w_closed[0], w_closed[1])\n",
    "\n",
    "# Gradient Descent implementation\n",
    "def linear_regression_gd(X, y, lr=1e-2, n_iters=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    for it in range(n_iters):\n",
    "        y_pred = X @ w + b\n",
    "        error = y_pred - y\n",
    "        loss = 0.5 / n_samples * np.sum(error ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Gradients\n",
    "        grad_w = (1.0 / n_samples) * (X.T @ error)\n",
    "        grad_b = (1.0 / n_samples) * np.sum(error)\n",
    "        \n",
    "        # Update\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "w_gd, b_gd, losses = linear_regression_gd(X, y, lr=5e-3, n_iters=2000)\n",
    "\n",
    "print(\"\\nGradient Descent w, b:\", w_gd[0], b_gd)\n",
    "\n",
    "# Plot data and fits\n",
    "x_plot = np.linspace(-5.5, 5.5, 200).reshape(-1, 1)\n",
    "y_true_line = x_plot * true_w[0] + true_b\n",
    "y_closed = x_plot * w_closed[0] + w_closed[1]\n",
    "y_gd = x_plot * w_gd[0] + b_gd\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: data and lines\n",
    "axes[0].scatter(X, y, alpha=0.6, label=\"Data\")\n",
    "axes[0].plot(x_plot, y_true_line, 'k--', label=\"True line\")\n",
    "axes[0].plot(x_plot, y_closed, 'r-', label=\"Closed-form fit\")\n",
    "axes[0].plot(x_plot, y_gd, 'g-', label=\"GD fit\")\n",
    "axes[0].set_title(\"Linear Regression: True vs Closed-form vs GD\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: loss curve\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_title(\"Gradient Descent Training Loss (MSE)\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Gradient descent converges to almost the same parameters as the closed-form solution.\n",
    "- The loss decreases smoothly until it plateaus.\n",
    "- This matches the theory from Chapter 3: linear regression has a convex loss and a unique global minimum.\n",
    "\n",
    "Next we will move to **logistic regression**, still with manual gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression by Hand (Binary Classification)\n",
    "\n",
    "For binary classification with labels $y \\in \\{0, 1\\}$, logistic regression models the probability of class 1 as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\sigma(Xw + b), \\quad \\sigma(t) = \\frac{1}{1 + e^{-t}}.\n",
    "$$\n",
    "\n",
    "The **Binary Cross-Entropy (BCE)** loss is:\n",
    "\n",
    "$$\n",
    "L(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right].\n",
    "$$\n",
    "\n",
    "The gradients for a batch of size $n$ can be written compactly as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(\\hat{y} - y), \\quad\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i).\n",
    "$$\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Generate a simple 2D dataset for binary classification\n",
    "2. Implement logistic regression training **from scratch** using these gradients\n",
    "3. Compare with `sklearn.linear_model.LogisticRegression` on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for logistic regression\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"Binary cross-entropy for vectors y_true, y_pred in [0,1].\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def logistic_regression_gd(X, y, lr=0.1, n_iters=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    for it in range(n_iters):\n",
    "        z = X @ w + b\n",
    "        y_pred = sigmoid(z)\n",
    "        loss = binary_cross_entropy(y, y_pred)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Gradients\n",
    "        error = y_pred - y\n",
    "        grad_w = (1.0 / n_samples) * (X.T @ error)\n",
    "        grad_b = (1.0 / n_samples) * np.sum(error)\n",
    "        \n",
    "        # Update\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "\n",
    "# Generate a simple 2D dataset\n",
    "X, y = make_moons(n_samples=400, noise=0.25, random_state=42)\n",
    "\n",
    "# Standardize features for easier optimization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train from-scratch logistic regression\n",
    "w_lr, b_lr, losses_lr = logistic_regression_gd(X_scaled, y, lr=0.1, n_iters=2000)\n",
    "\n",
    "print(\"From-scratch Logistic Regression:\")\n",
    "print(\"w:\", w_lr)\n",
    "print(\"b:\", b_lr)\n",
    "print(\"Final BCE loss:\", losses_lr[-1])\n",
    "\n",
    "# Compare with sklearn\n",
    "log_reg = LogisticRegression(fit_intercept=True, solver=\"lbfgs\")\n",
    "log_reg.fit(X_scaled, y)\n",
    "\n",
    "print(\"\\nSklearn LogisticRegression:\")\n",
    "print(\"w:\", log_reg.coef_.ravel())\n",
    "print(\"b:\", log_reg.intercept_[0])\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses_lr)\n",
    "plt.title(\"Logistic Regression (from scratch) - Training Loss (BCE)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We will use **NumPy** for numerical computation, **matplotlib** for plotting, and **scikit-learn** only for a few sanity checks (to compare with our from-scratch implementations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Boundary Visualization\n",
    "\n",
    "A powerful way to understand linear models is to **visualize the decision boundary** in 2D.\n",
    "\n",
    "For a logistic regression classifier trained on 2D features $x = (x_1, x_2)$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b),\n",
    "$$\n",
    "\n",
    "The **decision boundary** where the model is undecided ($\\hat{y} = 0.5$) satisfies:\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0.\n",
    "$$\n",
    "\n",
    "This is a **straight line** in the $(x_1, x_2)$ plane. We will:\n",
    "\n",
    "1. Train our from-scratch logistic regression on a 2D dataset\n",
    "2. Plot the data points\n",
    "3. Overlay the decision boundary and probability contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse X_scaled, y, w_lr, b_lr from previous cell\n",
    "\n",
    "# Create a grid over feature space\n",
    "x1_min, x1_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
    "x2_min, x2_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                       np.linspace(x2_min, x2_max, 200))\n",
    "\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs = sigmoid(grid @ w_lr + b_lr).reshape(xx1.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Plot decision regions\n",
    "contour = ax.contourf(xx1, xx2, probs, levels=20, cmap=\"RdBu\", alpha=0.6)\n",
    "plt.colorbar(contour, ax=ax, label=\"P(y=1 | x)\")\n",
    "\n",
    "# Decision boundary (p = 0.5)\n",
    "ax.contour(xx1, xx2, probs, levels=[0.5], colors=\"k\", linewidths=2)\n",
    "\n",
    "# Scatter data points\n",
    "ax.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c=\"blue\", edgecolor=\"k\", label=\"Class 0\")\n",
    "ax.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c=\"red\", edgecolor=\"k\", label=\"Class 1\")\n",
    "\n",
    "ax.set_title(\"Logistic Regression Decision Boundary (Moons Dataset)\")\n",
    "ax.set_xlabel(\"x1 (standardized)\")\n",
    "ax.set_ylabel(\"x2 (standardized)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization Effects (L2 / Ridge)\n",
    "\n",
    "Regularization helps control model complexity and reduce overfitting. For logistic regression with **L2 regularization** (ridge), we add a penalty on the weights:\n",
    "\n",
    "$$\n",
    "L_{\\text{reg}}(w, b) = L(w, b) + \\lambda \\lVert w \\rVert^2,\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ controls the strength of regularization.\n",
    "\n",
    "Effects of L2 regularization:\n",
    "\n",
    "- Encourages **smaller weights** (shrinks coefficients)\n",
    "- Can make the decision boundary **less sensitive** to noise\n",
    "- Often improves **generalization** on test data\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Train two logistic regressions on the same 2D dataset:\n",
    "   - One **without** regularization\n",
    "   - One **with** L2 regularization\n",
    "2. Compare their weights and decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression models with and without L2 regularization\n",
    "\n",
    "# No regularization (very large C)\n",
    "log_reg_no_reg = LogisticRegression(fit_intercept=True, solver=\"lbfgs\", C=1e6)\n",
    "log_reg_no_reg.fit(X_scaled, y)\n",
    "\n",
    "# With L2 regularization (smaller C)\n",
    "log_reg_l2 = LogisticRegression(fit_intercept=True, solver=\"lbfgs\", C=0.1)\n",
    "log_reg_l2.fit(X_scaled, y)\n",
    "\n",
    "print(\"No regularization (C=1e6):\")\n",
    "print(\"  w:\", log_reg_no_reg.coef_.ravel())\n",
    "print(\"  b:\", log_reg_no_reg.intercept_[0])\n",
    "\n",
    "print(\"\\nWith L2 regularization (C=0.1):\")\n",
    "print(\"  w:\", log_reg_l2.coef_.ravel())\n",
    "print(\"  b:\", log_reg_l2.intercept_[0])\n",
    "\n",
    "# Plot decision boundaries\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                       np.linspace(x2_min, x2_max, 200))\n",
    "\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs_no_reg = log_reg_no_reg.predict_proba(grid)[:, 1].reshape(xx1.shape)\n",
    "probs_l2 = log_reg_l2.predict_proba(grid)[:, 1].reshape(xx1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, probs, title in zip(\n",
    "    axes,\n",
    "    [probs_no_reg, probs_l2],\n",
    "    [\"No regularization (C=1e6)\", \"With L2 regularization (C=0.1)\"]):\n",
    "    \n",
    "    contour = ax.contourf(xx1, xx2, probs, levels=20, cmap=\"RdBu\", alpha=0.6)\n",
    "    ax.contour(xx1, xx2, probs, levels=[0.5], colors=\"k\", linewidths=2)\n",
    "    \n",
    "    ax.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c=\"blue\", edgecolor=\"k\", label=\"Class 0\", alpha=0.7)\n",
    "    ax.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c=\"red\", edgecolor=\"k\", label=\"Class 1\", alpha=0.7)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x1 (standardized)\")\n",
    "    ax.set_ylabel(\"x2 (standardized)\")\n",
    "\n",
    "axes[0].legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary and Connection to Multi-Layer Networks\n",
    "\n",
    "In this bonus notebook we:\n",
    "\n",
    "- Implemented **linear regression** from scratch and confirmed that gradient descent converges to the same solution as the closed-form formula.\n",
    "- Implemented **logistic regression** by hand with explicit gradients and compared it to scikit-learn's implementation.\n",
    "- Visualized **decision boundaries** in 2D and saw that logistic regression always learns a **linear (straight-line)** boundary in feature space.\n",
    "- Demonstrated how **L2 regularization** shrinks weights and can slightly smooth the decision boundary, improving generalization.\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "- Linear and logistic models are **simple, fast, and interpretable**.\n",
    "- However, they can only represent **linear decision boundaries**. Problems like XOR or highly non-linear patterns **cannot be solved** by a single linear model in the original feature space.\n",
    "- This is the main motivation for **multi-layer networks (MLPs)** in Chapter 4: by stacking layers with non-linear activations, we can represent complex, non-linear decision boundaries while still training with gradient-based methods.\n",
    "\n",
    "You can refer back to this notebook whenever you want a concrete reminder of how gradients and optimization work for the simplest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
