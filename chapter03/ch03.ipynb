{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Neural Networks\n",
    "\n",
    "This notebook covers **Chapter 3** of the Deep Learning in Hebrew book, focusing on regression problems and linear neural networks. We'll learn how to build models that can predict continuous values and perform classification tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This chapter deals with regression problems - how to build a model from a given set of examples that can predict continuous values. The model will learn the relationship between input features and output values.\n",
    "\n",
    "We'll focus on two main types:\n",
    "1. **Linear Regression** - for predicting continuous values\n",
    "2. **Softmax Regression** - for classification problems (including logistic regression)\n",
    "\n",
    "We'll solve regression problems using simple neural networks, which will serve as a foundation for deeper networks that can handle non-linearly separable data.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 3.1 Linear Regression\n",
    "- 3.1.1 [The Basic Concept](#311-the-basic-concept)\n",
    "- 3.1.2 [Gradient Descent](#312-gradient-descent)\n",
    "- 3.1.3 [Regularization and Cross Validation](#313-regularization-and-cross-validation)\n",
    "- 3.1.4 [Linear Regression as Classifier](#314-linear-regression-as-classifier)\n",
    "\n",
    "### 3.2 Softmax Regression\n",
    "- 3.2.1 [Logistic Regression](#321-logistic-regression)\n",
    "- 3.2.2 [Cross Entropy and Gradient Descent](#322-cross-entropy-and-gradient-descent)\n",
    "- 3.2.3 [Optimization](#323-optimization)\n",
    "- 3.2.4 [SoftMax Regression – Multi Class Logistic Regression](#324-softmax-regression--multi-class-logistic-regression)\n",
    "- 3.2.5 [SoftMax Regression as Neural Network](#325-softmax-regression-as-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Linear Regression\n",
    "\n",
    "## 3.1.1 The Basic Concept\n",
    "\n",
    "The simplest model is **linear regression**. This model tries to find a linear relationship between one or more input features and a dependent variable.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given independent variables $\\mathbf{x} \\in \\mathbb{R}^d$ and a dependent variable $y \\in \\mathbb{R}$, we model their relationship as:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d + b$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ are the weights (parameters)\n",
    "- $b \\in \\mathbb{R}$ is the bias\n",
    "\n",
    "### Example: House Price Prediction\n",
    "\n",
    "For example, to predict house prices, we might use features like:\n",
    "- House size\n",
    "- Location\n",
    "- Number of rooms\n",
    "\n",
    "The model will learn the weights and bias from known examples, then use them to predict prices for houses with unknown prices but known features.\n",
    "\n",
    "### Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "To build a model that accurately estimates $y$ given input features, we need to find the optimal weights and bias. Since they're unknown, we calculate them using a set of known examples.\n",
    "\n",
    "First, we define a **loss function** $L(\\mathbf{w}, b)$ that determines how good a model's performance is. The loss function is a function of the learned parameters. We want to find the optimal values that minimize this function.\n",
    "\n",
    "A common loss function is the **Mean Squared Error (MSE)**, which calculates the squared difference between predicted and actual values:\n",
    "\n",
    "$$L^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "For $n$ known examples, we sum all these differences:\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2$$\n",
    "\n",
    "To find the optimal parameters, we need to minimize the loss function:\n",
    "\n",
    "$$\\hat{\\mathbf{w}}, \\hat{b} \\equiv \\hat{\\boldsymbol{\\theta}} = \\arg\\min L(\\mathbf{w}, b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-Form Solution for Scalar Case\n",
    "\n",
    "For the scalar case where $d = 1$ (single feature), the linear relationship is $\\hat{y} = wx + b$.\n",
    "\n",
    "The loss function becomes:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - wx^{(i)} - b)^2$$\n",
    "\n",
    "To find the optimum, we take derivatives and set them to zero:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - wx^{(i)} - b) \\cdot (-x^{(i)}) = 0$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - wx^{(i)} - b) \\cdot (-1) = 0$$\n",
    "\n",
    "This gives us a system of linear equations:\n",
    "\n",
    "$$w\\sum x_i^2 + b\\sum x_i = \\sum y_i x_i$$\n",
    "\n",
    "$$w\\sum x_i + bn = \\sum y_i$$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\\begin{bmatrix} \\sum x_i^2 & \\sum x_i \\\\ \\sum x_i & n \\end{bmatrix} \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\sum y_i x_i \\\\ \\sum y_i \\end{bmatrix}$$\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate synthetic data for demonstration\n",
    "def generate_linear_data(n=100, w_true=2.0, b_true=1.0, noise_std=0.5):\n",
    "    \"\"\"Generate synthetic linear data with noise.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n, 1) * 2\n",
    "    y = w_true * X.flatten() + b_true + np.random.randn(n) * noise_std\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_linear_data(n=100, w_true=2.0, b_true=1.0, noise_std=0.5)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
    "plt.xlabel('X (feature)')\n",
    "plt.ylabel('y (target)')\n",
    "plt.title('Synthetic Linear Regression Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Sample values: X[0]={X[0,0]:.3f}, y[0]={y[0]:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Closed-form solution for linear regression (scalar case)\n",
    "def linear_regression_closed_form(X, y):\n",
    "    \"\"\"\n",
    "    Solve linear regression using closed-form solution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, 1)\n",
    "        Input features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w : float\n",
    "        Optimal weight\n",
    "    b : float\n",
    "        Optimal bias\n",
    "    \"\"\"\n",
    "    X = X.flatten()\n",
    "    n = len(X)\n",
    "    \n",
    "    # Build the system of equations\n",
    "    A = np.array([\n",
    "        [np.sum(X**2), np.sum(X)],\n",
    "        [np.sum(X), n]\n",
    "    ])\n",
    "    \n",
    "    b_vec = np.array([\n",
    "        np.sum(y * X),\n",
    "        np.sum(y)\n",
    "    ])\n",
    "    \n",
    "    # Solve the system\n",
    "    solution = np.linalg.solve(A, b_vec)\n",
    "    w, b = solution[0], solution[1]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Fit the model\n",
    "w_optimal, b_optimal = linear_regression_closed_form(X, y)\n",
    "\n",
    "print(f\"Optimal parameters:\")\n",
    "print(f\"  w (weight) = {w_optimal:.4f}\")\n",
    "print(f\"  b (bias) = {b_optimal:.4f}\")\n",
    "print(f\"\\nTrue parameters were: w=2.0, b=1.0\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the fitted line\n",
    "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_pred = w_optimal * X_plot.flatten() + b_optimal\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
    "plt.plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Fitted line: y = {w_optimal:.2f}x + {b_optimal:.2f}')\n",
    "plt.xlabel('X (feature)')\n",
    "plt.ylabel('y (target)')\n",
    "plt.title('Linear Regression: Closed-Form Solution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate MSE\n",
    "y_pred_all = w_optimal * X.flatten() + b_optimal\n",
    "mse = np.mean((y - y_pred_all)**2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Solution\n",
    "\n",
    "For convenience, we can incorporate the bias into the weight vector:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b = (\\mathbf{w}^T, b) \\begin{pmatrix} \\mathbf{x} \\\\ 1 \\end{pmatrix} = \\tilde{\\mathbf{w}}^T \\tilde{\\mathbf{x}}$$\n",
    "\n",
    "where $\\tilde{\\mathbf{w}}, \\tilde{\\mathbf{x}} \\in \\mathbb{R}^{d+1}$.\n",
    "\n",
    "For the vector case with $n$ examples, we have:\n",
    "- $X_{n \\times (d+1)} = (\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)})^T$\n",
    "- $Y = (y^{(1)}, \\ldots, y^{(n)})^T$\n",
    "\n",
    "The loss function becomes:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)})^2$$\n",
    "\n",
    "Minimizing this is equivalent to minimizing $\\|Y - X\\mathbf{w}\\|^2$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)}) \\cdot (-\\mathbf{x}^{(i)}) = 0$$\n",
    "\n",
    "$$\\rightarrow X^T(X\\mathbf{w} - Y) = 0$$\n",
    "\n",
    "$$\\hat{\\mathbf{w}} = (X^T X)^{-1} X^T Y$$\n",
    "\n",
    "This is known as the **normal equation**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vectorized closed-form solution\n",
    "def linear_regression_vectorized(X, y):\n",
    "    \"\"\"\n",
    "    Solve linear regression using vectorized normal equation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Input features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w : array, shape (n_features + 1,)\n",
    "        Optimal weights (including bias as last element)\n",
    "    \"\"\"\n",
    "    # Add bias column (column of ones)\n",
    "    n_samples = X.shape[0]\n",
    "    X_augmented = np.hstack([X, np.ones((n_samples, 1))])\n",
    "    \n",
    "    # Normal equation: w = (X^T X)^(-1) X^T y\n",
    "    w = np.linalg.solve(X_augmented.T @ X_augmented, X_augmented.T @ y)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Test with multiple features\n",
    "np.random.seed(42)\n",
    "X_multi = np.random.randn(100, 2) * 2\n",
    "w_true_multi = np.array([2.0, -1.5])\n",
    "b_true_multi = 1.0\n",
    "y_multi = X_multi @ w_true_multi + b_true_multi + np.random.randn(100) * 0.5\n",
    "\n",
    "# Fit the model\n",
    "w_optimal_multi = linear_regression_vectorized(X_multi, y_multi)\n",
    "\n",
    "print(\"Multi-feature linear regression:\")\n",
    "print(f\"Optimal weights: {w_optimal_multi[:-1]}\")\n",
    "print(f\"Optimal bias: {w_optimal_multi[-1]}\")\n",
    "print(f\"\\nTrue weights: {w_true_multi}\")\n",
    "print(f\"True bias: {b_true_multi}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Gradient Descent\n",
    "\n",
    "For complex problems where the closed-form solution is not feasible, we use **Gradient Descent (GD)**. This is an iterative optimization method that finds the minimum of the loss function.\n",
    "\n",
    "### How Gradient Descent Works\n",
    "\n",
    "1. Start with an initial guess for the parameters\n",
    "2. At each step, move in the direction of the negative gradient\n",
    "3. The gradient is the derivative of the function, indicating the direction of steepest ascent\n",
    "4. Moving in the negative gradient direction gives the steepest descent\n",
    "5. To avoid getting stuck at saddle points, we add a **learning rate** ($\\epsilon$)\n",
    "\n",
    "Formally, for an initial guess $\\boldsymbol{\\theta}^{(0)}$, at each step we update:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\epsilon \\cdot \\frac{\\partial}{\\partial \\boldsymbol{\\theta}^{(t)}} L(\\boldsymbol{\\theta}^{(t)})$$\n",
    "\n",
    "This process continues iteratively until convergence. Since the problem is convex, convergence to the minimum is guaranteed, though it can be slow if the learning rate is too large or too small.\n",
    "\n",
    "### Gradient Descent for Linear Regression\n",
    "\n",
    "For linear regression with MSE loss:\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2$$\n",
    "\n",
    "The gradients are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = -\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b) \\cdot x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the linear regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.random.randn(n_features)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            y_pred = X @ self.w + self.b\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = -(1/n_samples) * X.T @ (y - y_pred)\n",
    "            db = -(1/n_samples) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Store loss\n",
    "            loss = np.mean((y - y_pred)**2) / 2\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.w + self.b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train using gradient descent\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    model = LinearRegressionGD(learning_rate=lr, n_iterations=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot loss history\n",
    "    axes[idx].plot(model.loss_history)\n",
    "    axes[idx].set_title(f'Learning Rate = {lr}')\n",
    "    axes[idx].set_xlabel('Iteration')\n",
    "    axes[idx].set_ylabel('Loss')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    axes[idx].text(0.5, 0.95, f'MSE: {mse:.4f}\\nw: {model.w[0]:.4f}, b: {model.b:.4f}', \n",
    "                   transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Gradient Descent: Effect of Learning Rate', y=1.02, fontsize=14)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the optimization path in parameter space (for scalar case)\n",
    "def plot_optimization_path(X, y, learning_rate=0.01, n_iterations=100):\n",
    "    \"\"\"Visualize gradient descent optimization path.\"\"\"\n",
    "    # Create a grid of parameter values\n",
    "    w_range = np.linspace(-1, 5, 100)\n",
    "    b_range = np.linspace(-2, 4, 100)\n",
    "    W, B = np.meshgrid(w_range, b_range)\n",
    "    \n",
    "    # Compute loss for each point\n",
    "    Loss = np.zeros_like(W)\n",
    "    for i in range(len(w_range)):\n",
    "        for j in range(len(b_range)):\n",
    "            y_pred = W[j, i] * X.flatten() + B[j, i]\n",
    "            Loss[j, i] = np.mean((y - y_pred)**2) / 2\n",
    "    \n",
    "    # Run gradient descent\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    w_path = [w]\n",
    "    b_path = [b]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        y_pred = w * X.flatten() + b\n",
    "        dw = -np.mean((y - y_pred) * X.flatten())\n",
    "        db = -np.mean(y - y_pred)\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        w_path.append(w)\n",
    "        b_path.append(b)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    contour = plt.contour(W, B, Loss, levels=20, alpha=0.6)\n",
    "    plt.clabel(contour, inline=True, fontsize=8)\n",
    "    plt.plot(w_path, b_path, 'r-o', markersize=4, linewidth=2, label='GD Path')\n",
    "    plt.plot(w_path[0], b_path[0], 'go', markersize=10, label='Start')\n",
    "    plt.plot(w_path[-1], b_path[-1], 'ro', markersize=10, label='End')\n",
    "    plt.plot(w_optimal, b_optimal, 'b*', markersize=15, label='Optimal (closed-form)')\n",
    "    plt.xlabel('Weight (w)')\n",
    "    plt.ylabel('Bias (b)')\n",
    "    plt.title('Gradient Descent Optimization Path')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_optimization_path(X_train, y_train, learning_rate=0.05, n_iterations=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Regularization and Cross Validation\n",
    "\n",
    "One of the main challenges in regression is **overfitting** - when a model performs well on training data but poorly on new, unseen data (test set).\n",
    "\n",
    "Models can suffer from two types of bias:\n",
    "- **Overfitting**: The model fits too closely to training data, often using a high-order model with high variance\n",
    "- **Underfitting**: The model is too simple and cannot capture the underlying pattern\n",
    "\n",
    "### Regularization\n",
    "\n",
    "**Regularization** helps prevent overfitting by adding a penalty term to the loss function. Common regularization techniques include:\n",
    "\n",
    "1. **L2 Regularization (Ridge Regression)**:\n",
    "   $$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "2. **L1 Regularization (Lasso Regression)**:\n",
    "   $$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2 + \\lambda\\|\\mathbf{w}\\|_1$$\n",
    "\n",
    "where $\\lambda$ is the regularization strength.\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "**Cross-validation** is a technique to assess model performance and select hyperparameters. The most common method is **k-fold cross-validation**, where the data is split into k folds, and the model is trained k times, each time using k-1 folds for training and 1 fold for validation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"\n",
    "    Ridge Regression (L2 Regularization) using Gradient Descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, lambda_reg=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the ridge regression model.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.random.randn(n_features)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # Gradient descent with L2 regularization\n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            y_pred = X @ self.w + self.b\n",
    "            \n",
    "            # Compute gradients (with L2 penalty)\n",
    "            dw = -(1/n_samples) * X.T @ (y - y_pred) + self.lambda_reg * self.w\n",
    "            db = -(1/n_samples) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Store loss (with regularization term)\n",
    "            mse = np.mean((y - y_pred)**2) / 2\n",
    "            reg_term = (self.lambda_reg / 2) * np.sum(self.w**2)\n",
    "            loss = mse + reg_term\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.w + self.b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate overfitting and regularization\n",
    "# Generate data with some noise\n",
    "np.random.seed(42)\n",
    "X_poly = np.linspace(-3, 3, 30).reshape(-1, 1)\n",
    "y_poly = 0.5 * X_poly.flatten()**2 + X_poly.flatten() + 1 + np.random.randn(30) * 0.5\n",
    "\n",
    "# Split data\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "    X_poly, y_poly, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Try different regularization strengths\n",
    "lambda_values = [0.0, 0.1, 1.0, 10.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lambda_reg in enumerate(lambda_values):\n",
    "    model = RidgeRegression(learning_rate=0.01, n_iterations=2000, lambda_reg=lambda_reg)\n",
    "    model.fit(X_train_poly, y_train_poly)\n",
    "    \n",
    "    # Plot\n",
    "    X_plot = np.linspace(X_poly.min(), X_poly.max(), 100).reshape(-1, 1)\n",
    "    y_pred_plot = model.predict(X_plot)\n",
    "    \n",
    "    axes[idx].scatter(X_train_poly, y_train_poly, alpha=0.6, label='Train', color='blue')\n",
    "    axes[idx].scatter(X_test_poly, y_test_poly, alpha=0.6, label='Test', color='red', marker='x')\n",
    "    axes[idx].plot(X_plot, y_pred_plot, 'g-', linewidth=2, label='Model')\n",
    "    axes[idx].set_title(f'λ = {lambda_reg}')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_mse = mean_squared_error(y_train_poly, model.predict(X_train_poly))\n",
    "    test_mse = mean_squared_error(y_test_poly, model.predict(X_test_poly))\n",
    "    axes[idx].text(0.05, 0.95, f'Train MSE: {train_mse:.3f}\\nTest MSE: {test_mse:.3f}', \n",
    "                   transform=axes[idx].transAxes, ha='left', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Effect of L2 Regularization (Ridge Regression)', y=1.02, fontsize=14)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def k_fold_cross_validation(X, y, model_class, k=5, **model_params):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Targets\n",
    "    model_class : class\n",
    "        Model class to instantiate\n",
    "    k : int\n",
    "        Number of folds\n",
    "    **model_params : dict\n",
    "        Parameters to pass to model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cv_scores : list\n",
    "        List of MSE scores for each fold\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        cv_scores.append(mse)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Test different lambda values using cross-validation\n",
    "lambda_candidates = [0.0, 0.01, 0.1, 1.0, 10.0]\n",
    "cv_results = []\n",
    "\n",
    "for lambda_reg in lambda_candidates:\n",
    "    scores = k_fold_cross_validation(\n",
    "        X_poly, y_poly, \n",
    "        RidgeRegression, \n",
    "        k=5,\n",
    "        learning_rate=0.01,\n",
    "        n_iterations=2000,\n",
    "        lambda_reg=lambda_reg\n",
    "    )\n",
    "    cv_results.append({\n",
    "        'lambda': lambda_reg,\n",
    "        'mean_mse': np.mean(scores),\n",
    "        'std_mse': np.std(scores),\n",
    "        'scores': scores\n",
    "    })\n",
    "\n",
    "# Plot results\n",
    "means = [r['mean_mse'] for r in cv_results]\n",
    "stds = [r['std_mse'] for r in cv_results]\n",
    "lambdas = [r['lambda'] for r in cv_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(lambdas, means, yerr=stds, marker='o', capsize=5, capthick=2)\n",
    "plt.xlabel('Regularization Strength (λ)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('5-Fold Cross-Validation: Selecting Optimal λ')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best lambda\n",
    "best_idx = np.argmin(means)\n",
    "print(f\"Best λ: {lambdas[best_idx]}\")\n",
    "print(f\"Mean CV MSE: {means[best_idx]:.4f} ± {stds[best_idx]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Linear Regression as Classifier\n",
    "\n",
    "Linear regression can also be used for classification tasks. A classification task is defined as follows: given a set of parameters $X = \\{x^{(1)}, \\ldots, x^{(n)}\\}$ belonging to a certain observation, we need to classify it into one of $m$ possible categories $y \\in \\{1, \\ldots, m\\}$.\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "For the case where $m = 2$ (binary classification), we need to map each point to one of two categories. Using linear regression, we can perform a mapping from $\\mathbb{R}$ to $\\{0, 1\\}$, where each point in space is mapped to one of two possible values.\n",
    "\n",
    "We use a threshold $T = 0.5$, and for a new point $x^{(n)}$, we compare the value $w^T x^{(n)} + b$ with the threshold value. If the new point satisfies $w^T x^{(n)} + b < 0.5$, then the new point belongs to category 1. Otherwise, the new point belongs to category 0:\n",
    "\n",
    "$$y = \\text{sign}(w^T x^{(n)} + b - 0.5) = \\begin{cases} 1 & w^T x^{(n)} + b > 0.5 \\\\ 0 & w^T x^{(n)} + b < 0.5 \\end{cases}$$\n",
    "\n",
    "The choice of threshold $T = 0.5$ comes from the fact that there are two categories $\\{0, 1\\}$, and the threshold is set to be the midpoint between them.\n",
    "\n",
    "### Example: House Floor Classification\n",
    "\n",
    "For example: given $n$ houses, each with a known price and whether it has one or two floors. We want to find the relationship between the price and the number of floors and determine for a given house price what the number of floors is. In this case, there are 2 categories: $y \\in \\{0, 1\\} = \\{\\text{1 floor}, \\text{2 floors}\\}$.\n",
    "\n",
    "A way to do this is to use linear regression and check whether $w \\cdot \\text{price} + b$ is greater than 0.5 or less than it, where $(w, b)$ are the parameters of the linear regression.\n",
    "\n",
    "### Loss Function for Classification\n",
    "\n",
    "For $n$ known points – $(x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)}), y^{(i)} \\in \\{0, 1\\}$, the loss function is:\n",
    "\n",
    "$$L(\\theta) = \\sum_{i=1}^{n} \\mathbf{1}\\{y^{(i)} \\neq \\text{sign}(w^T x^{(i)} + b - 0.5)\\}$$\n",
    "\n",
    "The function $L(\\theta)$ contains a set of parameters – $\\theta = (\\mathbf{w}, b)$. Since the derivative of the function with respect to each parameter $w_i$ does not depend only on that parameter, it is difficult to find the optimal $\\theta$ that minimizes $L(\\theta)$.\n",
    "\n",
    "### Multi-Class Classification\n",
    "\n",
    "We can extend the classifier also for cases where there are more than two categories (multi-class). The training set will look like in the binary case, while $y^{(i)}$ now contains $m$ categories: $y^{(i)} \\in \\{1, \\ldots, m\\}$.\n",
    "\n",
    "Instead of a single linear separator, there are $m$ linear separators, separating between different regions. To calculate the lines, we use a process called \"one versus all\", where each time we isolate one category and find the separation line between it and the rest of the categories.\n",
    "\n",
    "The set of parameters will be the set consisting of all the parameters of the regression: $\\theta = \\{w_1, b_1, \\ldots, w_m, b_m\\}$.\n",
    "\n",
    "In this case, a new point will be classified to a category according to:\n",
    "\n",
    "$$y(x) = \\arg\\max_i (w_1^T x + b_1, \\ldots, w_m^T x + b_m)$$\n",
    "\n",
    "And each region will be defined according to:\n",
    "\n",
    "$$R_i = \\{x | y(x) = i\\}$$\n",
    "\n",
    "The loss function will be:\n",
    "\n",
    "$$L(\\theta) = \\sum_{i=1}^{n} \\mathbf{1}\\{y^{(i)} \\neq \\hat{y}^{(i)}\\} \\text{ s.t. } \\hat{y}^{(i)} = \\arg\\max_i (w_i^T x^{(i)} + b_i)$$\n",
    "\n",
    "To find the optimal parameters that minimize the loss function:\n",
    "\n",
    "$$\\hat{\\theta} = \\arg\\min_{\\theta} L(\\theta)$$\n",
    "\n",
    "Also in this case, since the derivative of the loss function with respect to each parameter is not smooth, it is difficult to find the optimal $\\theta$ that brings $L(\\theta)$ to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Linear Regression as Binary Classifier\n",
    "def linear_classifier(X, y):\n",
    "    \"\"\"\n",
    "    Fit linear regression and always return w as a 1-D ndarray and b as scalar.\n",
    "    \"\"\"\n",
    "    w, b = linear_regression_closed_form(X, y)\n",
    "    w = np.atleast_1d(w)  # convert scalar to array if needed\n",
    "    return w, b\n",
    "\n",
    "def predict_classifier(X, w, b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict binary class using linear regression weights that may be scalar or array.\n",
    "    \"\"\"\n",
    "    w = np.atleast_1d(w)\n",
    "    # If X is (n_samples, 1) and w is (1,), X @ w works; result is 1D after flattening\n",
    "    preds = (X @ w).flatten() + b\n",
    "    return (preds > threshold).astype(int)\n",
    "\n",
    "# Example: House floor classification based on price\n",
    "np.random.seed(42)\n",
    "# Generate synthetic data: price vs number of floors (0=1 floor, 1=2 floors)\n",
    "prices = np.random.uniform(100, 500, 100).reshape(-1, 1)\n",
    "# Higher prices more likely to have 2 floors\n",
    "floors = ((prices.flatten() > 300) + np.random.randn(100) * 0.3 > 0.5).astype(int)\n",
    "\n",
    "# Fit linear classifier\n",
    "w_clf, b_clf = linear_classifier(prices, floors)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(prices[floors == 0], floors[floors == 0], alpha=0.6, label='1 Floor', s=50)\n",
    "plt.scatter(prices[floors == 1], floors[floors == 1], alpha=0.6, label='2 Floors', s=50)\n",
    "\n",
    "# Plot decision boundary\n",
    "price_range = np.linspace(prices.min(), prices.max(), 100).reshape(-1, 1)\n",
    "w_val = np.squeeze(np.atleast_1d(w_clf))\n",
    "decision_line = w_val * price_range.flatten() + b_clf\n",
    "plt.plot(price_range, decision_line, 'r-', linewidth=2, label=f'Linear: {w_val:.4f}*price + {b_clf:.4f}')\n",
    "plt.axhline(y=0.5, color='g', linestyle='--', alpha=0.5, label='Threshold (0.5)')\n",
    "plt.xlabel('House Price')\n",
    "plt.ylabel('Number of Floors')\n",
    "plt.title('Linear Regression as Binary Classifier')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_clf = predict_classifier(prices, w_clf, b_clf)\n",
    "accuracy = accuracy_score(floors, y_pred_clf)\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Decision boundary: price = {(0.5 - b_clf) / w_clf[0]:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to avoid overflow\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigma_z, 'b-', linewidth=2, label='σ(z) = 1/(1 + e^(-z))')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision boundary (0.5)')\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('σ(z)')\n",
    "plt.title('Sigmoid (Logistic) Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Cross Entropy and Gradient Descent\n",
    "\n",
    "To find the optimal parameters $\\theta = (w, b)$ given $n$ examples, we replace the mean squared error criterion with another criterion for minimizing the loss function – **Cross Entropy**. This criterion says that we should minimize the log of all examples (the expression comes from maximum likelihood estimation):\n",
    "\n",
    "$$-\\log P(Y | X; \\theta) = -\\frac{1}{n}\\sum_{i=1}^{n} \\log p(y^{(i)} | x^{(i)}; \\theta) = L(\\theta)$$\n",
    "\n",
    "In practice, we need to find the set of parameters $\\hat{\\theta}$ that brings this expression to a minimum: $\\hat{\\theta} = \\arg\\min_{\\theta} L(\\theta)$.\n",
    "\n",
    "### Derivative of the Sigmoid\n",
    "\n",
    "To calculate the expression, we first need to develop the expression for the derivative of the sigmoid:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}} \\rightarrow \\frac{\\partial \\sigma(z)}{\\partial z} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}} = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "Also, $p(y = 0 | x; w, b) = 1 - p(y = 1 | x; \\theta) = 1 - \\sigma(\\theta)$, so:\n",
    "\n",
    "$$\\frac{\\partial (1 - \\sigma(z))}{\\partial z} = -\\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "The derivatives of the log sigmoid are:\n",
    "\n",
    "$$\\frac{\\partial \\log \\sigma(z)}{\\partial z} = \\frac{1}{\\sigma(z)} \\cdot \\frac{\\partial \\sigma(z)}{\\partial z} = (1 - \\sigma(z))$$\n",
    "\n",
    "$$\\frac{\\partial \\log (1 - \\sigma(z))}{\\partial z} = \\frac{1}{1 - \\sigma(z)} \\cdot \\frac{\\partial (1 - \\sigma(z))}{\\partial z} = -\\sigma(z)$$\n",
    "\n",
    "Now note that the derivative of $\\log p(y = 1 | z)$ is $1 - \\sigma(z) = y - \\sigma(z)$, and the derivative of $\\log p(y = 0 | z)$ is $0 - \\sigma(z) = y^{(i)} - \\sigma(z)$. Therefore, if $y \\in \\{0, 1\\}$, we can write in short:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} \\log p(y^{(i)} | z) = y^{(i)} - \\sigma(z)$$\n",
    "\n",
    "For logistic regression, we need:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} \\log p(y^{(i)} | x^{(i)}; \\theta)$$\n",
    "\n",
    "And according to the previous development, we can write this as:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} \\log p(y^{(i)} | x^{(i)}; \\theta) = (y^{(i)} - \\sigma(w^T x + b)) \\cdot \\frac{\\partial}{\\partial w}(w^T x + b) = (y^{(i)} - p(y^{(i)} = 1 | x^{(i)}; \\theta)) \\cdot x^{(i)}$$\n",
    "\n",
    "Now to find $\\arg\\min_{\\theta} L(\\theta)$ and substitute:\n",
    "\n",
    "$$\\frac{\\partial L(\\theta)}{\\partial \\theta} = -\\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\log p(y^{(i)} | x^{(i)}; \\theta) = -\\frac{1}{n}\\sum_{i=1}^{n} (y^{(i)} - \\sigma(w^T x + b)) x^{(i)} = -\\frac{1}{n}\\sum_{i=1}^{n} (y^{(i)} - p(y^{(i)} = 1 | \\theta; x)) x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Optimization\n",
    "\n",
    "Similar to linear regression, here too the calculation of the optimal value of $\\hat{\\theta}$ will be iterative using **gradient descent**:\n",
    "\n",
    "$$\\hat{\\theta}^{(t+1)} = \\hat{\\theta}^{(t)} - \\epsilon \\cdot \\frac{\\partial}{\\partial \\theta^{(t)}} L(\\theta)$$\n",
    "\n",
    "where $\\epsilon$ is the **learning rate** parameter. Since the loss function $L(\\theta)$ is convex, convergence to $\\hat{\\theta}$ is guaranteed.\n",
    "\n",
    "### Batch vs Mini-Batch vs Stochastic Gradient Descent\n",
    "\n",
    "When the dataset is large, and calculating the gradient for all the data is computationally expensive, we can calculate the gradient for a subset of the data and perform the update according to the direction of the resulting gradient. For example, we can choose a single random point each time. This choice is called **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "SGD can cause large variance as it progresses, so it is better to take a number of points. Calculating the gradient in this method is called **mini-batch learning** (as opposed to batch learning which is performed on all the data).\n",
    "\n",
    "Formally, the gradient in the mini-batch method is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[-\\frac{1}{|V|}\\sum_{i \\in V} \\log p(y^{(i)} | x^{(i)}; \\theta)\\right] \\approx \\frac{\\partial}{\\partial \\theta} \\left[-\\frac{1}{n}\\sum_{i=1}^{n} \\log p(y^{(i)} | x^{(i)}; \\theta)\\right]$$\n",
    "\n",
    "Although each step is an approximation of the gradient, the calculation is very fast relative to the exact gradient, and this is a significant advantage of this method over batch learning. In addition, it can be proven that in this method we get an unbiased estimator of the true gradient.\n",
    "\n",
    "### Regularization in Logistic Regression\n",
    "\n",
    "As in linear regression, in logistic regression there is also the issue of regularization, which is intended to prevent the model from giving too much weight to each point (overfitting) or not representing the data well enough (underfitting). For this purpose, we can add a constraint to the parameters:\n",
    "\n",
    "$$L(\\theta) = -\\frac{1}{n}\\sum_{i=1}^{n} \\log p(y^{(i)} | x^{(i)}; \\theta) + \\lambda \\|\\theta\\|^2$$\n",
    "\n",
    "And then the derivative is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = -\\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\log p(y^{(i)} | x^{(i)}; \\theta) + 2\\lambda \\|\\theta\\|$$\n",
    "\n",
    "The optimal parameter $\\lambda$ is found using **Cross Validation** on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 SoftMax Regression – Multi Class Logistic Regression\n",
    "\n",
    "As in linear regression, in logistic regression we can also generalize the model to a **multi-class** case (where there are more than two categories). Also in the generalization to the multi-category case, we need to map each category to a probability in $[0, 1]$, and for this purpose we use **SoftMax** instead of sigmoid.\n",
    "\n",
    "SoftMax is a function applied to a sequence, and it is defined as:\n",
    "\n",
    "$$\\text{SoftMax}(z_1, \\ldots, z_n) = \\left(\\frac{e^{z_1}}{\\sum_{j=1}^{n} e^{z_j}}, \\ldots, \\frac{e^{z_n}}{\\sum_{j=1}^{n} e^{z_j}}\\right)$$\n",
    "\n",
    "The numerator calculates an exponent to the power of $z_i$, and the denominator normalizes the result, so that the sum of all elements after the function is 1.\n",
    "\n",
    "In the case where there are multiple categories – there are multiple separation lines, and for each one there is a set of parameters $\\theta$. For a new point, we use SoftMax to give a probability to each category:\n",
    "\n",
    "$$p(y = i | x; \\theta) = \\text{SoftMax}(w_1^T x + b_1, \\ldots, w_m^T x + b_m)$$\n",
    "\n",
    "To classify, we take the element with the highest probability.\n",
    "\n",
    "Also in this model, the loss function will be **Cross Entropy**:\n",
    "\n",
    "$$L(\\theta) = -\\frac{1}{n}\\sum_{i=1}^{n} \\log p(y^{(i)} | x^{(i)}; \\theta)$$\n",
    "\n",
    "We calculate the derivative with respect to $\\theta_i$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i} \\log p(y^{(i)} = s | x^{(i)}; \\theta) = \\frac{\\partial}{\\partial \\theta_i} \\log \\frac{\\exp(w_s^T x + b_s)}{\\sum_{j=1}^{m} \\exp(w_j^T x + b_j)} = \\frac{\\partial}{\\partial \\theta_i} \\left[w_s^T x + b_s - \\log \\sum_{j=1}^{m} \\exp(w_j^T x + b_j)\\right]$$\n",
    "\n",
    "$$= \\mathbf{1}\\{i = s\\} x - \\frac{\\exp(w_i^T x + b_i) x}{\\sum_{j=1}^{m} \\exp(w_j^T x + b_j)} = (\\mathbf{1}\\{i = s\\} - p(y = i | x)) x$$\n",
    "\n",
    "where the notation $\\mathbf{1}\\{i = s\\}$ is 1 if $i = s$ and 0 otherwise.\n",
    "\n",
    "Now we can substitute the last expression in the derivative of $L(\\theta)$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta_i} = -\\frac{1}{n}\\sum_{t=1}^{n} (\\mathbf{1}\\{y^{(t)} = i\\} - p(y^{(t)} = i | x^{(t)}; \\theta)) x^{(t)}$$\n",
    "\n",
    "To find the optimal $\\theta$ using gradient descent:\n",
    "\n",
    "$$\\theta_i^{(t+1)} = \\theta_i^{(t)} - \\epsilon \\frac{\\partial L}{\\partial \\theta}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SoftMax Regression - Multi-Class Classification\n",
    "def softmax(z):\n",
    "    \"\"\"SoftMax function for multi-class classification.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "class SoftMaxRegression:\n",
    "    \"\"\"\n",
    "    SoftMax Regression for multi-class classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, n_classes=3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_classes = n_classes\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SoftMax regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Class labels (0 to n_classes-1)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W = np.random.randn(n_features, self.n_classes) * 0.01\n",
    "        self.b = np.zeros(self.n_classes)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            z = X @ self.W + self.b\n",
    "            probs = softmax(z)\n",
    "            \n",
    "            # One-hot encode labels\n",
    "            y_one_hot = np.eye(self.n_classes)[y]\n",
    "            \n",
    "            # Compute gradients\n",
    "            dW = -(1/n_samples) * X.T @ (y_one_hot - probs)\n",
    "            db = -(1/n_samples) * np.sum(y_one_hot - probs, axis=0)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Store loss (cross-entropy)\n",
    "            loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-15), axis=1))\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        z = X @ self.W + self.b\n",
    "        return softmax(z)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# Generate multi-class data\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "X_multi = np.random.randn(n_samples, 2) * 2\n",
    "# Create 3 classes\n",
    "y_multi = np.zeros(n_samples, dtype=int)\n",
    "y_multi[X_multi[:, 0] + X_multi[:, 1] > 1] = 1\n",
    "y_multi[X_multi[:, 0] + X_multi[:, 1] < -1] = 2\n",
    "\n",
    "# Train SoftMax regression\n",
    "model_softmax = SoftMaxRegression(learning_rate=0.1, n_iterations=1000, n_classes=3)\n",
    "model_softmax.fit(X_multi, y_multi)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Data and decision boundaries\n",
    "plt.subplot(1, 2, 1)\n",
    "h = 0.02\n",
    "x_min, x_max = X_multi[:, 0].min() - 1, X_multi[:, 0].max() + 1\n",
    "y_min, y_max = X_multi[:, 1].min() - 1, X_multi[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = model_softmax.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "for i in range(3):\n",
    "    plt.scatter(X_multi[y_multi == i, 0], X_multi[y_multi == i, 1], \n",
    "                alpha=0.6, label=f'Class {i}', s=50)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SoftMax Regression: Decision Boundaries')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss history\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_softmax.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('SoftMax Regression: Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "y_pred_multi = model_softmax.predict(X_multi)\n",
    "accuracy = accuracy_score(y_multi, y_pred_multi)\n",
    "print(f\"Multi-class Classification Accuracy: {accuracy:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.5 SoftMax Regression as Neural Network\n",
    "\n",
    "The logistic regression method has several advantages: it is relatively easy to train, provides good accuracy for separable data, is stable against overfitting, offers probabilistic classification, and is suitable also for cases where there are more than two categories.\n",
    "\n",
    "However, it has a significant disadvantage – the separation lines of the model are linear. This is a limitation, as in many cases, in order to build a model capable of separating between different categories, we need a non-linear separation mechanism.\n",
    "\n",
    "A common way to build non-linear models is to use deep neural networks, and to understand their concept, we need to understand that neural networks are essentially a generalization of the models we have seen so far.\n",
    "\n",
    "The **Linear regression** problem takes a set of features and multiplies each one by a weight, and then sums all the elements (along with bias) into a single variable that determines what the category of this set is.\n",
    "\n",
    "We can represent the model by the following graphical description:\n",
    "\n",
    "![Linear Regression as Neural Network](https://via.placeholder.com/400x200?text=Linear+Regression+as+Neural+Network)\n",
    "\n",
    "In this description, there are 2 features that form the input, and each one is connected to the output through a weight. There is also a bias, and together with the features multiplied by weights and the bias, we get the output: $F(x) = w^T x + b = w_1 x_1 + w_2 x_2 + b$.\n",
    "\n",
    "Each circle in the figure is called an **artificial neuron** – an element that can receive input, perform a computational operation, and output a result.\n",
    "\n",
    "Logistic regression can be described similarly, where the neurons of the input set are not directly connected to the output but pass through a sigmoid in the binary case or through SoftMax in the case where there are multiple categories:\n",
    "\n",
    "![Logistic Regression as Neural Network](https://via.placeholder.com/400x200?text=Logistic+Regression+as+Neural+Network)\n",
    "\n",
    "In addition to passing through the sigmoid function, there is another difference between the representation of linear regression and the representation of logistic regression: while linear regression provides a single number in the output (hard classifier), logistic regression provides a vector of length equal to the number of categories, where for each category there is a certain probability that the input belongs to it.\n",
    "\n",
    "In the next chapter, we will see that by adding more layers between the input and output, the model that will be obtained will be a mapping of a set of features in a non-linear way to a probability vector in the output. The flexibility of the model will allow dealing with tasks with non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize Linear and Logistic Regression as Neural Networks\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_neural_network_representation(model_type='linear'):\n",
    "    \"\"\"Visualize linear/logistic regression as a simple neural network.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Input layer\n",
    "    input_x = [1, 2]\n",
    "    input_y = [3, 3]\n",
    "    for i, (x, y) in enumerate(zip(input_x, input_y)):\n",
    "        circle = plt.Circle((x, y), 0.3, color='lightblue', ec='black', lw=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, f'$x_{i+1}$', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Output layer\n",
    "    if model_type == 'linear':\n",
    "        output_x = [1.5]\n",
    "        output_y = [1]\n",
    "        output_label = '$y = w^T x + b$'\n",
    "    else:  # logistic\n",
    "        output_x = [1.5]\n",
    "        output_y = [1]\n",
    "        output_label = '$\\\\sigma(w^T x + b)$'\n",
    "    \n",
    "    circle = plt.Circle((output_x[0], output_y[0]), 0.3, color='lightgreen', ec='black', lw=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(output_x[0], output_y[0], output_label, ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Bias\n",
    "    bias_x = 0.5\n",
    "    bias_y = 2\n",
    "    circle = plt.Circle((bias_x, bias_y), 0.2, color='orange', ec='black', lw=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(bias_x, bias_y, '$b$', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Connections\n",
    "    for i, (x_in, y_in) in enumerate(zip(input_x, input_y)):\n",
    "        ax.arrow(x_in, y_in-0.3, output_x[0]-x_in, output_y[0]-(y_in-0.3), \n",
    "                head_width=0.1, head_length=0.1, fc='gray', ec='gray', lw=1.5)\n",
    "        ax.text((x_in + output_x[0])/2, (y_in + output_y[0])/2 - 0.2, \n",
    "                f'$w_{i+1}$', fontsize=10, ha='center')\n",
    "    \n",
    "    # Bias connection\n",
    "    ax.arrow(bias_x, bias_y-0.2, output_x[0]-bias_x, output_y[0]-(bias_y-0.2),\n",
    "            head_width=0.1, head_length=0.1, fc='orange', ec='orange', lw=1.5)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{model_type.capitalize()} Regression as Neural Network', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot both representations\n",
    "print(\"Linear Regression as Neural Network:\")\n",
    "plot_neural_network_representation('linear')\n",
    "\n",
    "print(\"\\nLogistic Regression as Neural Network:\")\n",
    "plot_neural_network_representation('logistic')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression for binary classification using Gradient Descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Binary labels (0 or 1)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.random.randn(n_features) * 0.01\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            z = X @ self.w + self.b\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * X.T @ (y_pred - y)\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Store loss (binary cross-entropy)\n",
    "            loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities.\"\"\"\n",
    "        z = X @ self.w + self.b\n",
    "        return self.sigmoid(z)\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict binary labels.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate synthetic binary classification data\n",
    "def generate_classification_data(n=200, n_features=2, random_state=42):\n",
    "    \"\"\"Generate synthetic binary classification data.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create two classes\n",
    "    n_per_class = n // 2\n",
    "    \n",
    "    # Class 0: centered at (-1, -1)\n",
    "    X0 = np.random.randn(n_per_class, n_features) + np.array([-1, -1])\n",
    "    y0 = np.zeros(n_per_class)\n",
    "    \n",
    "    # Class 1: centered at (1, 1)\n",
    "    X1 = np.random.randn(n_per_class, n_features) + np.array([1, 1])\n",
    "    y1 = np.ones(n_per_class)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([y0, y1])\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(n)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate and visualize data\n",
    "X_clf, y_clf = generate_classification_data(n=200, n_features=2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_clf[y_clf == 0, 0], X_clf[y_clf == 0, 1], alpha=0.6, label='Class 0', s=50)\n",
    "plt.scatter(X_clf[y_clf == 1, 0], X_clf[y_clf == 1, 1], alpha=0.6, label='Class 1', s=50)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train logistic regression model\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_lr = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model_lr.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_lr.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Binary Cross-Entropy Loss')\n",
    "plt.title('Logistic Regression: Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "y_pred_clf = model_lr.predict(X_test_clf)\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
    "    \"\"\"Plot decision boundary for 2D classification.\"\"\"\n",
    "    # Create a mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', alpha=0.8, label='Class 0', s=50, edgecolors='black')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', alpha=0.8, label='Class 1', s=50, edgecolors='black')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_clf, y_clf, model_lr, \"Logistic Regression Decision Boundary\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we've covered:\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - Basic concept and mathematical formulation\n",
    "   - Closed-form solution using normal equations\n",
    "   - Gradient descent optimization\n",
    "   - Regularization (Ridge regression)\n",
    "   - Cross-validation for model selection\n",
    "\n",
    "2. **Logistic Regression**:\n",
    "   - Binary classification using sigmoid function\n",
    "   - Binary cross-entropy loss\n",
    "   - Gradient descent for logistic regression\n",
    "   - Decision boundary visualization\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Linear regression** predicts continuous values using a linear relationship between features and target\n",
    "- **Gradient descent** is an iterative optimization method that finds optimal parameters by following the negative gradient\n",
    "- **Regularization** helps prevent overfitting by penalizing large weights\n",
    "- **Cross-validation** is essential for selecting hyperparameters and assessing model performance\n",
    "- **Logistic regression** extends linear models to binary classification using the sigmoid activation function\n",
    "\n",
    "These concepts form the foundation for understanding neural networks, which we'll explore in later chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
