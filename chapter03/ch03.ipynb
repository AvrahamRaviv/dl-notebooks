{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Linear Neural Networks - Regression\n",
        "\n",
        "This notebook covers **Chapter 3** of the Deep Learning in Hebrew book, focusing on regression problems. We'll learn how to build models that can predict continuous values from a given set of examples.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This chapter deals with regression problems - how to build a model from a given set of examples that can predict continuous values. The model will learn the relationship between input features and output values.\n",
        "\n",
        "We'll focus on two main types:\n",
        "1. **Linear Regression** - for predicting continuous values\n",
        "2. **Logistic Regression** - for binary classification problems\n",
        "\n",
        "We'll solve regression problems using simple neural networks, which will serve as a foundation for deeper networks that can handle non-linearly separable data.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Linear Regression](#1-linear-regression)\n",
        "   - 1.1 [The Basic Concept](#11-the-basic-concept)\n",
        "   - 1.2 [Gradient Descent](#12-gradient-descent)\n",
        "   - 1.3 [Regularization and Cross Validation](#13-regularization-and-cross-validation)\n",
        "2. [Logistic Regression](#2-logistic-regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries for this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set matplotlib style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Linear Regression\n",
        "\n",
        "## 1.1 The Basic Concept\n",
        "\n",
        "The simplest model is **linear regression**. This model tries to find a linear relationship between one or more input features and a dependent variable.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "Given independent variables $\\mathbf{x} \\in \\mathbb{R}^d$ and a dependent variable $y \\in \\mathbb{R}$, we model their relationship as:\n",
        "\n",
        "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d + b$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{w} \\in \\mathbb{R}^d$ are the weights (parameters)\n",
        "- $b \\in \\mathbb{R}$ is the bias\n",
        "\n",
        "### Example: House Price Prediction\n",
        "\n",
        "For example, to predict house prices, we might use features like:\n",
        "- House size\n",
        "- Location\n",
        "- Number of rooms\n",
        "\n",
        "The model will learn the weights and bias from known examples, then use them to predict prices for houses with unknown prices but known features.\n",
        "\n",
        "### Loss Function: Mean Squared Error (MSE)\n",
        "\n",
        "To build a model that accurately estimates $y$ given input features, we need to find the optimal weights and bias. Since they're unknown, we calculate them using a set of known examples.\n",
        "\n",
        "First, we define a **loss function** $L(\\mathbf{w}, b)$ that determines how good a model's performance is. The loss function is a function of the learned parameters. We want to find the optimal values that minimize this function.\n",
        "\n",
        "A common loss function is the **Mean Squared Error (MSE)**, which calculates the squared difference between predicted and actual values:\n",
        "\n",
        "$$L^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
        "\n",
        "For $n$ known examples, we sum all these differences:\n",
        "\n",
        "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2$$\n",
        "\n",
        "To find the optimal parameters, we need to minimize the loss function:\n",
        "\n",
        "$$\\hat{\\mathbf{w}}, \\hat{b} \\equiv \\hat{\\boldsymbol{\\theta}} = \\arg\\min L(\\mathbf{w}, b)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closed-Form Solution for Scalar Case\n",
        "\n",
        "For the scalar case where $d = 1$ (single feature), the linear relationship is $\\hat{y} = wx + b$.\n",
        "\n",
        "The loss function becomes:\n",
        "\n",
        "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - wx^{(i)} - b)^2$$\n",
        "\n",
        "To find the optimum, we take derivatives and set them to zero:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - wx^{(i)} - b) \\cdot (-x^{(i)}) = 0$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - wx^{(i)} - b) \\cdot (-1) = 0$$\n",
        "\n",
        "This gives us a system of linear equations:\n",
        "\n",
        "$$w\\sum x_i^2 + b\\sum x_i = \\sum y_i x_i$$\n",
        "\n",
        "$$w\\sum x_i + bn = \\sum y_i$$\n",
        "\n",
        "In matrix form:\n",
        "\n",
        "$$\\begin{bmatrix} \\sum x_i^2 & \\sum x_i \\\\ \\sum x_i & n \\end{bmatrix} \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\sum y_i x_i \\\\ \\sum y_i \\end{bmatrix}$$\n",
        "\n",
        "Let's implement this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data for demonstration\n",
        "def generate_linear_data(n=100, w_true=2.0, b_true=1.0, noise_std=0.5):\n",
        "    \"\"\"Generate synthetic linear data with noise.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(n, 1) * 2\n",
        "    y = w_true * X.flatten() + b_true + np.random.randn(n) * noise_std\n",
        "    return X, y\n",
        "\n",
        "# Generate data\n",
        "X, y = generate_linear_data(n=100, w_true=2.0, b_true=1.0, noise_std=0.5)\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
        "plt.xlabel('X (feature)')\n",
        "plt.ylabel('y (target)')\n",
        "plt.title('Synthetic Linear Regression Data')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Sample values: X[0]={X[0,0]:.3f}, y[0]={y[0]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Closed-form solution for linear regression (scalar case)\n",
        "def linear_regression_closed_form(X, y):\n",
        "    \"\"\"\n",
        "    Solve linear regression using closed-form solution.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples, 1)\n",
        "        Input features\n",
        "    y : array-like, shape (n_samples,)\n",
        "        Target values\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    w : float\n",
        "        Optimal weight\n",
        "    b : float\n",
        "        Optimal bias\n",
        "    \"\"\"\n",
        "    X = X.flatten()\n",
        "    n = len(X)\n",
        "    \n",
        "    # Build the system of equations\n",
        "    A = np.array([\n",
        "        [np.sum(X**2), np.sum(X)],\n",
        "        [np.sum(X), n]\n",
        "    ])\n",
        "    \n",
        "    b_vec = np.array([\n",
        "        np.sum(y * X),\n",
        "        np.sum(y)\n",
        "    ])\n",
        "    \n",
        "    # Solve the system\n",
        "    solution = np.linalg.solve(A, b_vec)\n",
        "    w, b = solution[0], solution[1]\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "# Fit the model\n",
        "w_optimal, b_optimal = linear_regression_closed_form(X, y)\n",
        "\n",
        "print(f\"Optimal parameters:\")\n",
        "print(f\"  w (weight) = {w_optimal:.4f}\")\n",
        "print(f\"  b (bias) = {b_optimal:.4f}\")\n",
        "print(f\"\\nTrue parameters were: w=2.0, b=1.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the fitted line\n",
        "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "y_pred = w_optimal * X_plot.flatten() + b_optimal\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
        "plt.plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Fitted line: y = {w_optimal:.2f}x + {b_optimal:.2f}')\n",
        "plt.xlabel('X (feature)')\n",
        "plt.ylabel('y (target)')\n",
        "plt.title('Linear Regression: Closed-Form Solution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Calculate MSE\n",
        "y_pred_all = w_optimal * X.flatten() + b_optimal\n",
        "mse = np.mean((y - y_pred_all)**2)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorized Solution\n",
        "\n",
        "For convenience, we can incorporate the bias into the weight vector:\n",
        "\n",
        "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b = (\\mathbf{w}^T, b) \\begin{pmatrix} \\mathbf{x} \\\\ 1 \\end{pmatrix} = \\tilde{\\mathbf{w}}^T \\tilde{\\mathbf{x}}$$\n",
        "\n",
        "where $\\tilde{\\mathbf{w}}, \\tilde{\\mathbf{x}} \\in \\mathbb{R}^{d+1}$.\n",
        "\n",
        "For the vector case with $n$ examples, we have:\n",
        "- $X_{n \\times (d+1)} = (\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)})^T$\n",
        "- $Y = (y^{(1)}, \\ldots, y^{(n)})^T$\n",
        "\n",
        "The loss function becomes:\n",
        "\n",
        "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)})^2$$\n",
        "\n",
        "Minimizing this is equivalent to minimizing $\\|Y - X\\mathbf{w}\\|^2$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)}) \\cdot (-\\mathbf{x}^{(i)}) = 0$$\n",
        "\n",
        "$$\\rightarrow X^T(X\\mathbf{w} - Y) = 0$$\n",
        "\n",
        "$$\\hat{\\mathbf{w}} = (X^T X)^{-1} X^T Y$$\n",
        "\n",
        "This is known as the **normal equation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorized closed-form solution\n",
        "def linear_regression_vectorized(X, y):\n",
        "    \"\"\"\n",
        "    Solve linear regression using vectorized normal equation.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Input features\n",
        "    y : array-like, shape (n_samples,)\n",
        "        Target values\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    w : array, shape (n_features + 1,)\n",
        "        Optimal weights (including bias as last element)\n",
        "    \"\"\"\n",
        "    # Add bias column (column of ones)\n",
        "    n_samples = X.shape[0]\n",
        "    X_augmented = np.hstack([X, np.ones((n_samples, 1))])\n",
        "    \n",
        "    # Normal equation: w = (X^T X)^(-1) X^T y\n",
        "    w = np.linalg.solve(X_augmented.T @ X_augmented, X_augmented.T @ y)\n",
        "    \n",
        "    return w\n",
        "\n",
        "# Test with multiple features\n",
        "np.random.seed(42)\n",
        "X_multi = np.random.randn(100, 2) * 2\n",
        "w_true_multi = np.array([2.0, -1.5])\n",
        "b_true_multi = 1.0\n",
        "y_multi = X_multi @ w_true_multi + b_true_multi + np.random.randn(100) * 0.5\n",
        "\n",
        "# Fit the model\n",
        "w_optimal_multi = linear_regression_vectorized(X_multi, y_multi)\n",
        "\n",
        "print(\"Multi-feature linear regression:\")\n",
        "print(f\"Optimal weights: {w_optimal_multi[:-1]}\")\n",
        "print(f\"Optimal bias: {w_optimal_multi[-1]}\")\n",
        "print(f\"\\nTrue weights: {w_true_multi}\")\n",
        "print(f\"True bias: {b_true_multi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Gradient Descent\n",
        "\n",
        "For complex problems where the closed-form solution is not feasible, we use **Gradient Descent (GD)**. This is an iterative optimization method that finds the minimum of the loss function.\n",
        "\n",
        "### How Gradient Descent Works\n",
        "\n",
        "1. Start with an initial guess for the parameters\n",
        "2. At each step, move in the direction of the negative gradient\n",
        "3. The gradient is the derivative of the function, indicating the direction of steepest ascent\n",
        "4. Moving in the negative gradient direction gives the steepest descent\n",
        "5. To avoid getting stuck at saddle points, we add a **learning rate** ($\\epsilon$)\n",
        "\n",
        "Formally, for an initial guess $\\boldsymbol{\\theta}^{(0)}$, at each step we update:\n",
        "\n",
        "$$\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\epsilon \\cdot \\frac{\\partial}{\\partial \\boldsymbol{\\theta}^{(t)}} L(\\boldsymbol{\\theta}^{(t)})$$\n",
        "\n",
        "This process continues iteratively until convergence. Since the problem is convex, convergence to the minimum is guaranteed, though it can be slow if the learning rate is too large or too small.\n",
        "\n",
        "### Gradient Descent for Linear Regression\n",
        "\n",
        "For linear regression with MSE loss:\n",
        "\n",
        "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2$$\n",
        "\n",
        "The gradients are:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_j} = -\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b) \\cdot x_j^{(i)}$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = -\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionGD:\n",
        "    \"\"\"\n",
        "    Linear Regression using Gradient Descent.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the linear regression model using gradient descent.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.w = np.random.randn(n_features)\n",
        "        self.b = 0.0\n",
        "        \n",
        "        # Gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Predictions\n",
        "            y_pred = X @ self.w + self.b\n",
        "            \n",
        "            # Compute gradients\n",
        "            dw = -(1/n_samples) * X.T @ (y - y_pred)\n",
        "            db = -(1/n_samples) * np.sum(y - y_pred)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.w -= self.learning_rate * dw\n",
        "            self.b -= self.learning_rate * db\n",
        "            \n",
        "            # Store loss\n",
        "            loss = np.mean((y - y_pred)**2) / 2\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        return X @ self.w + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train using gradient descent\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Try different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, lr in enumerate(learning_rates):\n",
        "    model = LinearRegressionGD(learning_rate=lr, n_iterations=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Plot loss history\n",
        "    axes[idx].plot(model.loss_history)\n",
        "    axes[idx].set_title(f'Learning Rate = {lr}')\n",
        "    axes[idx].set_xlabel('Iteration')\n",
        "    axes[idx].set_ylabel('Loss')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    axes[idx].text(0.5, 0.95, f'MSE: {mse:.4f}\\nw: {model.w[0]:.4f}, b: {model.b:.4f}', \n",
        "                   transform=axes[idx].transAxes, ha='center', va='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Gradient Descent: Effect of Learning Rate', y=1.02, fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the optimization path in parameter space (for scalar case)\n",
        "def plot_optimization_path(X, y, learning_rate=0.01, n_iterations=100):\n",
        "    \"\"\"Visualize gradient descent optimization path.\"\"\"\n",
        "    # Create a grid of parameter values\n",
        "    w_range = np.linspace(-1, 5, 100)\n",
        "    b_range = np.linspace(-2, 4, 100)\n",
        "    W, B = np.meshgrid(w_range, b_range)\n",
        "    \n",
        "    # Compute loss for each point\n",
        "    Loss = np.zeros_like(W)\n",
        "    for i in range(len(w_range)):\n",
        "        for j in range(len(b_range)):\n",
        "            y_pred = W[j, i] * X.flatten() + B[j, i]\n",
        "            Loss[j, i] = np.mean((y - y_pred)**2) / 2\n",
        "    \n",
        "    # Run gradient descent\n",
        "    w = 0.0\n",
        "    b = 0.0\n",
        "    w_path = [w]\n",
        "    b_path = [b]\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        y_pred = w * X.flatten() + b\n",
        "        dw = -np.mean((y - y_pred) * X.flatten())\n",
        "        db = -np.mean(y - y_pred)\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "        w_path.append(w)\n",
        "        b_path.append(b)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    contour = plt.contour(W, B, Loss, levels=20, alpha=0.6)\n",
        "    plt.clabel(contour, inline=True, fontsize=8)\n",
        "    plt.plot(w_path, b_path, 'r-o', markersize=4, linewidth=2, label='GD Path')\n",
        "    plt.plot(w_path[0], b_path[0], 'go', markersize=10, label='Start')\n",
        "    plt.plot(w_path[-1], b_path[-1], 'ro', markersize=10, label='End')\n",
        "    plt.plot(w_optimal, b_optimal, 'b*', markersize=15, label='Optimal (closed-form)')\n",
        "    plt.xlabel('Weight (w)')\n",
        "    plt.ylabel('Bias (b)')\n",
        "    plt.title('Gradient Descent Optimization Path')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_optimization_path(X_train, y_train, learning_rate=0.05, n_iterations=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Regularization and Cross Validation\n",
        "\n",
        "One of the main challenges in regression is **overfitting** - when a model performs well on training data but poorly on new, unseen data (test set).\n",
        "\n",
        "Models can suffer from two types of bias:\n",
        "- **Overfitting**: The model fits too closely to training data, often using a high-order model with high variance\n",
        "- **Underfitting**: The model is too simple and cannot capture the underlying pattern\n",
        "\n",
        "### Regularization\n",
        "\n",
        "**Regularization** helps prevent overfitting by adding a penalty term to the loss function. Common regularization techniques include:\n",
        "\n",
        "1. **L2 Regularization (Ridge Regression)**:\n",
        "   $$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2$$\n",
        "\n",
        "2. **L1 Regularization (Lasso Regression)**:\n",
        "   $$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2}(y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b)^2 + \\lambda\\|\\mathbf{w}\\|_1$$\n",
        "\n",
        "where $\\lambda$ is the regularization strength.\n",
        "\n",
        "### Cross Validation\n",
        "\n",
        "**Cross-validation** is a technique to assess model performance and select hyperparameters. The most common method is **k-fold cross-validation**, where the data is split into k folds, and the model is trained k times, each time using k-1 folds for training and 1 fold for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RidgeRegression:\n",
        "    \"\"\"\n",
        "    Ridge Regression (L2 Regularization) using Gradient Descent.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, lambda_reg=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the ridge regression model.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.w = np.random.randn(n_features)\n",
        "        self.b = 0.0\n",
        "        \n",
        "        # Gradient descent with L2 regularization\n",
        "        for i in range(self.n_iterations):\n",
        "            # Predictions\n",
        "            y_pred = X @ self.w + self.b\n",
        "            \n",
        "            # Compute gradients (with L2 penalty)\n",
        "            dw = -(1/n_samples) * X.T @ (y - y_pred) + self.lambda_reg * self.w\n",
        "            db = -(1/n_samples) * np.sum(y - y_pred)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.w -= self.learning_rate * dw\n",
        "            self.b -= self.learning_rate * db\n",
        "            \n",
        "            # Store loss (with regularization term)\n",
        "            mse = np.mean((y - y_pred)**2) / 2\n",
        "            reg_term = (self.lambda_reg / 2) * np.sum(self.w**2)\n",
        "            loss = mse + reg_term\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        return X @ self.w + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate overfitting and regularization\n",
        "# Generate data with some noise\n",
        "np.random.seed(42)\n",
        "X_poly = np.linspace(-3, 3, 30).reshape(-1, 1)\n",
        "y_poly = 0.5 * X_poly.flatten()**2 + X_poly.flatten() + 1 + np.random.randn(30) * 0.5\n",
        "\n",
        "# Split data\n",
        "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
        "    X_poly, y_poly, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Try different regularization strengths\n",
        "lambda_values = [0.0, 0.1, 1.0, 10.0]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, lambda_reg in enumerate(lambda_values):\n",
        "    model = RidgeRegression(learning_rate=0.01, n_iterations=2000, lambda_reg=lambda_reg)\n",
        "    model.fit(X_train_poly, y_train_poly)\n",
        "    \n",
        "    # Plot\n",
        "    X_plot = np.linspace(X_poly.min(), X_poly.max(), 100).reshape(-1, 1)\n",
        "    y_pred_plot = model.predict(X_plot)\n",
        "    \n",
        "    axes[idx].scatter(X_train_poly, y_train_poly, alpha=0.6, label='Train', color='blue')\n",
        "    axes[idx].scatter(X_test_poly, y_test_poly, alpha=0.6, label='Test', color='red', marker='x')\n",
        "    axes[idx].plot(X_plot, y_pred_plot, 'g-', linewidth=2, label='Model')\n",
        "    axes[idx].set_title(f'λ = {lambda_reg}')\n",
        "    axes[idx].set_xlabel('X')\n",
        "    axes[idx].set_ylabel('y')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate errors\n",
        "    train_mse = mean_squared_error(y_train_poly, model.predict(X_train_poly))\n",
        "    test_mse = mean_squared_error(y_test_poly, model.predict(X_test_poly))\n",
        "    axes[idx].text(0.05, 0.95, f'Train MSE: {train_mse:.3f}\\nTest MSE: {test_mse:.3f}', \n",
        "                   transform=axes[idx].transAxes, ha='left', va='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Effect of L2 Regularization (Ridge Regression)', y=1.02, fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-fold Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_cross_validation(X, y, model_class, k=5, **model_params):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like\n",
        "        Features\n",
        "    y : array-like\n",
        "        Targets\n",
        "    model_class : class\n",
        "        Model class to instantiate\n",
        "    k : int\n",
        "        Number of folds\n",
        "    **model_params : dict\n",
        "        Parameters to pass to model\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    cv_scores : list\n",
        "        List of MSE scores for each fold\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "    \n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "        \n",
        "        # Train model\n",
        "        model = model_class(**model_params)\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        \n",
        "        # Evaluate\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "        mse = mean_squared_error(y_val_fold, y_pred)\n",
        "        cv_scores.append(mse)\n",
        "    \n",
        "    return cv_scores\n",
        "\n",
        "# Test different lambda values using cross-validation\n",
        "lambda_candidates = [0.0, 0.01, 0.1, 1.0, 10.0]\n",
        "cv_results = []\n",
        "\n",
        "for lambda_reg in lambda_candidates:\n",
        "    scores = k_fold_cross_validation(\n",
        "        X_poly, y_poly, \n",
        "        RidgeRegression, \n",
        "        k=5,\n",
        "        learning_rate=0.01,\n",
        "        n_iterations=2000,\n",
        "        lambda_reg=lambda_reg\n",
        "    )\n",
        "    cv_results.append({\n",
        "        'lambda': lambda_reg,\n",
        "        'mean_mse': np.mean(scores),\n",
        "        'std_mse': np.std(scores),\n",
        "        'scores': scores\n",
        "    })\n",
        "\n",
        "# Plot results\n",
        "means = [r['mean_mse'] for r in cv_results]\n",
        "stds = [r['std_mse'] for r in cv_results]\n",
        "lambdas = [r['lambda'] for r in cv_results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(lambdas, means, yerr=stds, marker='o', capsize=5, capthick=2)\n",
        "plt.xlabel('Regularization Strength (λ)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('5-Fold Cross-Validation: Selecting Optimal λ')\n",
        "plt.xscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Find best lambda\n",
        "best_idx = np.argmin(means)\n",
        "print(f\"Best λ: {lambdas[best_idx]}\")\n",
        "print(f\"Mean CV MSE: {means[best_idx]:.4f} ± {stds[best_idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the sigmoid function\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to avoid overflow\n",
        "\n",
        "z = np.linspace(-10, 10, 100)\n",
        "sigma_z = sigmoid(z)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, sigma_z, 'b-', linewidth=2, label='σ(z) = 1/(1 + e^(-z))')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision boundary (0.5)')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.title('Sigmoid (Logistic) Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression for binary classification using Gradient Descent.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the logistic regression model.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Binary labels (0 or 1)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.w = np.random.randn(n_features) * 0.01\n",
        "        self.b = 0.0\n",
        "        \n",
        "        # Gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            z = X @ self.w + self.b\n",
        "            y_pred = self.sigmoid(z)\n",
        "            \n",
        "            # Compute gradients\n",
        "            dw = (1/n_samples) * X.T @ (y_pred - y)\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.w -= self.learning_rate * dw\n",
        "            self.b -= self.learning_rate * db\n",
        "            \n",
        "            # Store loss (binary cross-entropy)\n",
        "            loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities.\"\"\"\n",
        "        z = X @ self.w + self.b\n",
        "        return self.sigmoid(z)\n",
        "        \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"Predict binary labels.\"\"\"\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic binary classification data\n",
        "def generate_classification_data(n=200, n_features=2, random_state=42):\n",
        "    \"\"\"Generate synthetic binary classification data.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Create two classes\n",
        "    n_per_class = n // 2\n",
        "    \n",
        "    # Class 0: centered at (-1, -1)\n",
        "    X0 = np.random.randn(n_per_class, n_features) + np.array([-1, -1])\n",
        "    y0 = np.zeros(n_per_class)\n",
        "    \n",
        "    # Class 1: centered at (1, 1)\n",
        "    X1 = np.random.randn(n_per_class, n_features) + np.array([1, 1])\n",
        "    y1 = np.ones(n_per_class)\n",
        "    \n",
        "    # Combine\n",
        "    X = np.vstack([X0, X1])\n",
        "    y = np.hstack([y0, y1])\n",
        "    \n",
        "    # Shuffle\n",
        "    indices = np.random.permutation(n)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Generate and visualize data\n",
        "X_clf, y_clf = generate_classification_data(n=200, n_features=2)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_clf[y_clf == 0, 0], X_clf[y_clf == 0, 1], alpha=0.6, label='Class 0', s=50)\n",
        "plt.scatter(X_clf[y_clf == 1, 0], X_clf[y_clf == 1, 1], alpha=0.6, label='Class 1', s=50)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Binary Classification Data')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression model\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model_lr = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
        "model_lr.fit(X_train_clf, y_train_clf)\n",
        "\n",
        "# Plot loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model_lr.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.title('Logistic Regression: Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "y_pred_clf = model_lr.predict(X_test_clf)\n",
        "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decision boundary\n",
        "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
        "    \"\"\"Plot decision boundary for 2D classification.\"\"\"\n",
        "    # Create a mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Predict on mesh\n",
        "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', alpha=0.8, label='Class 0', s=50, edgecolors='black')\n",
        "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', alpha=0.8, label='Class 1', s=50, edgecolors='black')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.colorbar(label='Probability of Class 1')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_clf, y_clf, model_lr, \"Logistic Regression Decision Boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this chapter, we've covered:\n",
        "\n",
        "1. **Linear Regression**:\n",
        "   - Basic concept and mathematical formulation\n",
        "   - Closed-form solution using normal equations\n",
        "   - Gradient descent optimization\n",
        "   - Regularization (Ridge regression)\n",
        "   - Cross-validation for model selection\n",
        "\n",
        "2. **Logistic Regression**:\n",
        "   - Binary classification using sigmoid function\n",
        "   - Binary cross-entropy loss\n",
        "   - Gradient descent for logistic regression\n",
        "   - Decision boundary visualization\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Linear regression** predicts continuous values using a linear relationship between features and target\n",
        "- **Gradient descent** is an iterative optimization method that finds optimal parameters by following the negative gradient\n",
        "- **Regularization** helps prevent overfitting by penalizing large weights\n",
        "- **Cross-validation** is essential for selecting hyperparameters and assessing model performance\n",
        "- **Logistic regression** extends linear models to binary classification using the sigmoid activation function\n",
        "\n",
        "These concepts form the foundation for understanding neural networks, which we'll explore in later chapters."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
