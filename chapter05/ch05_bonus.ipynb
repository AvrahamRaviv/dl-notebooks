{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Bonus: Practical CNN Lab\n",
    "\n",
    "This bonus notebook extends **Chapter 5 – Convolutional Neural Networks** with practical, visual experiments.\n",
    "\n",
    "We focus on:\n",
    "\n",
    "1. **Convolution Kernel Visualizations** – what 3×3 filters actually do to images\n",
    "2. **Feature Map Exploration** – visualizing activations inside a pretrained CNN\n",
    "3. **Training a Small CNN** – end-to-end training on a small dataset\n",
    "4. **Data Augmentation Demo** – how augmentations change images and affect training\n",
    "5. **Transfer Learning Exercise** – fine-tuning a pretrained model on a new task\n",
    "6. **(Optional) CNN Architecture Quiz** – quick recap of key architectures\n",
    "\n",
    "These experiments build **intuition** for CNNs as feature extractors and practical tools, complementing the more theoretical and architectural focus of the main Chapter 5 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We will use **PyTorch** and **torchvision** for CNNs and datasets, plus **NumPy** and **matplotlib** for visualization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolution Kernel Visualizations\n",
    "\n",
    "Before diving into deep CNNs, it’s helpful to see how **simple kernels** transform images.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Load a sample image\n",
    "- Define a few classic 3×3 kernels:\n",
    "  - **Identity** (no change)\n",
    "  - **Blur / Box filter**\n",
    "  - **Sharpen**\n",
    "  - **Edge detectors** (Sobel-like filters)\n",
    "- Apply them using a single convolution and visualize the results.\n",
    "\n",
    "This connects directly to the idea that convolutional filters act as **feature detectors** (edges, textures, etc.), as discussed in Section 5.1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a sample image from CIFAR10 (or use your own image if you prefer)\n",
    "\n",
    "transform_basic = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cifar10 = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_basic)\n",
    "img, label = cifar10[0]\n",
    "\n",
    "# Use only a single channel (convert to grayscale for kernel visualization)\n",
    "img_gray = img.mean(dim=0, keepdim=True)  # shape: (1, H, W)\n",
    "\n",
    "print(\"Image shape (C,H,W):\", img.shape)\n",
    "\n",
    "# Define 3x3 kernels\n",
    "kernels = {\n",
    "    \"identity\": torch.tensor([[0, 0, 0],\n",
    "                               [0, 1, 0],\n",
    "                               [0, 0, 0]], dtype=torch.float32),\n",
    "    \"blur\": (1/9.0) * torch.ones((3, 3), dtype=torch.float32),\n",
    "    \"sharpen\": torch.tensor([[0, -1, 0],\n",
    "                              [-1, 5, -1],\n",
    "                              [0, -1, 0]], dtype=torch.float32),\n",
    "    \"edge_horizontal\": torch.tensor([[-1, -2, -1],\n",
    "                                      [ 0,  0,  0],\n",
    "                                      [ 1,  2,  1]], dtype=torch.float32),\n",
    "    \"edge_vertical\": torch.tensor([[-1, 0, 1],\n",
    "                                    [-2, 0, 2],\n",
    "                                    [-1, 0, 1]], dtype=torch.float32),\n",
    "}\n",
    "\n",
    "# Apply kernels via conv2d\n",
    "img_batch = img_gray.unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "outputs = {}\n",
    "for name, k in kernels.items():\n",
    "    kernel = k.view(1, 1, 3, 3)  # (out_channels,in_channels,3,3)\n",
    "    out = F.conv2d(img_batch, kernel, padding=1)\n",
    "    outputs[name] = out.squeeze().detach().numpy()\n",
    "\n",
    "# Plot original and filtered images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(\"Original RGB\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img_gray.squeeze().numpy(), cmap=\"gray\")\n",
    "axes[1].set_title(\"Grayscale\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "for ax, (name, out) in zip(axes[2:], outputs.items()):\n",
    "    ax.imshow(out, cmap=\"gray\")\n",
    "    ax.set_title(name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Blur filters smooth the image and remove high-frequency details.\n",
    "- Sharpen filters emphasize edges and small details.\n",
    "- Edge detectors highlight horizontal or vertical transitions.\n",
    "\n",
    "This is exactly how early CNN layers learn to detect basic patterns such as edges and simple textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Map Exploration in a Pretrained CNN\n",
    "\n",
    "Next we peek **inside** a pretrained CNN (e.g., ResNet18) to see what different layers are doing.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load a pretrained ResNet18\n",
    "- Register forward hooks on an early conv layer and a deeper layer\n",
    "- Pass an image through the network\n",
    "- Visualize a few activation maps from early vs late layers\n",
    "\n",
    "Early layers usually detect **edges and simple color/texture blobs**, while deeper layers respond to **more abstract, class-specific patterns**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a pretrained ResNet18 (may download weights the first time)\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet18 = resnet18.to(device)\n",
    "resnet18.eval()\n",
    "\n",
    "# Preprocessing for ImageNet models\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Use a CIFAR10 image as input (resized to 224x224)\n",
    "img_raw, _ = cifar10[10]\n",
    "img_pil = transforms.ToPILImage()(img_raw)\n",
    "img_input = preprocess(img_pil).unsqueeze(0).to(device)  # (1,3,224,224)\n",
    "\n",
    "# Containers for feature maps\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on first conv layer and a deeper layer (layer3)\n",
    "resnet18.conv1.register_forward_hook(get_activation(\"conv1\"))\n",
    "resnet18.layer3.register_forward_hook(get_activation(\"layer3\"))\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    _ = resnet18(img_input)\n",
    "\n",
    "# Visualize a few feature maps\n",
    "conv1_feats = features[\"conv1\"]  # (1,C,H,W)\n",
    "layer3_feats = features[\"layer3\"]  # (1,C,H,W)\n",
    "\n",
    "print(\"conv1 feature map shape:\", conv1_feats.shape)\n",
    "print(\"layer3 feature map shape:\", layer3_feats.shape)\n",
    "\n",
    "# Helper to plot a grid of feature maps\n",
    "\n",
    "def plot_feature_maps(feats, title, n_maps=8):\n",
    "    feats = feats[0]  # remove batch dim -> (C,H,W)\n",
    "    n_maps = min(n_maps, feats.shape[0])\n",
    "    fig, axes = plt.subplots(1, n_maps, figsize=(2*n_maps, 2))\n",
    "    for i in range(n_maps):\n",
    "        ax = axes[i]\n",
    "        fmap = feats[i].numpy()\n",
    "        fmap = (fmap - fmap.min()) / (fmap.max() - fmap.min() + 1e-8)\n",
    "        ax.imshow(fmap, cmap=\"viridis\")\n",
    "        ax.axis(\"off\")\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot early and deeper feature maps\n",
    "plot_feature_maps(conv1_feats, \"ResNet18 conv1 feature maps\")\n",
    "plot_feature_maps(layer3_feats, \"ResNet18 layer3 feature maps\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Early feature maps tend to look like **edge detectors** and simple blob detectors.\n",
    "- Deeper feature maps are harder to interpret visually but tend to focus on **parts or high-level patterns**.\n",
    "\n",
    "This matches the idea that CNNs build a **hierarchy of features**, from low-level to high-level, as discussed in Section 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training a Small CNN on a Tiny Dataset\n",
    "\n",
    "Now we train a **small CNN** end-to-end on a subset of CIFAR10.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Define a simple CNN with a few conv + pooling layers\n",
    "- Train it on a **small subset** of CIFAR10 (e.g., first 10,000 images)\n",
    "- Track training and validation accuracy over epochs\n",
    "- Optionally show overfitting if the network is too large relative to the data\n",
    "\n",
    "This brings to life the training process described in Section 5.1.4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a simple CNN for CIFAR10-like images\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x16x16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Data transforms (basic for now; augmentation later)\n",
    "train_transform_basic = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform_basic)\n",
    "test_full = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Use a smaller subset for quicker experiments\n",
    "subset_size = 10000\n",
    "train_subset, _ = random_split(train_full, [subset_size, len(train_full) - subset_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader_cnn = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_cnn = DataLoader(test_full, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def train_cnn(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "        train_acc = correct / total\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        val_acc = correct / total\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "    \n",
    "    return train_accs, val_accs\n",
    "\n",
    "\n",
    "small_cnn = SmallCNN(num_classes=10)\n",
    "train_accs, val_accs = train_cnn(small_cnn, train_loader_cnn, val_loader_cnn, epochs=8, lr=1e-3)\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_accs, label=\"Train accuracy\")\n",
    "plt.plot(val_accs, label=\"Val accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Small CNN on CIFAR10 subset\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment by:\n",
    "\n",
    "- Increasing the network size (more filters or layers) to see overfitting (train accuracy → high, val accuracy → stagnates).\n",
    "- Reducing the subset size to make overfitting more dramatic.\n",
    "\n",
    "Next, we will add **data augmentation** and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Augmentation Demo\n",
    "\n",
    "Data augmentation is a simple but powerful way to improve **generalization** of CNNs by exposing them to varied versions of the training data.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Visualize common augmentations: random horizontal flip, random crop, color jitter\n",
    "- Compare training the small CNN **with vs without** augmentation (qualitatively via accuracy curves)\n",
    "\n",
    "This matches the practical training techniques discussed in Section 5.1.4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize augmentations on a few CIFAR10 images\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar_train_raw = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(12, 6))\n",
    "axes = axes.reshape(3, 6)\n",
    "\n",
    "for i in range(3):\n",
    "    img, label = cifar_train_raw[i]\n",
    "    axes[i, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title(\"Original\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "    \n",
    "    for j in range(1, 6):\n",
    "        img_aug = augment_transform(transforms.ToPILImage()(img))\n",
    "        axes[i, j].imshow(img_aug.permute(1, 2, 0).numpy())\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Data Augmentation Examples (CIFAR10)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) You can re-train SmallCNN with augment_transform instead of train_transform_basic\n",
    "# and compare accuracy curves manually."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transfer Learning Exercise (ResNet18)\n",
    "\n",
    "Transfer learning allows us to reuse a pretrained CNN as a **feature extractor** and adapt it to a new task with relatively little data.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Load a pretrained ResNet18\n",
    "- Replace the final fully-connected layer with a new layer for a **small number of classes** (e.g., 2 for cats vs dogs)\n",
    "- Freeze the backbone parameters and train only the new head\n",
    "- Show how to evaluate on a few images\n",
    "\n",
    "You can adapt the code to any `ImageFolder` dataset on disk (e.g., `data/cats_vs_dogs/train` and `data/cats_vs_dogs/val`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example transfer learning setup (paths assume an ImageFolder structure):\n",
    "# data/\n",
    "#   cats_vs_dogs/\n",
    "#     train/\n",
    "#       cats/...\n",
    "#       dogs/...\n",
    "#     val/\n",
    "#       cats/...\n",
    "#       dogs/...\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Change this path to your own small dataset if you have one\n",
    "root_tl = Path(\"data/cats_vs_dogs\")\n",
    "\n",
    "transform_tl = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "if root_tl.exists():\n",
    "    train_ds_tl = datasets.ImageFolder(root_tl / \"train\", transform=transform_tl)\n",
    "    val_ds_tl = datasets.ImageFolder(root_tl / \"val\", transform=transform_tl)\n",
    "\n",
    "    train_loader_tl = DataLoader(train_ds_tl, batch_size=16, shuffle=True)\n",
    "    val_loader_tl = DataLoader(val_ds_tl, batch_size=16, shuffle=False)\n",
    "\n",
    "    num_classes_tl = len(train_ds_tl.classes)\n",
    "\n",
    "    # Load pretrained ResNet18\n",
    "    base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace final FC layer\n",
    "    in_features = base_model.fc.in_features\n",
    "    base_model.fc = nn.Linear(in_features, num_classes_tl)\n",
    "\n",
    "    base_model = base_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(base_model.fc.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_transfer(model, train_loader, val_loader, epochs=5):\n",
    "        train_accs, val_accs = [], []\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "            train_acc = correct / total\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    logits = model(xb)\n",
    "                    preds = logits.argmax(dim=1)\n",
    "                    correct += (preds == yb).sum().item()\n",
    "                    total += yb.size(0)\n",
    "            val_acc = correct / total\n",
    "            val_accs.append(val_acc)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "        return train_accs, val_accs\n",
    "\n",
    "    print(\"\\nStarting transfer learning training (if dataset is available)...\")\n",
    "    train_accs_tl, val_accs_tl = train_transfer(base_model, train_loader_tl, val_loader_tl, epochs=3)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_accs_tl, label=\"Train accuracy\")\n",
    "    plt.plot(val_accs_tl, label=\"Val accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Transfer Learning (ResNet18) on Small Dataset\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Transfer learning data folder not found. Please create 'data/cats_vs_dogs/train' and 'data/cats_vs_dogs/val' to run this section.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical CNN lab we:\n",
    "\n",
    "- Visualized how simple **3×3 kernels** (blur, sharpen, edge detectors) transform images.\n",
    "- Explored **feature maps** inside a pretrained ResNet18, highlighting the hierarchy from low-level to high-level features.\n",
    "- Trained a **small CNN** on a CIFAR10 subset and inspected training/validation accuracy.\n",
    "- Demonstrated **data augmentation** techniques and how they change images.\n",
    "- Provided a **transfer learning** template to fine-tune ResNet18 on a small custom dataset.\n",
    "- Recapped important **CNN architectures** with a short quiz.\n",
    "\n",
    "These experiments should give you stronger intuition for how CNNs operate in practice and how to apply them to real-world image tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
