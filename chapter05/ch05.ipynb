{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "This notebook covers **Chapter 5** of the Deep Learning in Hebrew book, focusing on Convolutional Neural Networks (CNNs). We'll learn how to build networks that are efficient for image data and can learn spatial patterns.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The networks described so far are **Fully-Connected (FC)** - each neuron in each layer is connected to every neuron in the next layer. This approach is computationally expensive, and often there is no need for all connections between neurons.\n",
    "\n",
    "For example, a grayscale image of 256×256×1 pixels fed into an FC network with N=1000 neurons would require 65 million connections, all of which need to be updated during learning. If there are many layers, the number becomes enormous, and the amount of connections and parameters grows exponentially, making it difficult to maintain the network.\n",
    "\n",
    "Besides the size problem, in practice there is not always a need for all connections, since there is not always a relationship between all input elements. For example, for an image fed into a network, in many tasks the relationship between distant pixels is not significant, so there is no importance in connecting the input to all neurons in the first layer and connecting all layers fully.\n",
    "\n",
    "Instead, it is common to use layers with fewer connections and not connect every two neurons, but only between nearby elements, as will be detailed. Many modern networks are based on **convolutional layers**, on top of which advanced architectures have been built.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 5.1 Convolutional Layers\n",
    "- 5.1.1 [From Fully-Connected Layers to Convolutions](#511-from-fully-connected-layers-to-convolutions)\n",
    "- 5.1.2 [Padding, Stride and Dilation](#512-padding-stride-and-dilation)\n",
    "- 5.1.3 [Pooling](#513-pooling)\n",
    "- 5.1.4 [Training](#514-training)\n",
    "- 5.1.5 [Convolutional Neural Networks (LeNet)](#515-convolutional-neural-networks-lenet)\n",
    "\n",
    "### 5.2 CNN Architectures\n",
    "- 5.2.1 [AlexNet](#521-alexnet)\n",
    "- 5.2.2 [VGG](#522-vgg)\n",
    "- 5.2.3 [GoogleNet](#523-googlenet)\n",
    "- 5.2.4 [Residual Networks (ResNet)](#524-residual-networks-resnet)\n",
    "- 5.2.5 [Densely Connected Networks (DenseNet)](#525-densely-connected-networks-densenet)\n",
    "- 5.2.6 [U-Net](#526-u-net)\n",
    "- 5.2.7 [Transfer Learning](#527-transfer-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Convolutional Layers\n",
    "\n",
    "## 5.1.1 From Fully-Connected Layers to Convolutions\n",
    "\n",
    "A **convolutional layer** is a type of layer that performs a linear operation on the input in order to obtain a different and simpler representation of it. Usually, a convolutional layer performs a **cross-correlation** operation between a weight vector and a certain input (input vector or output vector from a hidden layer).\n",
    "\n",
    "The weight vector is called the **convolution kernel** (or **filter**), and with its help the cross-correlation operation is performed:\n",
    "\n",
    "$$y[n] = \\sum_{m=0}^{M-1} x[n-m] w[m]$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^N$ is the input vector and $w \\in \\mathbb{R}^M$ is the weight vector.\n",
    "\n",
    "### Parameter Sharing\n",
    "\n",
    "The key insight is that the weights $w$ are **shared** across all inputs in the layer, so the number of learned parameters in a convolutional layer is much smaller than in an FC layer - an FC layer contains $N_{input} \\times N_{output}$ weights, while a convolutional layer contains only $K$ weights (usually $K \\ll N_{input} \\times N_{output}$).\n",
    "\n",
    "### Feature Detection\n",
    "\n",
    "Besides reducing the number of weights, the convolution operation helps identify patterns and find features. This capability comes from the fact that the convolution operation checks the similarity between the input vector and the convolution kernel. The convolution can find features in the signal, and there are convolution kernels that can perform a set of useful operations, such as smoothing, derivative, and more.\n",
    "\n",
    "If we apply many different kernels to an image, we can find all kinds of features in it - for example, if the kernel is in the shape of an eye or nose, then it can find the regions in the original image similar to an eye or nose.\n",
    "\n",
    "### Activation Maps\n",
    "\n",
    "The output of the convolutional layer passes through a non-linear activation function (usually tanh or ReLU), and it is called an **activation map** or **feature map**.\n",
    "\n",
    "Usually, in each convolutional layer there will be several filters, each of which is supposed to learn a different feature in the image. As the network goes deeper, the features become more abstract and distinguished, and in the deeper layers the filters identify more complex features like parts of objects, while in the deeper layers the filters identify even more complex features like complete objects or shapes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1D Convolution Example\n",
    "def conv1d(x, kernel):\n",
    "    \"\"\"\n",
    "    Perform 1D convolution (cross-correlation).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array, shape (N,)\n",
    "        Input signal\n",
    "    kernel : array, shape (M,)\n",
    "        Convolution kernel/filter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y : array\n",
    "        Output after convolution\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    M = len(kernel)\n",
    "    output_size = N - M + 1\n",
    "    y = np.zeros(output_size)\n",
    "    \n",
    "    for n in range(output_size):\n",
    "        y[n] = np.sum(x[n:n+M] * kernel)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Example: Edge detection\n",
    "np.random.seed(42)\n",
    "# Create a signal (rectangle with small noise)\n",
    "x = np.ones(100)\n",
    "x[40:60] = 1.0\n",
    "x = x + np.random.randn(100) * 0.1\n",
    "\n",
    "# Create a rectangular kernel\n",
    "kernel = np.ones(10) / 10  # Moving average filter\n",
    "\n",
    "# Perform convolution\n",
    "y = conv1d(x, kernel)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, 'b-', linewidth=1.5, label='Input signal $x_1$', alpha=0.7)\n",
    "plt.plot(kernel * 5, 'orange', linewidth=2, label='Kernel $x_2$', alpha=0.8)\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Value')\n",
    "plt.title('1D Convolution: Input Signal and Kernel')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y, 'k-', linewidth=2, label='Convolution result')\n",
    "plt.axvline(x=45, color='r', linestyle='--', alpha=0.5, label='Peak at x≈0.5')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Convolution Output: High Overlap Around x=0.5±0.1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input size: {len(x)}\")\n",
    "print(f\"Kernel size: {len(kernel)}\")\n",
    "print(f\"Output size: {len(y)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2D Convolution for Images\n",
    "def conv2d(image, kernel, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution (cross-correlation) on an image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : array, shape (H, W) or (H, W, C)\n",
    "        Input image\n",
    "    kernel : array, shape (K, K) or (K, K, C)\n",
    "        Convolution kernel\n",
    "    padding : int\n",
    "        Padding size\n",
    "    stride : int\n",
    "        Stride size\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : array\n",
    "        Convolved output\n",
    "    \"\"\"\n",
    "    # Handle grayscale vs color\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis]\n",
    "    if len(kernel.shape) == 2:\n",
    "        kernel = kernel[:, :, np.newaxis]\n",
    "    \n",
    "    H, W, C = image.shape\n",
    "    K, _, _ = kernel.shape\n",
    "    \n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        image_padded = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        image_padded = image\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_h = (H + 2*padding - K) // stride + 1\n",
    "    out_w = (W + 2*padding - K) // stride + 1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, out_h):\n",
    "        for j in range(0, out_w):\n",
    "            h_start = i * stride\n",
    "            w_start = j * stride\n",
    "            h_end = h_start + K\n",
    "            w_end = w_start + K\n",
    "            \n",
    "            # Extract region and apply kernel\n",
    "            region = image_padded[h_start:h_end, w_start:w_end, :]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example: Edge detection on 2D image\n",
    "# Create a simple test image\n",
    "test_image = np.zeros((32, 32))\n",
    "test_image[10:22, 10:22] = 1.0  # White square\n",
    "\n",
    "# Edge detection kernel (Sobel-like)\n",
    "edge_kernel = np.array([[-1, -1, -1],\n",
    "                        [0, 0, 0],\n",
    "                        [1, 1, 1]])\n",
    "\n",
    "# Apply convolution\n",
    "edge_detected = conv2d(test_image, edge_kernel, padding=1, stride=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(test_image, cmap='gray')\n",
    "axes[0].set_title('Input Image (32×32)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(edge_kernel, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Edge Detection Kernel (3×3)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edge_detected, cmap='gray')\n",
    "axes[2].set_title('Output: Edge Detection (32×32)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input shape: {test_image.shape}\")\n",
    "print(f\"Kernel shape: {edge_kernel.shape}\")\n",
    "print(f\"Output shape: {edge_detected.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Channel Input and Output\n",
    "\n",
    "The input of a convolutional layer can be **multi-channel** (for example, a color image is usually represented using RGB values). In this case, the convolution operation is performed on each channel separately, and then the results are summed to produce a single output channel.\n",
    "\n",
    "The convolution can be a multi-dimensional vector operating on a certain input, but it can also be of higher dimension. Usually, filters operating on images are two-dimensional, and the convolution operation is performed between the filter and the image in a spatial manner.\n",
    "\n",
    "For a filter $F \\in \\mathbb{R}^{5 \\times 5 \\times 3}$ operating on input $x \\in \\mathbb{R}^{32 \\times 32 \\times 3}$, we get an activation map $y \\in \\mathbb{R}^{28 \\times 28}$.\n",
    "\n",
    "The input can pass through multiple filters and produce an activation map with multiple layers - for six filters, the dimension of the map is $y \\in \\mathbb{R}^{28 \\times 28 \\times 6}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multi-channel convolution example\n",
    "# Create RGB image (32×32×3)\n",
    "rgb_image = np.random.rand(32, 32, 3)\n",
    "\n",
    "# Create multiple filters (5×5×3 each)\n",
    "num_filters = 6\n",
    "filters = np.random.randn(5, 5, 3, num_filters)\n",
    "\n",
    "# Apply each filter\n",
    "output_maps = []\n",
    "for i in range(num_filters):\n",
    "    # For each filter, convolve across all channels and sum\n",
    "    output = np.zeros((28, 28))\n",
    "    for c in range(3):  # RGB channels\n",
    "        channel_output = conv2d(rgb_image[:, :, c], filters[:, :, c, i], padding=0, stride=1)\n",
    "        output += channel_output\n",
    "    output_maps.append(output)\n",
    "\n",
    "# Stack outputs\n",
    "feature_map = np.stack(output_maps, axis=2)\n",
    "\n",
    "print(f\"Input shape: {rgb_image.shape}\")\n",
    "print(f\"Number of filters: {num_filters}\")\n",
    "print(f\"Each filter shape: {filters[:, :, :, 0].shape}\")\n",
    "print(f\"Output feature map shape: {feature_map.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Show input RGB image\n",
    "axes[0, 0].imshow(rgb_image)\n",
    "axes[0, 0].set_title('Input: RGB Image\\n(32×32×3)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show one filter (averaged across channels for visualization)\n",
    "axes[0, 1].imshow(filters[:, :, :, 0].mean(axis=2), cmap='RdBu')\n",
    "axes[0, 1].set_title('Example Filter\\n(5×5×3)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].axis('off')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Show feature maps\n",
    "for i in range(6):\n",
    "    row = 1 if i < 4 else 0\n",
    "    col = (i % 4) if i < 4 else i - 4\n",
    "    if i < 6:\n",
    "        axes[row, col].imshow(feature_map[:, :, i], cmap='viridis')\n",
    "        axes[row, col].set_title(f'Feature Map {i+1}\\n(28×28)')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Multi-Channel Convolution: Input → Multiple Feature Maps', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Padding, Stride and Dilation\n",
    "\n",
    "Like in FC networks, in convolutional networks there are **hyperparameters** that determine the network's behavior. There are three main hyperparameters of the convolutional layer - the size of the filter, the number of channels of the input, and parameters that determine how the convolution operation is performed:\n",
    "\n",
    "### Padding\n",
    "\n",
    "**Padding** is an operation that adds zeros (or replicates edge values) around the input to allow the convolution operation to be performed on the edges as well. The convolution operation is spatial, meaning the filter operates on a number of pixels, and there are pixels on the edges for which the convolution operation cannot be performed if the filter is not placed in these places.\n",
    "\n",
    "For a filter of size $K \\times K$, the required padding size is: $\\text{Padding} = \\frac{K-1}{2}$.\n",
    "\n",
    "### Dilation\n",
    "\n",
    "**Dilation** is used to reduce the number of calculations even more, by operating on larger regions under the assumption that geographically close values have the same value. For this purpose, we can apply the convolution with gaps between close elements. The parameter that controls this is $d = 2$.\n",
    "\n",
    "### Stride\n",
    "\n",
    "**Stride** is the step size - instead of applying the filter to every possible region in the network, we can perform jumps, so that after each convolution calculation, a jump of stride size is performed before the next convolution. A typical stride size is $s = 2$.\n",
    "\n",
    "### Output Size Formula\n",
    "\n",
    "The size of the output layer after performing convolution depends on the sizes of the input and filter, zero padding, and stride size. Formally, we can calculate the output layer size according to the formula:\n",
    "\n",
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "\n",
    "where:\n",
    "- $W$ is the input size\n",
    "- $K$ is the filter size\n",
    "- $P$ is the padding size\n",
    "- $S$ is the stride size\n",
    "\n",
    "The number of layers in the output is equal to the number of filters (the output can be multi-channel). Note that the hyperparameter values (stride, padding, dilation) as well as the kernel size must be chosen so that $O$ is a natural number."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate Padding, Stride, and Dilation\n",
    "def visualize_conv_parameters():\n",
    "    \"\"\"Visualize the effect of padding, stride, and dilation.\"\"\"\n",
    "    # Create a simple image\n",
    "    img = np.zeros((32, 32))\n",
    "    img[12:20, 12:20] = 1.0\n",
    "    \n",
    "    kernel = np.ones((5, 5)) / 25  # 5×5 averaging kernel\n",
    "    \n",
    "    # No padding\n",
    "    output_no_pad = conv2d(img, kernel, padding=0, stride=1)\n",
    "    \n",
    "    # With padding\n",
    "    output_with_pad = conv2d(img, kernel, padding=2, stride=1)\n",
    "    \n",
    "    # With stride\n",
    "    output_stride = conv2d(img, kernel, padding=0, stride=2)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    axes[0, 0].imshow(img, cmap='gray')\n",
    "    axes[0, 0].set_title('Input Image (32×32)')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(output_no_pad, cmap='gray')\n",
    "    axes[0, 1].set_title(f'No Padding (Output: {output_no_pad.shape})')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(output_with_pad, cmap='gray')\n",
    "    axes[1, 0].set_title(f'With Padding=2 (Output: {output_with_pad.shape})')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(output_stride, cmap='gray')\n",
    "    axes[1, 1].set_title(f'Stride=2 (Output: {output_stride.shape})')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Effect of Padding and Stride on Convolution', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate output sizes\n",
    "    W, K, P, S = 32, 5, 2, 2\n",
    "    O_no_pad = (W - K + 2*0) // S + 1\n",
    "    O_with_pad = (W - K + 2*P) // S + 1\n",
    "    \n",
    "    print(\"Output Size Calculations:\")\n",
    "    print(f\"  Input size W = {W}, Kernel size K = {K}\")\n",
    "    print(f\"  No padding, stride=1: O = ({W} - {K} + 2×0)/1 + 1 = {output_no_pad.shape[0]}\")\n",
    "    print(f\"  Padding={P}, stride=1: O = ({W} - {K} + 2×{P})/1 + 1 = {output_with_pad.shape[0]}\")\n",
    "    print(f\"  No padding, stride={S}: O = ({W} - {K} + 2×0)/{S} + 1 = {output_stride.shape[0]}\")\n",
    "\n",
    "visualize_conv_parameters()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Field\n",
    "\n",
    "The **receptive field** of a value in the output of a network is defined to be all the domain in the input that affects that value in the output.\n",
    "\n",
    "As we go deeper into the network, the receptive field grows, allowing neurons in deeper layers to \"see\" larger portions of the input image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize Receptive Field\n",
    "def calculate_receptive_field(layers):\n",
    "    \"\"\"\n",
    "    Calculate receptive field size.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layers : list of tuples (kernel_size, stride, padding)\n",
    "        List of layer configurations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rf : int\n",
    "        Receptive field size\n",
    "    \"\"\"\n",
    "    rf = 1\n",
    "    for k, s, p in layers:\n",
    "        rf = rf + (k - 1) * s\n",
    "    return rf\n",
    "\n",
    "# Example: Three consecutive 3×3 convolutions\n",
    "layers = [(3, 1, 1), (3, 1, 1), (3, 1, 1)]\n",
    "rf = calculate_receptive_field(layers)\n",
    "\n",
    "print(\"Receptive Field Calculation:\")\n",
    "print(\"Three consecutive 3×3 convolutions:\")\n",
    "for i, (k, s, p) in enumerate(layers, 1):\n",
    "    print(f\"  Layer {i}: kernel={k}×{k}, stride={s}, padding={p}\")\n",
    "print(f\"\\nTotal Receptive Field: {rf}×{rf}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Draw input\n",
    "input_size = 32\n",
    "ax.add_patch(plt.Rectangle((0, 0), input_size, input_size, fill=False, edgecolor='blue', linewidth=2))\n",
    "ax.text(input_size/2, -2, 'Input (32×32)', ha='center', fontsize=12, color='blue')\n",
    "\n",
    "# Draw receptive field\n",
    "rf_start = (input_size - rf) / 2\n",
    "ax.add_patch(plt.Rectangle((rf_start, rf_start), rf, rf, fill=True, \n",
    "                          edgecolor='red', linewidth=3, facecolor='red', alpha=0.3))\n",
    "ax.text(input_size/2, input_size/2, f'Receptive\\nField\\n({rf}×{rf})', \n",
    "       ha='center', va='center', fontsize=14, weight='bold', color='red')\n",
    "\n",
    "ax.set_xlim(-5, input_size + 5)\n",
    "ax.set_ylim(-5, input_size + 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Receptive Field for Value in Output of Three Convolutional Layers', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Pooling\n",
    "\n",
    "Often spatial data is characterized by the fact that nearby elements are similar to each other, for example - nearby pixels in an image usually have the same value. We can exploit this fact to reduce the number of required calculations using strides and dilation as described above.\n",
    "\n",
    "Another method to exploit this fact is to perform **Pooling** - after each convolution, sample a single value from a region with multiple values, representing the region. The way to calculate the value of the pooling result can be chosen in several ways, where the common ones are choosing the largest element in its region (**max pooling**) or averaging the elements (**average pooling**).\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "$$\\text{MaxPool}(x) = \\max_{i,j \\in \\text{region}} x[i, j]$$\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "$$\\text{AvgPool}(x) = \\frac{1}{|\\text{region}|} \\sum_{i,j \\in \\text{region}} x[i, j]$$\n",
    "\n",
    "Pooling reduces the spatial dimensions while preserving the most important information, making the network more efficient and providing translation invariance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pooling Operations\n",
    "def max_pooling(image, pool_size=2, stride=2):\n",
    "    \"\"\"Perform max pooling.\"\"\"\n",
    "    H, W = image.shape\n",
    "    out_h = (H - pool_size) // stride + 1\n",
    "    out_w = (W - pool_size) // stride + 1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            h_start = i * stride\n",
    "            w_start = j * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_end = w_start + pool_size\n",
    "            \n",
    "            region = image[h_start:h_end, w_start:w_end]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def avg_pooling(image, pool_size=2, stride=2):\n",
    "    \"\"\"Perform average pooling.\"\"\"\n",
    "    H, W = image.shape\n",
    "    out_h = (H - pool_size) // stride + 1\n",
    "    out_w = (W - pool_size) // stride + 1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            h_start = i * stride\n",
    "            w_start = j * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_end = w_start + pool_size\n",
    "            \n",
    "            region = image[h_start:h_end, w_start:w_end]\n",
    "            output[i, j] = np.mean(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create example image\n",
    "example_img = np.array([[1, 2, 3, 4],\n",
    "                        [5, 6, 7, 8],\n",
    "                        [9, 10, 11, 12],\n",
    "                        [13, 14, 15, 16]])\n",
    "\n",
    "# Apply pooling\n",
    "max_pooled = max_pooling(example_img, pool_size=2, stride=2)\n",
    "avg_pooled = avg_pooling(example_img, pool_size=2, stride=2)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(example_img, cmap='viridis', vmin=1, vmax=16)\n",
    "axes[0].set_title('Input Image (4×4)')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, str(example_img[i, j]), ha='center', va='center', \n",
    "                    color='white', fontsize=12, weight='bold')\n",
    "axes[0].set_xticks(range(4))\n",
    "axes[0].set_yticks(range(4))\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].imshow(max_pooled, cmap='viridis', vmin=1, vmax=16)\n",
    "axes[1].set_title('Max Pooling (2×2)\\nOutput: 2×2')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, str(int(max_pooled[i, j])), ha='center', va='center',\n",
    "                    color='white', fontsize=14, weight='bold')\n",
    "axes[1].set_xticks(range(2))\n",
    "axes[1].set_yticks(range(2))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].imshow(avg_pooled, cmap='viridis', vmin=1, vmax=16)\n",
    "axes[2].set_title('Average Pooling (2×2)\\nOutput: 2×2')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[2].text(j, i, f'{avg_pooled[i, j]:.1f}', ha='center', va='center',\n",
    "                    color='white', fontsize=14, weight='bold')\n",
    "axes[2].set_xticks(range(2))\n",
    "axes[2].set_yticks(range(2))\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pooling Example:\")\n",
    "print(f\"Input shape: {example_img.shape}\")\n",
    "print(f\"Max pooling output: {max_pooled}\")\n",
    "print(f\"Average pooling output:\\n{avg_pooled}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.5 Convolutional Neural Networks (LeNet)\n",
    "\n",
    "By chaining layers and connecting all elements belonging to the convolution, we can build a complete network suitable for various tasks. Usually, at the output of convolutional layers there is one or more FC layers. The purpose of the FC is to allow connection of the information contained in the features collected during the convolutional layers.\n",
    "\n",
    "We can look at the overall network as two stages:\n",
    "- **First stage**: Perform feature extraction using convolutional filters, each learning a different feature\n",
    "- **Second stage**: Connect back all the information collected by connecting all neurons using FC\n",
    "\n",
    "This architecture was first used in 1998, in a network called **LeNet** (named after Yann LeCun), and is shown in the figure. This network achieved 98.9% accuracy on handwritten digits, where its structure is two convolutional layers and three FC layers, where after each of the convolutional layers pooling is performed.\n",
    "\n",
    "### LeNet Architecture\n",
    "\n",
    "1. **Convolutional Layer 1**: 6 filters of size 5×5\n",
    "2. **Pooling Layer 1**: 2×2 max pooling\n",
    "3. **Convolutional Layer 2**: 16 filters of size 5×5\n",
    "4. **Pooling Layer 2**: 2×2 max pooling\n",
    "5. **Fully Connected Layer 1**: 120 neurons\n",
    "6. **Fully Connected Layer 2**: 84 neurons\n",
    "7. **Output Layer**: 10 neurons (for 10 digit classes)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LeNet Implementation using PyTorch\n",
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 architecture for digit classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)  # 28×28 → 28×28\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)     # 28×28 → 14×14\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)          # 14×14 → 10×10\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)     # 10×10 → 5×5\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Stage 1: Feature extraction\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Stage 2: Classification\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model_lenet = LeNet(num_classes=10)\n",
    "\n",
    "# Print architecture\n",
    "print(\"LeNet Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model_lenet)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_lenet.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_lenet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(1, 1, 28, 28)  # Batch of 1, 1 channel, 28×28 image\n",
    "with torch.no_grad():\n",
    "    output = model_lenet(x_test)\n",
    "    print(f\"\\nInput shape: {x_test.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output (logits): {output[0].numpy()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 CNN Architectures\n",
    "\n",
    "After LeNet, many more efficient and faster networks were developed. In 2012, a network called **AlexNet** based on convolutional layers won the ImageNet competition (an image recognition competition), when it showed a significant improvement over the best result from the previous year.\n",
    "\n",
    "Along with the improvement in computation, work on deep networks returned to be central and many advanced architectures were developed.\n",
    "\n",
    "## 5.2.1 AlexNet\n",
    "\n",
    "AlexNet drew inspiration for its structure from the LeNet architecture, but the more complex structure compared to LeNet comes from the fact that there was more data available, and GPUs already existed that could perform complex calculations.\n",
    "\n",
    "The architecture contains five convolutional layers and three FC layers, where after the first two convolutional layers pooling and normalization are performed. The input is of dimension 224×224×3, and 96 filters of size 11×11 are applied to it with stride $s=4$ and without zero padding. Therefore, the output of the convolution is of dimension 55×55×96.\n",
    "\n",
    "After that, max-pooling is performed which reduces the first two dimensions, and a layer of dimension 27×27×96 is obtained. In the second convolutional layer there are 256 filters of size 5×5 with stride size $s=1$ and zero padding $p=2$, so the dimension is 27×27×256, and after max-pooling a layer of dimension 13×13×256 is obtained.\n",
    "\n",
    "After that there are 2 more convolutional layers with 384 filters of dimension 3×3, stride $s=1$ and padding $p=1$, and then a final convolutional layer with 256 filters of dimension 3×3, with $s=p=1$.\n",
    "\n",
    "At the output of the convolutions there is another max-pooling, and then three FC layers, where the output of the last layer is a vector of length 1000, representing 1000 different categories in the ImageNet dataset.\n",
    "\n",
    "The activation function of the network is **ReLU** (unlike LeNet which used tanh), and the hyperparameters are: Dropout=0.5, batch size=128, SGD+momentum=0.9, lr=1e-2. In total, the number of parameters in the network is approximately 60 million."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# AlexNet Architecture\n",
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet architecture for ImageNet classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # Feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 224×224×3 → 55×55×96\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 55×55 → 27×27\n",
    "            nn.LocalResponseNorm(size=5),  # Normalization\n",
    "            \n",
    "            # Conv2: 27×27×96 → 27×27×256\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 27×27 → 13×13\n",
    "            nn.LocalResponseNorm(size=5),\n",
    "            \n",
    "            # Conv3: 13×13×256 → 13×13×384\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv4: 13×13×384 → 13×13×384\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv5: 13×13×384 → 13×13×256\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # 13×13 → 5×5 (or 6×6 depending on input)\n",
    "        )\n",
    "        \n",
    "        # Use adaptive pooling to handle different input sizes robustly\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)  # Ensure consistent size before flattening\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model_alexnet = AlexNet(num_classes=1000)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_alexnet.parameters())\n",
    "print(\"AlexNet Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters: {total_params:,} (~60 million)\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  - ReLU activation (instead of tanh)\")\n",
    "print(\"  - Local Response Normalization\")\n",
    "print(\"  - Dropout (0.5)\")\n",
    "print(\"  - Max pooling after conv1, conv2, conv5\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(1, 3, 224, 224)  # ImageNet input size\n",
    "with torch.no_grad():\n",
    "    output = model_alexnet(x_test)\n",
    "    print(f\"\\nInput shape: {x_test.shape}\")\n",
    "    print(f\"Output shape: {output.shape} (1000 classes)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 VGG\n",
    "\n",
    "A year after ZFNet, a deep network with **19 layers** was presented in the competition, making better use of convolutional layers. The network developers showed that a layer of 7×7 filters can be replaced by three layers of 3×3 filters while maintaining the same receptive field, but with fewer parameters.\n",
    "\n",
    "For a $d \\times d$ filter operating on $c$ input and output channels, there are $d^2 c^2$ learned parameters. Therefore, a 7×7 filter has $49c^2$ parameters, while three layers of 3×3 filters have $3 \\cdot 3^2 \\cdot c^2 = 27c^2$ learned parameters - a savings of 45%.\n",
    "\n",
    "The original network is called **VGG16** and contains 138 million parameters, and there is a variation that adds two more convolutional layers called **VGG19**.\n",
    "\n",
    "### VGG Architecture\n",
    "\n",
    "- Uses small 3×3 filters throughout\n",
    "- Deeper than AlexNet (16-19 layers)\n",
    "- More parameters but better feature extraction\n",
    "- Standard architecture: conv layers → FC layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# VGG16 Architecture (simplified)\n",
    "def make_vgg_layers(cfg, batch_norm=False):\n",
    "    \"\"\"Make VGG layers from configuration.\"\"\"\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# VGG16 configuration\n",
    "cfg_vgg16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', \n",
    "             512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    \"\"\"VGG16 architecture.\"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = make_vgg_layers(cfg_vgg16)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model_vgg = VGG16(num_classes=1000)\n",
    "total_params = sum(p.numel() for p in model_vgg.parameters())\n",
    "\n",
    "print(\"VGG16 Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters: {total_params:,} (~138 million)\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  - All 3×3 convolutions\")\n",
    "print(\"  - 16 layers (13 conv + 3 FC)\")\n",
    "print(\"  - Max pooling after each block\")\n",
    "print(\"  - Deeper than AlexNet for better feature extraction\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 GoogleNet\n",
    "\n",
    "The previous models were computationally expensive due to the large number of parameters. To succeed in achieving the same performance with the same depth but with much fewer parameters, a group of developers from Google presented a concept called **Inception module** - a block that performs many simple operations in parallel, instead of performing one complex operation.\n",
    "\n",
    "Each block receives an input and performs on it several operations in parallel, and then the outputs of all operations are concatenated. The operations are:\n",
    "1. 1×1 convolution\n",
    "2. 1×1 convolution followed by 3×3 convolution with padding of size 1\n",
    "3. 1×1 convolution followed by 5×5 convolution with padding of size 2\n",
    "4. 3×3 max pooling with padding 1 followed by 1×1 convolution\n",
    "\n",
    "Finally, the outputs of the four branches are concatenated together and form the block output.\n",
    "\n",
    "This structure is equivalent to several networks in parallel, where the advantage of this method is that it has fewer parameters and relatively fast calculations since they are done in parallel. By combining many convolutional layers with this approach, we can find the right ratio between components and the depth of layers that bring optimal performance.\n",
    "\n",
    "### Inception Module Benefits\n",
    "\n",
    "- **Reduced parameters**: 1×1 convolutions reduce channels before expensive 3×3 and 5×5\n",
    "- **Parallel processing**: Multiple paths processed simultaneously\n",
    "- **Multi-scale features**: Captures features at different scales"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inception Module Implementation\n",
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Inception module with 4 parallel branches.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # Branch 1: 1×1 conv\n",
    "        self.branch1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        \n",
    "        # Branch 2: 1×1 → 3×3\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 96, kernel_size=1),\n",
    "            nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Branch 3: 1×1 → 5×5\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        )\n",
    "        \n",
    "        # Branch 4: 3×3 max pool → 1×1\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Test Inception module\n",
    "inception = InceptionModule(in_channels=192)\n",
    "x_test = torch.randn(1, 192, 28, 28)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = inception(x_test)\n",
    "    \n",
    "print(\"Inception Module:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"  Branch 1 (1×1): 64 channels\")\n",
    "print(f\"  Branch 2 (1×1→3×3): 128 channels\")\n",
    "print(f\"  Branch 3 (1×1→5×5): 32 channels\")\n",
    "print(f\"  Branch 4 (pool→1×1): 32 channels\")\n",
    "print(f\"Output: {output.shape} (64+128+32+32=256 channels)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in inception.parameters())\n",
    "print(f\"\\nTotal parameters in module: {total_params:,}\")\n",
    "print(\"\\nKey Benefit: Parallel processing of multi-scale features\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 Residual Networks (ResNet)\n",
    "\n",
    "After seeing that the deeper the network, the better results it achieves, they tried to build networks with many layers, but they achieved less good results than the previous networks that were on the order of 20 layers.\n",
    "\n",
    "The problem with deep networks came from the fact that after a certain number of layers, a sufficiently good representation was obtained, and therefore the additional layers should not change the input but pass the representation as is. To do this, the weights in these layers need to be 1.\n",
    "\n",
    "It turned out that it is difficult for layers to learn the identity function and they actually hurt the result. The challenge came from the difficulty in performing optimization properly for weights in deep layers.\n",
    "\n",
    "We can formulate the central problem slightly differently - given a network with $N$ layers, it makes sense to add another layer only if it adds information that does not exist until now. To solve this problem, they built a new network using **Residual Blocks** - creating blocks of convolutional layers, where in addition to the information passing through the layers, there is also a connection from the input to the output.\n",
    "\n",
    "Now if a block performs a certain function $(\\mathcal{F}(x))$, then the output is $\\mathcal{F}(x) + x$. In this way, if the block learns something different from what was learned so far - the function $\\mathcal{F}(x)$ simply remains 0. This structure of the blocks prevents the gradient in deep layers from diverging or vanishing, and training succeeds in converging.\n",
    "\n",
    "In this way, a network with **152 layers** was developed which achieved better results than all previous networks. The layers were composed of triplets of blocks, where each block has two convolutional layers. Between each triplet there is a doubling of the number of filters and a reduction of the dimension by a factor of two using pooling.\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "$$y = \\mathcal{F}(x) + x$$\n",
    "\n",
    "where $\\mathcal{F}(x)$ is the learned transformation and $x$ is the identity/skip connection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Residual Block Implementation\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Residual Block with skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main path: F(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Skip connection: x\n",
    "        out += self.shortcut(x)\n",
    "        \n",
    "        # Final activation\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Test Residual Block\n",
    "res_block = ResidualBlock(in_channels=64, out_channels=64, stride=1)\n",
    "x_test = torch.randn(1, 64, 32, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = res_block(x_test)\n",
    "\n",
    "print(\"Residual Block:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nStructure:\")\n",
    "print(\"  Main path: Conv → BN → ReLU → Conv → BN\")\n",
    "print(\"  Skip connection: Identity (or 1×1 conv if dimensions differ)\")\n",
    "print(\"  Output: F(x) + x\")\n",
    "\n",
    "# Visualize the concept\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Draw residual block diagram\n",
    "ax.text(1, 3, 'Input $x$', ha='center', fontsize=12, weight='bold')\n",
    "ax.arrow(1, 2.7, 0, -0.5, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "ax.text(1, 2, 'Conv + BN', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax.arrow(1, 1.7, 0, -0.5, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "ax.text(1, 1.2, 'ReLU', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax.arrow(1, 0.9, 0, -0.5, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "ax.text(1, 0.4, 'Conv + BN', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax.text(1, 0, '$\\\\mathcal{F}(x)$', ha='center', fontsize=12, weight='bold')\n",
    "\n",
    "# Skip connection\n",
    "ax.plot([1, 3], [3, 0], 'r-', linewidth=3, label='Skip connection $x$')\n",
    "ax.text(2, 1.5, '+', fontsize=20, ha='center', va='center', color='red')\n",
    "\n",
    "ax.text(3, 0, 'Output\\n$\\\\mathcal{F}(x) + x$', ha='center', fontsize=12, \n",
    "        weight='bold', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "ax.set_xlim(-0.5, 4)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('Residual Block: $y = \\\\mathcal{F}(x) + x$', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.5 Densely Connected Networks (DenseNet)\n",
    "\n",
    "We can extend the idea of Residual Block so that not only do we connect the input of each block to its output, but we also keep the input itself and check its relationship to deeper layers.\n",
    "\n",
    "A **Dense Block** is a block built so that the input of each layer is connected to all subsequent layers. We can combine several such blocks together and perform various operations between them like pooling or even an independent convolutional layer.\n",
    "\n",
    "Since we combine the inputs of several blocks, the number of channels grows, and to prevent the model from becoming too complex, there are **transition layers** at the end of each dense block that perform 1×1 convolution with stride $s=2$, so the number of channels remains reasonable and the model does not become too complex.\n",
    "\n",
    "### Dense Block\n",
    "\n",
    "In a dense block, each layer receives feature maps from all preceding layers:\n",
    "\n",
    "$$x_l = H_l([x_0, x_1, \\ldots, x_{l-1}])$$\n",
    "\n",
    "where $[x_0, x_1, \\ldots, x_{l-1}]$ denotes concatenation of feature maps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dense Block Implementation\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"Single layer in a Dense Block.\"\"\"\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(x))\n",
    "        out = self.conv(out)\n",
    "        return torch.cat([x, out], dim=1)  # Concatenate along channels\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense Block with multiple dense layers.\"\"\"\n",
    "    def __init__(self, num_layers, in_channels, growth_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Test Dense Block\n",
    "dense_block = DenseBlock(num_layers=4, in_channels=64, growth_rate=32)\n",
    "x_test = torch.randn(1, 64, 32, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = dense_block(x_test)\n",
    "\n",
    "print(\"Dense Block:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input channels: 64\")\n",
    "print(f\"Growth rate: 32 (channels added per layer)\")\n",
    "print(f\"Number of layers: 4\")\n",
    "print(f\"Output channels: {output.shape[1]} (64 + 4×32 = 192)\")\n",
    "print(\"\\nKey Feature: Each layer receives all previous feature maps\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.6 U-Net\n",
    "\n",
    "While CNNs are suitable for classification, at the end of the process a vector of probabilities is obtained for each label. In segmentation tasks this is problematic, because the model cannot map the features in the image back to the pixels of the original image with the appropriate segmentation.\n",
    "\n",
    "To deal with this problem, they proposed the **U-Net** architecture with three main sections: contraction, bottleneck, and expansion (contraction, bottleneck, and expansion section).\n",
    "\n",
    "In the contraction section, the image is compressed using convolutional layers with pooling. The difference between this stage and a regular CNN is that there are connections between corresponding layers in the process. After passing through the bottleneck, there is actually reconstruction of the image with segmentation. The reconstruction is done using **up-sampling** on the vector, and the architecture is symmetric to the contraction section.\n",
    "\n",
    "The loss function used is **pixel-wise cross entropy loss**, which checks each pixel relative to its label.\n",
    "\n",
    "### U-Net Architecture\n",
    "\n",
    "- **Encoder (Contraction)**: Downsampling path with conv + pooling\n",
    "- **Bottleneck**: Lowest resolution features\n",
    "- **Decoder (Expansion)**: Upsampling path with up-conv + skip connections\n",
    "- **Skip Connections**: Connect encoder features to decoder at same resolution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# U-Net Architecture (simplified)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture for image segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (contraction)\n",
    "        self.enc1 = self._conv_block(in_channels, 64)\n",
    "        self.enc2 = self._conv_block(64, 128)\n",
    "        self.enc3 = self._conv_block(128, 256)\n",
    "        self.enc4 = self._conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self._conv_block(512, 1024)\n",
    "        \n",
    "        # Decoder (expansion)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = self._conv_block(1024, 512)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = self._conv_block(512, 256)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = self._conv_block(256, 128)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self._conv_block(128, 64)\n",
    "        \n",
    "        self.final = nn.Conv2d(64, num_classes, 1)\n",
    "    \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "# Create model\n",
    "model_unet = UNet(in_channels=1, num_classes=2)\n",
    "\n",
    "print(\"U-Net Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Encoder (Contraction):\")\n",
    "print(\"  64 → 128 → 256 → 512 channels\")\n",
    "print(\"Bottleneck: 1024 channels\")\n",
    "print(\"Decoder (Expansion):\")\n",
    "print(\"  512 → 256 → 128 → 64 channels\")\n",
    "print(\"\\nKey Feature: Skip connections preserve spatial information\")\n",
    "\n",
    "# Test\n",
    "x_test = torch.randn(1, 1, 256, 256)\n",
    "with torch.no_grad():\n",
    "    output = model_unet(x_test)\n",
    "    print(f\"\\nInput shape: {x_test.shape}\")\n",
    "    print(f\"Output shape: {output.shape} (segmentation map)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Transfer Learning Example\n",
    "# Using a pre-trained ResNet and adapting it for a new task\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "pretrained_resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "print(\"Pre-trained ResNet18:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in pretrained_resnet.parameters()):,}\")\n",
    "\n",
    "# Freeze all layers\n",
    "for param in pretrained_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer for a new task (e.g., 10 classes instead of 1000)\n",
    "num_new_classes = 10\n",
    "pretrained_resnet.fc = nn.Linear(pretrained_resnet.fc.in_features, num_new_classes)\n",
    "\n",
    "# Only the new classifier will be trained\n",
    "trainable_params = sum(p.numel() for p in pretrained_resnet.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters (new classifier only): {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {sum(p.numel() for p in pretrained_resnet.parameters()) - trainable_params:,}\")\n",
    "\n",
    "print(\"\\nTransfer Learning Strategy:\")\n",
    "print(\"  1. Use pre-trained ResNet18 (trained on ImageNet)\")\n",
    "print(\"  2. Freeze all convolutional layers\")\n",
    "print(\"  3. Replace final FC layer for new task\")\n",
    "print(\"  4. Train only the new classifier on new data\")\n",
    "\n",
    "# Example: Fine-tuning (unfreeze last few layers)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Fine-tuning Strategy:\")\n",
    "# Unfreeze last two blocks\n",
    "for name, param in pretrained_resnet.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "trainable_finetune = sum(p.numel() for p in pretrained_resnet.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters (layer4 + fc): {trainable_finetune:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we've covered:\n",
    "\n",
    "### 5.1 Convolutional Layers\n",
    "- **5.1.1 From Fully-Connected Layers to Convolutions**: Parameter sharing, feature detection, activation maps\n",
    "- **5.1.2 Padding, Stride and Dilation**: Hyperparameters controlling convolution behavior\n",
    "- **5.1.3 Pooling**: Max and average pooling for dimensionality reduction\n",
    "- **5.1.4 Training**: Shared weights, gradient computation, batch normalization\n",
    "- **5.1.5 Convolutional Neural Networks (LeNet)**: First successful CNN architecture\n",
    "\n",
    "### 5.2 CNN Architectures\n",
    "- **5.2.1 AlexNet**: Deep CNN that won ImageNet 2012, ReLU activation\n",
    "- **5.2.2 VGG**: Deeper networks with 3×3 filters, 16-19 layers\n",
    "- **5.2.3 GoogleNet**: Inception modules for efficient parallel processing\n",
    "- **5.2.4 Residual Networks (ResNet)**: Skip connections solve vanishing gradients\n",
    "- **5.2.5 Densely Connected Networks (DenseNet)**: Dense blocks with feature reuse\n",
    "- **5.2.6 U-Net**: Encoder-decoder architecture for segmentation\n",
    "- **5.2.7 Transfer Learning**: Using pre-trained models for new tasks\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Convolutional layers** use parameter sharing to dramatically reduce parameters compared to FC layers\n",
    "- **Padding, stride, and dilation** control the spatial behavior of convolutions\n",
    "- **Pooling** reduces spatial dimensions while preserving important information\n",
    "- **CNNs** are highly effective for image data due to spatial structure exploitation\n",
    "- **Modern architectures** (ResNet, DenseNet) enable training of very deep networks\n",
    "- **Transfer learning** allows leveraging pre-trained models for new tasks with less data\n",
    "\n",
    "CNNs form the foundation of modern computer vision and have revolutionized image understanding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
