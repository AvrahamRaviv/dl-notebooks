{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Neural Networks\n",
        "\n",
        "This notebook covers **Chapter 4** of the Deep Learning in Hebrew book, focusing on neural networks. We'll learn how to build multi-layer neural networks that can handle non-linear relationships and complex patterns in data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In Chapter 3, we learned about linear models (linear regression and logistic regression) that can be viewed as single-layer neural networks. While these models are powerful, they have limitations - they can only learn linear decision boundaries.\n",
        "\n",
        "In this chapter, we'll extend these concepts to **multi-layer neural networks** (also called Multi-Layer Perceptrons or MLPs), which can learn non-linear patterns by stacking multiple layers of neurons with non-linear activation functions.\n",
        "\n",
        "We'll cover:\n",
        "1. **Multi-Layer Perceptrons (MLPs)** - Building deeper networks\n",
        "2. **Forward Propagation** - How information flows through the network\n",
        "3. **Backpropagation** - How to train multi-layer networks\n",
        "4. **Activation Functions** - Non-linear transformations\n",
        "5. **Building and Training Neural Networks** - Practical implementation\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "### 4.1 Multi-Layer Perceptrons\n",
        "- 4.1.1 [From Linear to Non-Linear Models](#411-from-linear-to-non-linear-models)\n",
        "- 4.1.2 [Network Architecture](#412-network-architecture)\n",
        "- 4.1.3 [Layer Types](#413-layer-types)\n",
        "\n",
        "### 4.2 Forward Propagation\n",
        "- 4.2.1 [Matrix Formulation](#421-matrix-formulation)\n",
        "- 4.2.2 [Vectorized Implementation](#422-vectorized-implementation)\n",
        "\n",
        "### 4.3 Backpropagation\n",
        "- 4.3.1 [The Chain Rule](#431-the-chain-rule)\n",
        "- 4.3.2 [Gradient Computation](#432-gradient-computation)\n",
        "- 4.3.3 [Backpropagation Algorithm](#433-backpropagation-algorithm)\n",
        "\n",
        "### 4.4 Activation Functions\n",
        "- 4.4.1 [Sigmoid and Tanh](#441-sigmoid-and-tanh)\n",
        "- 4.4.2 [ReLU and Variants](#442-relu-and-variants)\n",
        "- 4.4.3 [Choosing Activation Functions](#443-choosing-activation-functions)\n",
        "\n",
        "### 4.5 Building Neural Networks\n",
        "- 4.5.1 [Network Initialization](#451-network-initialization)\n",
        "- 4.5.2 [Training Process](#452-training-process)\n",
        "- 4.5.3 [Practical Considerations](#453-practical-considerations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries for this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification, make_moons, make_circles\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set matplotlib style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.1 Multi-Layer Perceptrons\n",
        "\n",
        "## 4.1.1 From Linear to Non-Linear Models\n",
        "\n",
        "In Chapter 3, we saw that linear models (linear regression, logistic regression) can be represented as single-layer neural networks. However, these models have a fundamental limitation: they can only learn **linear decision boundaries**.\n",
        "\n",
        "### The Limitation of Linear Models\n",
        "\n",
        "Consider the XOR problem - a classic example where linear models fail. The XOR function outputs 1 when the inputs are different, and 0 when they are the same:\n",
        "\n",
        "| $x_1$ | $x_2$ | XOR |\n",
        "|-------|-------|-----|\n",
        "| 0     | 0     | 0   |\n",
        "| 0     | 1     | 1   |\n",
        "| 1     | 0     | 1   |\n",
        "| 1     | 1     | 0   |\n",
        "\n",
        "This problem is **not linearly separable** - there is no single line that can separate the two classes.\n",
        "\n",
        "### Solution: Multi-Layer Networks\n",
        "\n",
        "By adding **hidden layers** between the input and output, we can learn non-linear decision boundaries. A Multi-Layer Perceptron (MLP) consists of:\n",
        "\n",
        "1. **Input Layer**: Receives the input features\n",
        "2. **Hidden Layers**: One or more layers of neurons with non-linear activation functions\n",
        "3. **Output Layer**: Produces the final prediction\n",
        "\n",
        "The key insight is that by composing multiple linear transformations with non-linear activations, we can approximate any continuous function (Universal Approximation Theorem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the XOR problem - linear models fail\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Try linear classifier (logistic regression)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "linear_model = LogisticRegression()\n",
        "linear_model.fit(X_xor, y_xor)\n",
        "linear_pred = linear_model.predict(X_xor)\n",
        "\n",
        "print(\"XOR Problem:\")\n",
        "print(\"Input | True Label | Linear Model Prediction\")\n",
        "print(\"-\" * 50)\n",
        "for i, (x, y_true, y_pred) in enumerate(zip(X_xor, y_xor, linear_pred)):\n",
        "    print(f\"{x} | {y_true:11d} | {y_pred:23d}\")\n",
        "\n",
        "print(f\"\\nLinear Model Accuracy: {accuracy_score(y_xor, linear_pred):.2%}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot 1: XOR data\n",
        "plt.subplot(1, 2, 1)\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
        "for i, (x, y, c) in enumerate(zip(X_xor, y_xor, colors)):\n",
        "    plt.scatter(x[0], x[1], c=c, s=200, marker='o' if y == 0 else 's', \n",
        "                edgecolors='black', linewidths=2, label=f'Class {y}' if i < 2 else '')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('XOR Problem - Not Linearly Separable')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xlim(-0.5, 1.5)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "\n",
        "# Plot 2: Decision boundary attempt\n",
        "plt.subplot(1, 2, 2)\n",
        "h = 0.01\n",
        "x_min, x_max = -0.5, 1.5\n",
        "y_min, y_max = -0.5, 1.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = linear_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "for i, (x, y, c) in enumerate(zip(X_xor, y_xor, colors)):\n",
        "    plt.scatter(x[0], x[1], c=c, s=200, marker='o' if y == 0 else 's',\n",
        "                edgecolors='black', linewidths=2)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Linear Model Cannot Separate XOR')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(-0.5, 1.5)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize network architecture\n",
        "def plot_network_architecture(input_dim=2, hidden_dims=[4, 3], output_dim=1):\n",
        "    \"\"\"Visualize a multi-layer neural network architecture.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    \n",
        "    layers = [input_dim] + hidden_dims + [output_dim]\n",
        "    layer_names = ['Input'] + [f'Hidden {i+1}' for i in range(len(hidden_dims))] + ['Output']\n",
        "    \n",
        "    # Calculate positions\n",
        "    n_layers = len(layers)\n",
        "    layer_spacing = 3\n",
        "    neuron_spacing = 1.5\n",
        "    \n",
        "    for layer_idx, (n_neurons, layer_name) in enumerate(zip(layers, layer_names)):\n",
        "        x_pos = layer_idx * layer_spacing\n",
        "        y_center = (n_neurons - 1) * neuron_spacing / 2\n",
        "        \n",
        "        # Draw neurons\n",
        "        for neuron_idx in range(n_neurons):\n",
        "            y_pos = neuron_idx * neuron_spacing - y_center\n",
        "            circle = plt.Circle((x_pos, y_pos), 0.3, \n",
        "                              color='lightblue' if layer_idx == 0 else \n",
        "                                    'lightgreen' if layer_idx == n_layers - 1 else 'lightyellow',\n",
        "                              ec='black', lw=2)\n",
        "            ax.add_patch(circle)\n",
        "            \n",
        "            # Label\n",
        "            if layer_idx == 0:\n",
        "                label = f'$x_{neuron_idx+1}$'\n",
        "            elif layer_idx == n_layers - 1:\n",
        "                label = f'$y_{neuron_idx+1}$' if output_dim > 1 else '$y$'\n",
        "            else:\n",
        "                label = f'$a_{{{layer_idx}}}^{{{neuron_idx+1}}}$'\n",
        "            ax.text(x_pos, y_pos, label, ha='center', va='center', fontsize=10)\n",
        "        \n",
        "        # Layer name\n",
        "        ax.text(x_pos, y_center + 1.5, layer_name, ha='center', va='bottom', \n",
        "                fontsize=12, weight='bold')\n",
        "        \n",
        "        # Draw connections to next layer\n",
        "        if layer_idx < n_layers - 1:\n",
        "            next_n_neurons = layers[layer_idx + 1]\n",
        "            next_y_center = (next_n_neurons - 1) * neuron_spacing / 2\n",
        "            \n",
        "            for i in range(n_neurons):\n",
        "                y1 = i * neuron_spacing - y_center\n",
        "                for j in range(next_n_neurons):\n",
        "                    y2 = j * neuron_spacing - next_y_center\n",
        "                    ax.plot([x_pos + 0.3, x_pos + layer_spacing - 0.3], \n",
        "                           [y1, y2], 'gray', alpha=0.3, linewidth=0.5)\n",
        "    \n",
        "    ax.set_xlim(-1, (n_layers - 1) * layer_spacing + 1)\n",
        "    ax.set_ylim(-3, 3)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Multi-Layer Perceptron Architecture', fontsize=16, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: Network with 2 inputs, 2 hidden layers (4 and 3 neurons), 1 output\n",
        "plot_network_architecture(input_dim=2, hidden_dims=[4, 3], output_dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1.3 Layer Types\n",
        "\n",
        "In a neural network, we have different types of layers:\n",
        "\n",
        "### Fully Connected (Dense) Layers\n",
        "\n",
        "Each neuron in a layer is connected to every neuron in the previous layer. This is the most common type of layer in MLPs.\n",
        "\n",
        "### Input Layer\n",
        "\n",
        "The input layer receives the raw features. No computation is performed here - it just passes the data to the first hidden layer.\n",
        "\n",
        "### Hidden Layers\n",
        "\n",
        "Hidden layers perform the non-linear transformations. They typically use activation functions like ReLU, sigmoid, or tanh.\n",
        "\n",
        "### Output Layer\n",
        "\n",
        "The output layer produces the final prediction:\n",
        "- **Regression**: Linear activation (or no activation)\n",
        "- **Binary Classification**: Sigmoid activation\n",
        "- **Multi-class Classification**: SoftMax activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.2 Forward Propagation\n",
        "\n",
        "Forward propagation is the process of passing input data through the network to compute the output. It's called \"forward\" because information flows from input to output.\n",
        "\n",
        "## 4.2.1 Matrix Formulation\n",
        "\n",
        "For a single example $x \\in \\mathbb{R}^d$, forward propagation through layer $l$ is:\n",
        "\n",
        "$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$\n",
        "\n",
        "$$a^{(l)} = g^{(l)}(z^{(l)})$$\n",
        "\n",
        "where:\n",
        "- $a^{(0)} = x$ (input)\n",
        "- $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ is the weight matrix\n",
        "- $b^{(l)} \\in \\mathbb{R}^{n_l}$ is the bias vector\n",
        "- $g^{(l)}$ is the activation function\n",
        "\n",
        "### For a Batch of Examples\n",
        "\n",
        "When processing multiple examples simultaneously (batch processing), we stack them into a matrix:\n",
        "\n",
        "- Input: $X \\in \\mathbb{R}^{m \\times d}$ where $m$ is the batch size\n",
        "- Layer $l$ output: $A^{(l)} \\in \\mathbb{R}^{m \\times n_l}$\n",
        "\n",
        "The forward propagation becomes:\n",
        "\n",
        "$$Z^{(l)} = A^{(l-1)} (W^{(l)})^T + \\mathbf{1} (b^{(l)})^T$$\n",
        "\n",
        "$$A^{(l)} = g^{(l)}(Z^{(l)})$$\n",
        "\n",
        "where $\\mathbf{1}$ is a column vector of ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward Propagation Implementation\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron with forward propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_dims, activation='relu', output_activation='linear'):\n",
        "        \"\"\"\n",
        "        Initialize MLP.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        layer_dims : list\n",
        "            List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]\n",
        "        activation : str\n",
        "            Activation function for hidden layers ('relu', 'sigmoid', 'tanh')\n",
        "        output_activation : str\n",
        "            Activation function for output layer ('linear', 'sigmoid', 'softmax')\n",
        "        \"\"\"\n",
        "        self.layer_dims = layer_dims\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        for i in range(len(layer_dims) - 1):\n",
        "            # Xavier/Glorot initialization\n",
        "            w = np.random.randn(layer_dims[i+1], layer_dims[i]) * np.sqrt(2.0 / layer_dims[i])\n",
        "            b = np.zeros((layer_dims[i+1], 1))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "    \n",
        "    def _activate(self, z, activation_type):\n",
        "        \"\"\"Apply activation function.\"\"\"\n",
        "        if activation_type == 'relu':\n",
        "            return np.maximum(0, z)\n",
        "        elif activation_type == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "        elif activation_type == 'tanh':\n",
        "            return np.tanh(z)\n",
        "        elif activation_type == 'softmax':\n",
        "            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "            return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        elif activation_type == 'linear':\n",
        "            return z\n",
        "        else:\n",
        "            return z\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array, shape (m, n_features)\n",
        "            Input data (m examples)\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        A : array\n",
        "            Output of the network\n",
        "        cache : dict\n",
        "            Cache of intermediate values for backpropagation\n",
        "        \"\"\"\n",
        "        cache = {'A': [X]}\n",
        "        A = X.T  # Transpose for matrix multiplication: (n_features, m)\n",
        "        \n",
        "        # Forward through hidden layers\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            Z = self.weights[i] @ A + self.biases[i]\n",
        "            A = self._activate(Z, self.activation)\n",
        "            cache['A'].append(A)\n",
        "            cache[f'Z_{i}'] = Z\n",
        "        \n",
        "        # Forward through output layer\n",
        "        i = len(self.weights) - 1\n",
        "        Z = self.weights[i] @ A + self.biases[i]\n",
        "        A = self._activate(Z, self.output_activation)\n",
        "        cache['A'].append(A)\n",
        "        cache[f'Z_{i}'] = Z\n",
        "        \n",
        "        return A.T, cache  # Transpose back: (m, n_output)\n",
        "\n",
        "# Test forward propagation\n",
        "mlp = MLP(layer_dims=[2, 4, 3, 1], activation='relu', output_activation='sigmoid')\n",
        "X_test = np.random.randn(5, 2)  # 5 examples, 2 features\n",
        "output, cache = mlp.forward(X_test)\n",
        "\n",
        "print(\"Forward Propagation Test:\")\n",
        "print(f\"Input shape: {X_test.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output values:\\n{output}\")\n",
        "print(f\"\\nNumber of layers: {len(mlp.weights)}\")\n",
        "for i, (w, b) in enumerate(zip(mlp.weights, mlp.biases)):\n",
        "    print(f\"Layer {i+1}: W shape {w.shape}, b shape {b.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.3 Backpropagation\n",
        "\n",
        "Backpropagation is the algorithm used to train neural networks. It computes the gradients of the loss function with respect to all parameters in the network, allowing us to update the weights using gradient descent.\n",
        "\n",
        "## 4.3.1 The Chain Rule\n",
        "\n",
        "Backpropagation is based on the **chain rule** from calculus. For a composite function $f(g(x))$, the derivative is:\n",
        "\n",
        "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
        "\n",
        "In neural networks, the loss $L$ depends on the output $a^{(L)}$, which depends on $z^{(L)}$, which depends on $W^{(L)}$ and $b^{(L)}$, and so on. We use the chain rule to compute gradients layer by layer, starting from the output and moving backward.\n",
        "\n",
        "### Gradient Flow\n",
        "\n",
        "For the output layer $L$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial z^{(L)}} = \\frac{\\partial L}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}$$\n",
        "\n",
        "For hidden layer $l$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial z^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial a^{(l)}} = (W^{(l+1)})^T \\frac{\\partial L}{\\partial z^{(l+1)}}$$\n",
        "\n",
        "This shows how gradients flow backward through the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3.2 Gradient Computation\n",
        "\n",
        "For a network with $L$ layers, we need to compute gradients for all weights and biases.\n",
        "\n",
        "### Output Layer Gradients\n",
        "\n",
        "For the output layer:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W^{(L)}} = \\frac{\\partial L}{\\partial z^{(L)}} \\cdot (a^{(L-1)})^T$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b^{(L)}} = \\frac{\\partial L}{\\partial z^{(L)}}$$\n",
        "\n",
        "### Hidden Layer Gradients\n",
        "\n",
        "For hidden layer $l$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} \\cdot (a^{(l-1)})^T$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}}$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial z^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\odot g'^{(l)}(z^{(l)})$$\n",
        "\n",
        "and $\\odot$ denotes element-wise multiplication (Hadamard product).\n",
        "\n",
        "### Loss Function Derivatives\n",
        "\n",
        "For common loss functions:\n",
        "\n",
        "**Mean Squared Error (MSE)**:\n",
        "$$L = \\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{m}(\\hat{y} - y)$$\n",
        "\n",
        "**Cross-Entropy (with SoftMax)**:\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}\\log(\\hat{y}_k^{(i)})$$\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{1}{m}\\frac{y}{\\hat{y}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backpropagation Implementation\n",
        "def backward_propagation(y, output, cache, mlp, loss_type='mse'):\n",
        "    \"\"\"\n",
        "    Backward propagation to compute gradients.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    y : array\n",
        "        True labels\n",
        "    output : array\n",
        "        Network output\n",
        "    cache : dict\n",
        "        Cache from forward propagation\n",
        "    mlp : MLP\n",
        "        The neural network\n",
        "    loss_type : str\n",
        "        Type of loss function ('mse' or 'cross_entropy')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    grads : dict\n",
        "        Gradients for weights and biases\n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "    grads = {}\n",
        "    \n",
        "    # Compute output layer gradient\n",
        "    if loss_type == 'mse':\n",
        "        dA = (output - y) / m\n",
        "    elif loss_type == 'cross_entropy':\n",
        "        dA = -y / (output + 1e-15) / m\n",
        "    else:\n",
        "        dA = (output - y) / m\n",
        "    \n",
        "    # Backpropagate through output layer\n",
        "    L = len(mlp.weights) - 1\n",
        "    Z_L = cache[f'Z_{L}']\n",
        "    A_prev = cache['A'][L]\n",
        "    \n",
        "    # Gradient of activation function\n",
        "    if mlp.output_activation == 'sigmoid':\n",
        "        dZ = dA * (output * (1 - output))\n",
        "    elif mlp.output_activation == 'softmax':\n",
        "        dZ = dA\n",
        "    else:  # linear\n",
        "        dZ = dA\n",
        "    \n",
        "    # Gradients for output layer\n",
        "    grads[f'dW_{L}'] = dZ.T @ A_prev.T / m\n",
        "    grads[f'db_{L}'] = np.sum(dZ.T, axis=1, keepdims=True) / m\n",
        "    dA_prev = mlp.weights[L].T @ dZ.T\n",
        "    \n",
        "    # Backpropagate through hidden layers\n",
        "    for l in range(L-1, -1, -1):\n",
        "        Z_l = cache[f'Z_{l}']\n",
        "        A_l = cache['A'][l]\n",
        "        A_prev = cache['A'][l]\n",
        "        \n",
        "        # Gradient of activation function\n",
        "        if mlp.activation == 'relu':\n",
        "            dZ = dA_prev * (Z_l > 0)\n",
        "        elif mlp.activation == 'sigmoid':\n",
        "            sig = 1 / (1 + np.exp(-np.clip(Z_l, -500, 500)))\n",
        "            dZ = dA_prev * sig * (1 - sig)\n",
        "        elif mlp.activation == 'tanh':\n",
        "            dZ = dA_prev * (1 - np.tanh(Z_l)**2)\n",
        "        else:\n",
        "            dZ = dA_prev\n",
        "        \n",
        "        # Gradients for layer l\n",
        "        grads[f'dW_{l}'] = dZ @ A_prev.T / m\n",
        "        grads[f'db_{l}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        \n",
        "        # Gradient for next iteration\n",
        "        if l > 0:\n",
        "            dA_prev = mlp.weights[l].T @ dZ\n",
        "    \n",
        "    return grads\n",
        "\n",
        "# Test backpropagation\n",
        "mlp_test = MLP(layer_dims=[2, 3, 1], activation='relu', output_activation='sigmoid')\n",
        "X_test = np.random.randn(10, 2)\n",
        "y_test = np.random.randint(0, 2, (10, 1)).astype(float)\n",
        "\n",
        "output_test, cache_test = mlp_test.forward(X_test)\n",
        "grads = backward_propagation(y_test, output_test, cache_test, mlp_test, loss_type='mse')\n",
        "\n",
        "print(\"Backpropagation Test:\")\n",
        "print(\"Gradients computed:\")\n",
        "for key in sorted(grads.keys()):\n",
        "    print(f\"  {key}: shape {grads[key].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3.3 Backpropagation Algorithm\n",
        "\n",
        "The complete backpropagation algorithm:\n",
        "\n",
        "1. **Forward Pass**: Compute all activations $a^{(l)}$ and pre-activations $z^{(l)}$ for all layers\n",
        "2. **Compute Output Error**: Calculate $\\frac{\\partial L}{\\partial a^{(L)}}$ based on the loss function\n",
        "3. **Backward Pass**: For each layer from $L$ to $1$:\n",
        "   - Compute $\\frac{\\partial L}{\\partial z^{(l)}}$ using the chain rule\n",
        "   - Compute $\\frac{\\partial L}{\\partial W^{(l)}}$ and $\\frac{\\partial L}{\\partial b^{(l)}}$\n",
        "   - Compute $\\frac{\\partial L}{\\partial a^{(l-1)}}$ for the previous layer\n",
        "4. **Update Parameters**: Use gradients to update weights and biases\n",
        "\n",
        "### Pseudocode\n",
        "\n",
        "```\n",
        "Forward:\n",
        "  a[0] = x\n",
        "  for l = 1 to L:\n",
        "    z[l] = W[l] * a[l-1] + b[l]\n",
        "    a[l] = g[l](z[l])\n",
        "\n",
        "Backward:\n",
        "  dA[L] = dL/da[L]  # from loss function\n",
        "  for l = L to 1:\n",
        "    dZ[l] = dA[l] * g'[l](z[l])\n",
        "    dW[l] = dZ[l] * a[l-1]^T\n",
        "    db[l] = sum(dZ[l])\n",
        "    dA[l-1] = W[l]^T * dZ[l]\n",
        "\n",
        "Update:\n",
        "  for l = 1 to L:\n",
        "    W[l] = W[l] - learning_rate * dW[l]\n",
        "    b[l] = b[l] - learning_rate * db[l]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.4 Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks. Without them, a multi-layer network would be equivalent to a single linear layer.\n",
        "\n",
        "## 4.4.1 Sigmoid and Tanh\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "**Properties:**\n",
        "- Range: $(0, 1)$\n",
        "- Smooth and differentiable\n",
        "- Historically popular for hidden layers\n",
        "- **Problem**: Vanishing gradient problem for large $|z|$\n",
        "\n",
        "### Tanh Function\n",
        "\n",
        "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1$$\n",
        "\n",
        "**Properties:**\n",
        "- Range: $(-1, 1)$\n",
        "- Zero-centered (unlike sigmoid)\n",
        "- Still suffers from vanishing gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize activation functions\n",
        "z = np.linspace(-5, 5, 100)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, alpha * z)\n",
        "\n",
        "sigmoid_z = sigmoid(z)\n",
        "tanh_z = tanh(z)\n",
        "relu_z = relu(z)\n",
        "leaky_relu_z = leaky_relu(z)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Plot activation functions\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(z, sigmoid_z, 'b-', linewidth=2, label='Sigmoid')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.title('Sigmoid Activation')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(z, tanh_z, 'g-', linewidth=2, label='Tanh')\n",
        "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('tanh(z)')\n",
        "plt.title('Tanh Activation')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(z, relu_z, 'r-', linewidth=2, label='ReLU')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('ReLU(z)')\n",
        "plt.title('ReLU Activation')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(z, leaky_relu_z, 'm-', linewidth=2, label='Leaky ReLU (α=0.01)')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('Leaky ReLU(z)')\n",
        "plt.title('Leaky ReLU Activation')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot derivatives\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(z, sigmoid_z * (1 - sigmoid_z), 'b-', linewidth=2, label=\"Sigmoid'\")\n",
        "plt.plot(z, 1 - tanh_z**2, 'g-', linewidth=2, label=\"Tanh'\")\n",
        "plt.xlabel('z')\n",
        "plt.ylabel(\"g'(z)\")\n",
        "plt.title('Derivatives: Sigmoid and Tanh')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "relu_deriv = (z > 0).astype(float)\n",
        "leaky_relu_deriv = np.where(z > 0, 1, 0.01)\n",
        "plt.plot(z, relu_deriv, 'r-', linewidth=2, label=\"ReLU'\")\n",
        "plt.plot(z, leaky_relu_deriv, 'm-', linewidth=2, label=\"Leaky ReLU'\")\n",
        "plt.xlabel('z')\n",
        "plt.ylabel(\"g'(z)\")\n",
        "plt.title('Derivatives: ReLU and Leaky ReLU')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4.2 ReLU and Variants\n",
        "\n",
        "### ReLU (Rectified Linear Unit)\n",
        "\n",
        "$$\\text{ReLU}(z) = \\max(0, z) = \\begin{cases} z & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases}$$\n",
        "\n",
        "**Properties:**\n",
        "- Simple and computationally efficient\n",
        "- Solves vanishing gradient problem (for positive values)\n",
        "- **Problem**: \"Dying ReLU\" - neurons can become inactive (output always 0)\n",
        "\n",
        "**Derivative:**\n",
        "$$\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases}$$\n",
        "\n",
        "### Leaky ReLU\n",
        "\n",
        "$$\\text{LeakyReLU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha z & \\text{if } z \\leq 0 \\end{cases}$$\n",
        "\n",
        "where $\\alpha$ is a small positive constant (typically 0.01).\n",
        "\n",
        "**Properties:**\n",
        "- Prevents \"dying ReLU\" problem\n",
        "- Allows small negative gradients\n",
        "\n",
        "### Other Variants\n",
        "\n",
        "- **ELU (Exponential Linear Unit)**: Smooth version of ReLU\n",
        "- **Swish**: $z \\cdot \\sigma(z)$\n",
        "- **GELU**: Used in transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4.3 Choosing Activation Functions\n",
        "\n",
        "### Guidelines\n",
        "\n",
        "1. **Hidden Layers**: \n",
        "   - **ReLU** is the default choice for most cases\n",
        "   - Use **Leaky ReLU** or **ELU** if you encounter dying ReLU problems\n",
        "   - Avoid sigmoid/tanh for deep networks (vanishing gradients)\n",
        "\n",
        "2. **Output Layer**:\n",
        "   - **Regression**: Linear (no activation) or ReLU if outputs must be non-negative\n",
        "   - **Binary Classification**: Sigmoid\n",
        "   - **Multi-class Classification**: SoftMax\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| Activation | Range | Pros | Cons |\n",
        "|------------|-------|------|------|\n",
        "| Sigmoid | (0,1) | Smooth, bounded | Vanishing gradients, not zero-centered |\n",
        "| Tanh | (-1,1) | Zero-centered | Vanishing gradients |\n",
        "| ReLU | [0,∞) | Fast, no vanishing gradient for z>0 | Dying ReLU problem |\n",
        "| Leaky ReLU | (-∞,∞) | Prevents dying ReLU | Extra hyperparameter |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.5 Building Neural Networks\n",
        "\n",
        "## 4.5.1 Network Initialization\n",
        "\n",
        "Proper weight initialization is crucial for training deep networks. Poor initialization can lead to:\n",
        "- Vanishing gradients (weights too small)\n",
        "- Exploding gradients (weights too large)\n",
        "- Symmetry breaking issues\n",
        "\n",
        "### Common Initialization Methods\n",
        "\n",
        "1. **Xavier/Glorot Initialization**:\n",
        "   $$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in} + n_{out}}\\right)$$\n",
        "   or\n",
        "   $$W \\sim \\mathcal{U}\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}\\right)$$\n",
        "   \n",
        "   Good for sigmoid/tanh activations.\n",
        "\n",
        "2. **He Initialization** (for ReLU):\n",
        "   $$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)$$\n",
        "   \n",
        "   Accounts for ReLU's zero output for half the inputs.\n",
        "\n",
        "3. **Random Small Values**:\n",
        "   $$W \\sim \\mathcal{N}(0, 0.01)$$\n",
        "   \n",
        "   Simple but can work for small networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete MLP with Training\n",
        "class MLPTrainer:\n",
        "    \"\"\"\n",
        "    Complete MLP with training capabilities.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_dims, activation='relu', output_activation='linear', \n",
        "                 initialization='xavier'):\n",
        "        self.layer_dims = layer_dims\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize weights\n",
        "        for i in range(len(layer_dims) - 1):\n",
        "            n_in, n_out = layer_dims[i], layer_dims[i+1]\n",
        "            \n",
        "            if initialization == 'xavier':\n",
        "                limit = np.sqrt(6.0 / (n_in + n_out))\n",
        "                w = np.random.uniform(-limit, limit, (n_out, n_in))\n",
        "            elif initialization == 'he':\n",
        "                std = np.sqrt(2.0 / n_in)\n",
        "                w = np.random.randn(n_out, n_in) * std\n",
        "            else:  # random small\n",
        "                w = np.random.randn(n_out, n_in) * 0.01\n",
        "            \n",
        "            b = np.zeros((n_out, 1))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "    \n",
        "    def _activate(self, z, activation_type):\n",
        "        \"\"\"Apply activation function.\"\"\"\n",
        "        if activation_type == 'relu':\n",
        "            return np.maximum(0, z)\n",
        "        elif activation_type == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "        elif activation_type == 'tanh':\n",
        "            return np.tanh(z)\n",
        "        elif activation_type == 'softmax':\n",
        "            exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "            return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "        else:\n",
        "            return z\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation.\"\"\"\n",
        "        cache = {'A': [X.T]}\n",
        "        A = X.T\n",
        "        \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            Z = self.weights[i] @ A + self.biases[i]\n",
        "            A = self._activate(Z, self.activation)\n",
        "            cache['A'].append(A)\n",
        "            cache[f'Z_{i}'] = Z\n",
        "        \n",
        "        i = len(self.weights) - 1\n",
        "        Z = self.weights[i] @ A + self.biases[i]\n",
        "        A = self._activate(Z, self.output_activation)\n",
        "        cache['A'].append(A)\n",
        "        cache[f'Z_{i}'] = Z\n",
        "        \n",
        "        return A.T, cache\n",
        "    \n",
        "    def compute_loss(self, y_pred, y_true, loss_type='mse'):\n",
        "        \"\"\"Compute loss.\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "        if loss_type == 'mse':\n",
        "            return np.mean((y_pred - y_true)**2) / 2\n",
        "        elif loss_type == 'cross_entropy':\n",
        "            return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
        "        else:\n",
        "            return np.mean((y_pred - y_true)**2) / 2\n",
        "    \n",
        "    def backward(self, y, output, cache, loss_type='mse'):\n",
        "        \"\"\"Backward propagation.\"\"\"\n",
        "        m = y.shape[0]\n",
        "        grads = {}\n",
        "        \n",
        "        # Output layer gradient\n",
        "        if loss_type == 'mse':\n",
        "            dA = (output - y) / m\n",
        "        elif loss_type == 'cross_entropy':\n",
        "            dA = -y / (output + 1e-15) / m\n",
        "        else:\n",
        "            dA = (output - y) / m\n",
        "        \n",
        "        L = len(self.weights) - 1\n",
        "        Z_L = cache[f'Z_{L}']\n",
        "        A_prev = cache['A'][L]\n",
        "        \n",
        "        if self.output_activation == 'sigmoid':\n",
        "            dZ = dA.T * (output.T * (1 - output.T))\n",
        "        elif self.output_activation == 'softmax':\n",
        "            dZ = dA.T\n",
        "        else:\n",
        "            dZ = dA.T\n",
        "        \n",
        "        grads[f'dW_{L}'] = dZ @ A_prev.T\n",
        "        grads[f'db_{L}'] = np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = self.weights[L].T @ dZ\n",
        "        \n",
        "        for l in range(L-1, -1, -1):\n",
        "            Z_l = cache[f'Z_{l}']\n",
        "            A_prev = cache['A'][l]\n",
        "            \n",
        "            if self.activation == 'relu':\n",
        "                dZ = dA_prev * (Z_l > 0)\n",
        "            elif self.activation == 'sigmoid':\n",
        "                sig = 1 / (1 + np.exp(-np.clip(Z_l, -500, 500)))\n",
        "                dZ = dA_prev * sig * (1 - sig)\n",
        "            elif self.activation == 'tanh':\n",
        "                dZ = dA_prev * (1 - np.tanh(Z_l)**2)\n",
        "            else:\n",
        "                dZ = dA_prev\n",
        "            \n",
        "            grads[f'dW_{l}'] = dZ @ A_prev.T\n",
        "            grads[f'db_{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
        "            \n",
        "            if l > 0:\n",
        "                dA_prev = self.weights[l].T @ dZ\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def update_parameters(self, grads, learning_rate):\n",
        "        \"\"\"Update parameters using gradients.\"\"\"\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * grads[f'dW_{i}']\n",
        "            self.biases[i] -= learning_rate * grads[f'db_{i}']\n",
        "    \n",
        "    def train(self, X, y, epochs=1000, learning_rate=0.01, loss_type='mse', verbose=True):\n",
        "        \"\"\"Train the network.\"\"\"\n",
        "        loss_history = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward\n",
        "            output, cache = self.forward(X)\n",
        "            loss = self.compute_loss(output, y, loss_type)\n",
        "            loss_history.append(loss)\n",
        "            \n",
        "            # Backward\n",
        "            grads = self.backward(y, output, cache, loss_type)\n",
        "            \n",
        "            # Update\n",
        "            self.update_parameters(grads, learning_rate)\n",
        "            \n",
        "            if verbose and (epoch + 1) % (epochs // 10) == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
        "        \n",
        "        return loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MLP on XOR problem\n",
        "print(\"Training MLP on XOR Problem:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train network\n",
        "mlp_xor = MLPTrainer(layer_dims=[2, 4, 1], activation='relu', \n",
        "                     output_activation='sigmoid', initialization='he')\n",
        "\n",
        "loss_history = mlp_xor.train(X_xor, y_xor, epochs=2000, learning_rate=0.1, \n",
        "                             loss_type='mse', verbose=True)\n",
        "\n",
        "# Test predictions\n",
        "predictions, _ = mlp_xor.forward(X_xor)\n",
        "pred_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "print(\"Input | True | Predicted | Probability\")\n",
        "print(\"-\" * 45)\n",
        "for i, (x, y_true, y_pred, prob) in enumerate(zip(X_xor, y_xor, pred_binary, predictions)):\n",
        "    print(f\"{x} | {y_true[0]:4d} | {y_pred[0]:9d} | {prob[0]:.4f}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_xor, pred_binary):.2%}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss: MLP on XOR Problem')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = -0.5, 1.5\n",
        "y_min, y_max = -0.5, 1.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z, _ = mlp_xor.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor.flatten()]\n",
        "for i, (x, y, c) in enumerate(zip(X_xor, y_xor, colors)):\n",
        "    plt.scatter(x[0], x[1], c=c, s=300, marker='o' if y[0] == 0 else 's',\n",
        "                edgecolors='black', linewidths=2, label=f'Class {y[0]}' if i < 2 else '')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('MLP Decision Boundary on XOR Problem')\n",
        "plt.legend()\n",
        "plt.colorbar(label='Probability')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5.3 Practical Considerations\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "1. **Normalization/Standardization**: Scale features to similar ranges\n",
        "   - StandardScaler: $(x - \\mu) / \\sigma$\n",
        "   - MinMaxScaler: $(x - x_{min}) / (x_{max} - x_{min})$\n",
        "\n",
        "2. **Train/Validation/Test Split**: \n",
        "   - Training set: Used to learn parameters\n",
        "   - Validation set: Used to tune hyperparameters\n",
        "   - Test set: Used for final evaluation\n",
        "\n",
        "### Regularization\n",
        "\n",
        "1. **L2 Regularization (Weight Decay)**:\n",
        "   $$L_{reg} = L + \\lambda \\sum_{l} \\|W^{(l)}\\|^2$$\n",
        "\n",
        "2. **Dropout**: Randomly set some neurons to zero during training\n",
        "\n",
        "3. **Early Stopping**: Stop training when validation loss stops improving\n",
        "\n",
        "### Monitoring Training\n",
        "\n",
        "- **Loss Curves**: Track training and validation loss\n",
        "- **Accuracy Curves**: For classification tasks\n",
        "- **Gradient Norms**: Check for vanishing/exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on non-linearly separable data (moons dataset)\n",
        "X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "y_moons = y_moons.reshape(-1, 1)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train MLP\n",
        "print(\"Training MLP on Moons Dataset:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "mlp_moons = MLPTrainer(layer_dims=[2, 10, 10, 1], activation='relu',\n",
        "                       output_activation='sigmoid', initialization='he')\n",
        "\n",
        "loss_history = mlp_moons.train(X_train_scaled, y_train, epochs=1000, \n",
        "                               learning_rate=0.01, loss_type='mse', verbose=True)\n",
        "\n",
        "# Evaluate\n",
        "train_pred, _ = mlp_moons.forward(X_train_scaled)\n",
        "test_pred, _ = mlp_moons.forward(X_test_scaled)\n",
        "\n",
        "train_acc = accuracy_score(y_train, (train_pred > 0.5).astype(int))\n",
        "test_acc = accuracy_score(y_test, (test_pred > 0.5).astype(int))\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_acc:.2%}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Training loss\n",
        "axes[0].plot(loss_history)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Decision boundary on training data\n",
        "h = 0.02\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z, _ = mlp_moons.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "axes[1].contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "axes[1].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "scatter1 = axes[1].scatter(X_train_scaled[y_train.flatten() == 0, 0], \n",
        "                          X_train_scaled[y_train.flatten() == 0, 1],\n",
        "                          c='red', alpha=0.6, s=30, label='Class 0')\n",
        "scatter2 = axes[1].scatter(X_train_scaled[y_train.flatten() == 1, 0],\n",
        "                          X_train_scaled[y_train.flatten() == 1, 1],\n",
        "                          c='blue', alpha=0.6, s=30, label='Class 1')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title(f'Training Data (Acc: {train_acc:.2%})')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Decision boundary on test data\n",
        "axes[2].contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "axes[2].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "axes[2].scatter(X_test_scaled[y_test.flatten() == 0, 0],\n",
        "               X_test_scaled[y_test.flatten() == 0, 1],\n",
        "               c='red', alpha=0.6, s=30, label='Class 0')\n",
        "axes[2].scatter(X_test_scaled[y_test.flatten() == 1, 0],\n",
        "               X_test_scaled[y_test.flatten() == 1, 1],\n",
        "               c='blue', alpha=0.6, s=30, label='Class 1')\n",
        "axes[2].set_xlabel('Feature 1')\n",
        "axes[2].set_ylabel('Feature 2')\n",
        "axes[2].set_title(f'Test Data (Acc: {test_acc:.2%})')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this chapter, we've covered:\n",
        "\n",
        "### 4.1 Multi-Layer Perceptrons\n",
        "- **4.1.1 From Linear to Non-Linear Models**: Limitations of linear models, XOR problem, need for hidden layers\n",
        "- **4.1.2 Network Architecture**: Layer notation, network dimensions, matrix formulation\n",
        "- **4.1.3 Layer Types**: Input, hidden, and output layers\n",
        "\n",
        "### 4.2 Forward Propagation\n",
        "- **4.2.1 Matrix Formulation**: Mathematical description of forward pass\n",
        "- **4.2.2 Vectorized Implementation**: Batch processing for efficiency\n",
        "\n",
        "### 4.3 Backpropagation\n",
        "- **4.3.1 The Chain Rule**: Mathematical foundation\n",
        "- **4.3.2 Gradient Computation**: Computing gradients for all parameters\n",
        "- **4.3.3 Backpropagation Algorithm**: Complete algorithm for training\n",
        "\n",
        "### 4.4 Activation Functions\n",
        "- **4.4.1 Sigmoid and Tanh**: Traditional activation functions\n",
        "- **4.4.2 ReLU and Variants**: Modern activation functions\n",
        "- **4.4.3 Choosing Activation Functions**: Guidelines for selection\n",
        "\n",
        "### 4.5 Building Neural Networks\n",
        "- **4.5.1 Network Initialization**: Weight initialization strategies\n",
        "- **4.5.2 Training Process**: Complete training loop\n",
        "- **4.5.3 Practical Considerations**: Data preprocessing, regularization, monitoring\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Multi-layer networks** can learn non-linear patterns that linear models cannot\n",
        "- **Forward propagation** computes predictions by passing data through the network\n",
        "- **Backpropagation** computes gradients efficiently using the chain rule\n",
        "- **Activation functions** introduce non-linearity (ReLU is the default choice)\n",
        "- **Proper initialization** is crucial for training deep networks\n",
        "- **Neural networks** can solve complex problems like XOR and non-linearly separable data\n",
        "\n",
        "These concepts form the foundation for understanding and building deep neural networks. In the next chapters, we'll explore more advanced architectures and techniques."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
