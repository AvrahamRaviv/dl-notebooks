{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Bonus: Neural Network Experimental Lab\n",
    "\n",
    "This bonus notebook extends **Chapter 4 – Neural Networks** with practical experiments that deepen your intuition about how architectural and training choices affect learning.\n",
    "\n",
    "We will run a series of controlled experiments on simple datasets to see how different **activations, initializations, depths, and optimizers** change training dynamics and decision boundaries.\n",
    "\n",
    "## What we will explore\n",
    "\n",
    "1. **Comparing Activations** – Sigmoid vs Tanh vs ReLU in the same network\n",
    "2. **Impact of Initialization** – good vs bad initialization\n",
    "3. **Depth and Width Experiment** – from logistic regression to deeper MLPs\n",
    "4. **Optimizers and Learning Rates** – SGD, Momentum, Adam, and different learning rates\n",
    "5. **(Optional) Manual Backprop Verification** – sanity-checking gradients on a tiny network\n",
    "\n",
    "These experiments complement the theory in Chapter 4 and prepare you for designing and debugging your own networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We will use **PyTorch** for defining and training networks, **NumPy** for utilities, **matplotlib** for plots, and **scikit-learn** for generating synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Utility: create a 2D nonlinear dataset (moons) and dataloaders\n",
    "\n",
    "def make_moons_dataloaders(n_samples=600, batch_size=64, test_size=0.3):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=0.25, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test), (train_loader, test_loader), scaler\n",
    "\n",
    "\n",
    "# Utility: plot decision boundary for a 2D classifier\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision boundary\", scaler=None, ax=None, device=device):\n",
    "    model.eval()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                           np.linspace(x2_min, x2_max, 200))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    \n",
    "    if scaler is not None:\n",
    "        grid_scaled = scaler.transform(grid)\n",
    "    else:\n",
    "        grid_scaled = grid\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(grid_scaled, dtype=torch.float32, device=device)\n",
    "        logits = model(inputs)\n",
    "        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "    Z = probs.reshape(xx1.shape)\n",
    "    \n",
    "    contour = ax.contourf(xx1, xx2, Z, levels=20, cmap=\"RdBu\", alpha=0.6)\n",
    "    ax.contour(xx1, xx2, Z, levels=[0.5], colors=\"k\", linewidths=2)\n",
    "    \n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c=\"blue\", edgecolor=\"k\", label=\"Class 0\", alpha=0.7)\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c=\"red\", edgecolor=\"k\", label=\"Class 1\", alpha=0.7)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Comparing Activations: Sigmoid vs Tanh vs ReLU\n",
    "\n",
    "Activation functions strongly affect gradient flow and training speed.\n",
    "\n",
    "In this experiment we:\n",
    "\n",
    "- Use the **same architecture** (2-layer MLP: 2 → 32 → 2) and the same dataset (moons)\n",
    "- Train three networks that differ **only** in activation: Sigmoid, Tanh, ReLU\n",
    "- Track the training loss curves\n",
    "\n",
    "We expect to see that:\n",
    "\n",
    "- Sigmoid may train more slowly and can suffer from **saturation** (vanishing gradients)\n",
    "- Tanh is often better than Sigmoid but can still saturate\n",
    "- ReLU tends to converge faster on this kind of problem\n",
    "\n",
    "This visualizes the ideas discussed in the **Activation Functions** section of Chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=32, out_dim=2, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.act = nn.Sigmoid()\n",
    "        elif activation == \"tanh\":\n",
    "            self.act = nn.Tanh()\n",
    "        elif activation == \"relu\":\n",
    "            self.act = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=100, lr=1e-2, device=device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_losses.append(running_loss / len(train_loader.dataset))\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss_test = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                running_loss_test += loss.item() * xb.size(0)\n",
    "        test_losses.append(running_loss_test / len(test_loader.dataset))\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "# Run the activation comparison experiment\n",
    "(X_train, y_train, X_test, y_test), (train_loader, test_loader), scaler = make_moons_dataloaders()\n",
    "\n",
    "activations = [\"sigmoid\", \"tanh\", \"relu\"]\n",
    "results = {}\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\nTraining MLP with {act} activation\")\n",
    "    model = SimpleMLP(activation=act)\n",
    "    train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=150, lr=5e-2)\n",
    "    results[act] = (model, train_losses, test_losses)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for act in activations:\n",
    "    _, train_losses, test_losses = results[act][0], results[act][1], results[act][2]\n",
    "    plt.plot(train_losses, label=f\"{act} (train)\")\n",
    "plt.title(\"Training Loss vs Epoch for Different Activations\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize decision boundaries for the final models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for ax, act in zip(axes, activations):\n",
    "    model, _, _ = results[act]\n",
    "    plot_decision_boundary(model, np.vstack([X_train, X_test]), \n",
    "                           np.hstack([y_train, y_test]),\n",
    "                           title=f\"Activation: {act}\", scaler=scaler, ax=ax)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "- All three activations can eventually separate the moons dataset.\n",
    "- **Sigmoid** often yields slower training and can plateau earlier due to **vanishing gradients** when activations saturate.\n",
    "- **Tanh** is usually better behaved than Sigmoid (zero-centered), but can still saturate.\n",
    "- **ReLU** tends to converge faster and is the de facto default for hidden layers in modern networks.\n",
    "\n",
    "This aligns with the activation function guidelines from Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Impact of Initialization\n",
    "\n",
    "Initialization can dramatically influence how fast (or whether) a network learns.\n",
    "\n",
    "In this experiment we:\n",
    "\n",
    "- Use the same ReLU MLP (2 → 64 → 64 → 2)\n",
    "- Compare three initialization schemes:\n",
    "  1. **He initialization** (good for ReLU)\n",
    "  2. **Too small** weights (almost zero)\n",
    "  3. **Too large** weights (can cause saturation / exploding activations)\n",
    "- Train each model and compare loss curves\n",
    "\n",
    "From Chapter 4 you know that for ReLU, **He initialization** keeps the variance of activations roughly constant across layers, which helps gradients flow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=64, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def init_he(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def init_small(model, scale=1e-3):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=scale)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def init_large(model, scale=1.0):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=scale)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# Reuse moons data\n",
    "(X_train2, y_train2, X_test2, y_test2), (train_loader2, test_loader2), scaler2 = make_moons_dataloaders()\n",
    "\n",
    "inits = {\n",
    "    \"he\": init_he,\n",
    "    \"small\": lambda m: init_small(m, scale=1e-3),\n",
    "    \"large\": lambda m: init_large(m, scale=1.0),\n",
    "}\n",
    "\n",
    "init_results = {}\n",
    "\n",
    "for name, init_fn in inits.items():\n",
    "    print(f\"\\nTraining DeepMLP with {name} initialization\")\n",
    "    model = DeepMLP()\n",
    "    init_fn(model)\n",
    "    train_losses, test_losses = train_model(model, train_loader2, test_loader2, epochs=120, lr=1e-2)\n",
    "    init_results[name] = (model, train_losses, test_losses)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name in inits.keys():\n",
    "    _, train_losses, _ = init_results[name]\n",
    "    plt.plot(train_losses, label=f\"{name} init\")\n",
    "plt.title(\"Initialization Impact on Training Loss (ReLU MLP)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Depth and Width Experiment\n",
    "\n",
    "Here we illustrate how **network capacity** (depth and width) affects what decision boundaries a model can represent.\n",
    "\n",
    "We will train on the same moons dataset:\n",
    "\n",
    "1. **Logistic regression** (no hidden layer → linear boundary)\n",
    "2. **Shallow MLP** with 1 hidden layer (2 → 32 → 2)\n",
    "3. **Deeper MLP** with 2 hidden layers (2 → 32 → 32 → 2)\n",
    "\n",
    "We will then visualize their decision regions to see how depth allows increasingly complex boundaries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LogisticReg(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MLP1Hidden(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=32, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP2Hidden(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=32, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Use the same training data as before\n",
    "(X_train3, y_train3, X_test3, y_test3), (train_loader3, test_loader3), scaler3 = make_moons_dataloaders()\n",
    "\n",
    "models_depth = {\n",
    "    \"logistic\": LogisticReg(),\n",
    "    \"1-hidden\": MLP1Hidden(),\n",
    "    \"2-hidden\": MLP2Hidden(),\n",
    "}\n",
    "\n",
    "trained_models_depth = {}\n",
    "\n",
    "for name, model in models_depth.items():\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    train_losses, _ = train_model(model, train_loader3, test_loader3, epochs=150, lr=5e-2)\n",
    "    trained_models_depth[name] = model\n",
    "\n",
    "# Plot decision regions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "all_X = np.vstack([X_train3, X_test3])\n",
    "all_y = np.hstack([y_train3, y_test3])\n",
    "\n",
    "for ax, name in zip(axes, [\"logistic\", \"1-hidden\", \"2-hidden\"]):\n",
    "    model = trained_models_depth[name]\n",
    "    plot_decision_boundary(model, all_X, all_y, title=name, scaler=scaler3, ax=ax)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "- **Logistic regression** can only learn a **linear** decision boundary; it struggles to separate the interleaving moons.\n",
    "- A **single hidden layer** already allows a non-linear boundary that fits the data much better.\n",
    "- Adding a **second hidden layer** can further increase flexibility, often capturing subtler structure with smoother boundaries.\n",
    "\n",
    "This connects directly to the motivation for deeper networks in Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizers and Learning Rates\n",
    "\n",
    "So far we used vanilla SGD. In practice, optimization details matter a lot.\n",
    "\n",
    "In this experiment we:\n",
    "\n",
    "- Use the same 2-layer MLP (2 → 32 → 2) with ReLU\n",
    "- Compare three optimizers:\n",
    "  - **SGD**\n",
    "  - **SGD with Momentum**\n",
    "  - **Adam**\n",
    "- Compare two learning rates for SGD: a reasonable one and a too-large one\n",
    "\n",
    "We will plot loss curves to see differences in convergence speed and stability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=32, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_with_optimizer(optimizer_name, lr, train_loader, test_loader, epochs=80):\n",
    "    model = SmallMLP().to(device)\n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_losses.append(running_loss / len(train_loader.dataset))\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "\n",
    "# Use a fresh moons dataset\n",
    "(_, _, _, _), (train_loader_opt, test_loader_opt), _ = make_moons_dataloaders()\n",
    "\n",
    "optimizers = [\"sgd\", \"momentum\", \"adam\"]\n",
    "lr = 0.05\n",
    "opt_results = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training with optimizer: {opt}\")\n",
    "    losses = train_with_optimizer(opt, lr, train_loader_opt, test_loader_opt, epochs=80)\n",
    "    opt_results[opt] = losses\n",
    "\n",
    "# Learning rate experiment for SGD\n",
    "print(\"\\nLearning rate experiment with SGD\")\n",
    "sgd_good = train_with_optimizer(\"sgd\", lr=0.05, train_loader=train_loader_opt, test_loader=test_loader_opt, epochs=80)\n",
    "sgd_bad = train_with_optimizer(\"sgd\", lr=0.5, train_loader=train_loader_opt, test_loader=test_loader_opt, epochs=80)\n",
    "\n",
    "# Plot optimizer comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "for opt in optimizers:\n",
    "    plt.plot(opt_results[opt], label=opt)\n",
    "plt.title(\"Optimizer Comparison (Training Loss)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sgd_good, label=\"SGD lr=0.05\")\n",
    "plt.plot(sgd_bad, label=\"SGD lr=0.5\")\n",
    "plt.title(\"Learning Rate Effect (SGD)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "- **Adam** and **SGD with Momentum** usually converge faster and more smoothly than plain SGD.\n",
    "- Too large a learning rate can cause loss to oscillate or diverge, even for simple problems.\n",
    "- In practice you should **tune both the optimizer and the learning rate** rather than relying on defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Optional) Manual Backprop Verification\n",
    "\n",
    "To build trust in backpropagation, we can verify gradients on a tiny network by comparing them to **numerical gradients**.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Define a very small network: 2 → 2 → 1 with Tanh activation\n",
    "- Use a single input–target pair\n",
    "- Compute the loss and gradients using PyTorch autograd\n",
    "- Approximate the gradients with finite differences\n",
    "- Compare the two sets of gradients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.act = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def numerical_gradient(model, x, y, loss_fn, eps=1e-4):\n",
    "    grads = {}\n",
    "    # Ensure we work on CPU tensors for simplicity\n",
    "    x = x.detach().cpu()\n",
    "    y = y.detach().cpu()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_data = param.data.clone()\n",
    "        grad_approx = torch.zeros_like(param_data)\n",
    "        \n",
    "        it = np.nditer(param_data.cpu().numpy(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            original = param_data[idx].item()\n",
    "            \n",
    "            # f(theta + eps)\n",
    "            param_data[idx] = original + eps\n",
    "            param.data = param_data\n",
    "            y_pred_pos = model(x)\n",
    "            loss_pos = loss_fn(y_pred_pos, y)\n",
    "            \n",
    "            # f(theta - eps)\n",
    "            param_data[idx] = original - eps\n",
    "            param.data = param_data\n",
    "            y_pred_neg = model(x)\n",
    "            loss_neg = loss_fn(y_pred_neg, y)\n",
    "            \n",
    "            # central difference\n",
    "            grad_approx[idx] = (loss_pos - loss_neg) / (2 * eps)\n",
    "            \n",
    "            # restore\n",
    "            param_data[idx] = original\n",
    "            param.data = param_data\n",
    "            \n",
    "            it.iternext()\n",
    "        \n",
    "        grads[name] = grad_approx\n",
    "    return grads\n",
    "\n",
    "\n",
    "# Gradient check\n",
    "torch.manual_seed(0)\n",
    "net = TinyNet()\n",
    "net = net.to(device)\n",
    "\n",
    "x = torch.tensor([[0.5, -1.0]], dtype=torch.float32, device=device)\n",
    "y = torch.tensor([[1.0]], dtype=torch.float32, device=device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Autograd gradients\n",
    "net.zero_grad()\n",
    "y_pred = net(x)\n",
    "loss = loss_fn(y_pred, y)\n",
    "loss.backward()\n",
    "autograd_grads = {name: p.grad.detach().cpu().clone() for name, p in net.named_parameters()}\n",
    "\n",
    "# Numerical gradients (on CPU copy)\n",
    "net_cpu = TinyNet()\n",
    "net_cpu.load_state_dict(net.state_dict())\n",
    "num_grads = numerical_gradient(net_cpu, x.detach().cpu(), y.detach().cpu(), loss_fn)\n",
    "\n",
    "print(\"Gradient check (autograd vs numerical):\")\n",
    "for name in autograd_grads.keys():\n",
    "    print(f\"\\nParameter: {name}\")\n",
    "    print(\"Autograd:\\n\", autograd_grads[name].numpy())\n",
    "    print(\"Numerical:\\n\", num_grads[name].detach().numpy())\n",
    "    diff = autograd_grads[name] - num_grads[name]\n",
    "    print(\"Max abs diff:\", diff.abs().max().item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this experimental lab we:\n",
    "\n",
    "- Compared **Sigmoid, Tanh, and ReLU** activations and saw how they affect training speed and decision boundaries.\n",
    "- Demonstrated how **initialization** (He vs too small vs too large) influences convergence.\n",
    "- Showed how **depth** changes model capacity, from linear logistic regression to deeper MLPs.\n",
    "- Compared **optimizers** (SGD, Momentum, Adam) and learning rates, highlighting convergence and stability issues.\n",
    "- Performed a small **gradient check** to verify that backpropagated gradients match numerical approximations.\n",
    "\n",
    "These experiments bring the theory of Chapter 4 to life and provide intuition for designing, initializing, and training your own neural networks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
