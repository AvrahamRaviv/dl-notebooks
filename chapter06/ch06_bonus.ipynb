{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6 Bonus: Practical RNN & LSTM Lab\n",
        "\n",
        "This bonus notebook extends **Chapter 6 – Recurrent Neural Networks** with hands-on sequence modeling experiments.\n",
        "\n",
        "We will focus on:\n",
        "\n",
        "1. **Sequence Generation (Toy Text)** – character-level RNN/LSTM that generates text\n",
        "2. **Sequence Prediction (Sine Wave)** – RNN/LSTM predicting the next step in a time series\n",
        "3. **RNN vs LSTM on Long Sequences** – illustrating vanishing gradients and long-term dependencies\n",
        "4. **Bidirectional RNN for Classification** – simple palindrome (or structured) sequence classification\n",
        "5. **Hidden State Visualization (Optional)** – seeing how hidden states evolve over time\n",
        "\n",
        "These experiments bring the abstract concepts from Chapter 6 to life and give you intuition for how recurrent networks behave on real sequence tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "We will use **PyTorch** for RNNs/LSTMs and **NumPy** / **matplotlib** for data generation and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Plot style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Sequence Generation (Toy Character-Level RNN)\n",
        "\n",
        "RNNs (and LSTMs) are often used for **language modeling** – predicting the next character or word in a sequence.\n",
        "\n",
        "In this section we:\n",
        "\n",
        "- Take a small text corpus (e.g., a nursery rhyme or short paragraph)\n",
        "- Build a **character-level vocabulary**\n",
        "- Train a small LSTM to predict the next character\n",
        "- Use the trained model to **generate new text** one character at a time\n",
        "\n",
        "This demonstrates a simple many-to-many generation setup, directly connected to the sequence modeling ideas in Chapter 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toy text corpus (you can replace this with any short text you like)\n",
        "text = (\n",
        "    \"Twinkle, twinkle, little star,\\n\"\n",
        "    \"How I wonder what you are!\\n\"\n",
        "    \"Up above the world so high,\\n\"\n",
        "    \"Like a diamond in the sky.\\n\"\n",
        ")\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Characters:\", chars)\n",
        "\n",
        "# Encode entire text as indices\n",
        "encoded = np.array([stoi[ch] for ch in text], dtype=np.int64)\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_len=40):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.seq_len]\n",
        "        y = self.data[idx+1:idx+self.seq_len+1]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "seq_len = 40\n",
        "batch_size = 32\n",
        "\n",
        "dataset = CharDataset(encoded, seq_len=seq_len)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "model = CharLSTM(vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_char_model(model, loader, epochs=20):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(logits.view(-1, vocab_size), yb.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * xb.size(0)\n",
        "        epoch_loss /= len(loader.dataset)\n",
        "        losses.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - loss={epoch_loss:.3f}\")\n",
        "    return losses\n",
        "\n",
        "losses_char = train_char_model(model, loader, epochs=20)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(losses_char)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Char-level LSTM Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def generate_text(model, start_text=\"Twinkle, \", length=200):\n",
        "    model.eval()\n",
        "    chars_out = list(start_text)\n",
        "    hidden = None\n",
        "    x = torch.tensor([[stoi[ch] for ch in start_text]], dtype=torch.long, device=device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(length):\n",
        "            logits, hidden = model(x[:, -1:].contiguous(), hidden)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            ch = itos[idx]\n",
        "            chars_out.append(ch)\n",
        "            x = torch.cat([x, torch.tensor([[idx]], dtype=torch.long, device=device)], dim=1)\n",
        "    return \"\".join(chars_out)\n",
        "\n",
        "sample = generate_text(model, start_text=\"Twinkle, \", length=200)\n",
        "print(\"\\nGenerated text:\\n\", sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even with a tiny dataset and a small model, you should see that the generated text gradually starts to mimic the structure and vocabulary of the input rhyme (though it may still be noisy). This is a concrete example of **sequence generation** with RNNs/LSTMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Sequence Prediction: Sine Wave Forecasting\n",
        "\n",
        "RNNs are natural models for **time series**.\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Generate noisy sine waves\n",
        "- Build input–target pairs where the model sees a window of $T$ steps and predicts the **next value**\n",
        "- Train a small LSTM to perform this one-step-ahead prediction\n",
        "- Plot the model’s predictions vs the true sine wave\n",
        "\n",
        "This demonstrates how RNNs/LSTMs maintain a summary of recent history to make future predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sine wave data\n",
        "\n",
        "def generate_sine_data(n_samples=1000, noise_std=0.1):\n",
        "    x = np.linspace(0, 40 * np.pi, n_samples)\n",
        "    y = np.sin(x) + noise_std * np.random.randn(n_samples)\n",
        "    return y\n",
        "\n",
        "series = generate_sine_data()\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(series[:400])\n",
        "plt.title(\"Noisy Sine Wave (first 400 points)\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def create_sequences(series, seq_len=50):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(series) - seq_len):\n",
        "        X.append(series[i:i+seq_len])\n",
        "        Y.append(series[i+seq_len])\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    Y = np.array(Y, dtype=np.float32)\n",
        "    return X, Y\n",
        "\n",
        "seq_len_sine = 50\n",
        "X, Y = create_sequences(series, seq_len=seq_len_sine)\n",
        "\n",
        "# Train/test split\n",
        "split = int(0.8 * len(X))\n",
        "X_train_s, X_test_s = X[:split], X[split:]\n",
        "Y_train_s, Y_test_s = Y[:split], Y[split:]\n",
        "\n",
        "class SineDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]).unsqueeze(-1), torch.tensor(self.Y[idx])\n",
        "\n",
        "train_ds_s = SineDataset(X_train_s, Y_train_s)\n",
        "test_ds_s = SineDataset(X_test_s, Y_test_s)\n",
        "\n",
        "train_loader_s = DataLoader(train_ds_s, batch_size=32, shuffle=True)\n",
        "test_loader_s = DataLoader(test_ds_s, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "class SineLSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim=32, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x, hidden=None):\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out_last = out[:, -1, :]\n",
        "        y_hat = self.fc(out_last)\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "sine_model = SineLSTM(hidden_dim=32).to(device)\n",
        "optimizer_s = torch.optim.Adam(sine_model.parameters(), lr=1e-3)\n",
        "criterion_s = nn.MSELoss()\n",
        "\n",
        "\n",
        "def train_sine(model, train_loader, test_loader, epochs=15):\n",
        "    train_losses, test_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).unsqueeze(-1)\n",
        "            optimizer_s.zero_grad()\n",
        "            y_hat = model(xb)\n",
        "            loss = criterion_s(y_hat, yb)\n",
        "            loss.backward()\n",
        "            optimizer_s.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        train_loss = running / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        model.eval()\n",
        "        running = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device).unsqueeze(-1)\n",
        "                y_hat = model(xb)\n",
        "                loss = criterion_s(y_hat, yb)\n",
        "                running += loss.item() * xb.size(0)\n",
        "        test_loss = running / len(test_loader.dataset)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - train_loss={train_loss:.4f}, test_loss={test_loss:.4f}\")\n",
        "    return train_losses, test_losses\n",
        "\n",
        "train_losses_s, test_losses_s = train_sine(sine_model, train_loader_s, test_loader_s, epochs=15)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(train_losses_s, label=\"Train loss\")\n",
        "plt.plot(test_losses_s, label=\"Test loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Sine Wave Prediction Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions on a segment\n",
        "sine_model.eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(test_loader_s))\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    y_hat = sine_model(xb).cpu().squeeze().numpy()\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(yb.numpy()[:100], label=\"True\")\n",
        "plt.plot(y_hat[:100], label=\"Predicted\")\n",
        "plt.title(\"Sine Wave: True vs Predicted (segment)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. RNN vs LSTM on Long Sequences (Echo Task)\n",
        "\n",
        "One of the key motivations for LSTMs is handling **long-term dependencies**.\n",
        "\n",
        "We will create an **echo task**:\n",
        "\n",
        "- Input: a sequence of length $T$ containing random bits (0 or 1)\n",
        "- Target: output the **first element** of the sequence at the final time step\n",
        "\n",
        "This requires the network to remember information from the very beginning. We will:\n",
        "\n",
        "- Train a small **vanilla RNN** on this task\n",
        "- Train a small **LSTM** of the same size\n",
        "- Compare their training losses and final accuracies\n",
        "\n",
        "We expect the LSTM to succeed much more easily, while the RNN struggles due to **vanishing gradients** over long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Echo dataset\n",
        "\n",
        "class EchoDataset(Dataset):\n",
        "    def __init__(self, n_sequences=2000, seq_len=50):\n",
        "        self.n_sequences = n_sequences\n",
        "        self.seq_len = seq_len\n",
        "        self.data = np.random.randint(0, 2, size=(n_sequences, seq_len)).astype(np.float32)\n",
        "        self.targets = self.data[:, 0]  # echo of first element\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_sequences\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.targets[idx]\n",
        "        return torch.tensor(x).unsqueeze(-1), torch.tensor(y)\n",
        "\n",
        "seq_len_echo = 50\n",
        "train_ds_e = EchoDataset(n_sequences=2000, seq_len=seq_len_echo)\n",
        "test_ds_e = EchoDataset(n_sequences=500, seq_len=seq_len_echo)\n",
        "\n",
        "train_loader_e = DataLoader(train_ds_e, batch_size=64, shuffle=True)\n",
        "test_loader_e = DataLoader(test_ds_e, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "class EchoRNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out_last = out[:, -1, :]\n",
        "        y_hat = torch.sigmoid(self.fc(out_last))\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "class EchoLSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out_last = out[:, -1, :]\n",
        "        y_hat = torch.sigmoid(self.fc(out_last))\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "\n",
        "def train_echo(model, train_loader, test_loader, epochs=20, lr=1e-2):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    train_losses, test_losses, test_accs = [], [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).unsqueeze(-1)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(xb)\n",
        "            loss = criterion(y_hat, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        train_loss = running / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        model.eval()\n",
        "        running = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device).unsqueeze(-1)\n",
        "                y_hat = model(xb)\n",
        "                loss = criterion(y_hat, yb)\n",
        "                running += loss.item() * xb.size(0)\n",
        "                preds = (y_hat > 0.5).float()\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        test_loss = running / len(test_loader.dataset)\n",
        "        test_acc = correct / total\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - train_loss={train_loss:.3f}, test_loss={test_loss:.3f}, test_acc={test_acc:.3f}\")\n",
        "    \n",
        "    return train_losses, test_losses, test_accs\n",
        "\n",
        "\n",
        "print(\"\\nTraining EchoRNN (vanilla RNN)...\")\n",
        "echo_rnn = EchoRNN(hidden_dim=16)\n",
        "train_rnn_loss, test_rnn_loss, test_rnn_acc = train_echo(echo_rnn, train_loader_e, test_loader_e, epochs=20, lr=1e-2)\n",
        "\n",
        "print(\"\\nTraining EchoLSTM...\")\n",
        "echo_lstm = EchoLSTM(hidden_dim=16)\n",
        "train_lstm_loss, test_lstm_loss, test_lstm_acc = train_echo(echo_lstm, train_loader_e, test_loader_e, epochs=20, lr=1e-2)\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(test_rnn_acc, label=\"RNN test acc\")\n",
        "plt.plot(test_lstm_acc, label=\"LSTM test acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Echo Task: RNN vs LSTM\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see that the LSTM achieves near-perfect accuracy on this task, while the vanilla RNN often plateaus at much lower accuracy. This vividly illustrates the **long-term dependency problem** and why LSTMs (and GRUs) are so useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Bidirectional RNN for Palindrome Classification\n",
        "\n",
        "Bidirectional RNNs process a sequence **from both directions**, giving the model access to past and future context.\n",
        "\n",
        "We will build a toy classification task:\n",
        "\n",
        "- Input: a sequence of digits (0–9) of fixed length\n",
        "- Label: 1 if the sequence is a **palindrome**, 0 otherwise\n",
        "\n",
        "We will:\n",
        "\n",
        "- Train a simple unidirectional RNN classifier\n",
        "- Train a bidirectional RNN classifier\n",
        "- Compare their accuracies\n",
        "\n",
        "This shows how using information from both ends of the sequence can improve performance on tasks that depend on the entire sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PalindromeDataset(Dataset):\n",
        "    def __init__(self, n_samples=2000, seq_len=5):\n",
        "        self.n_samples = n_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.data, self.labels = self._generate()\n",
        "    \n",
        "    def _generate(self):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for _ in range(self.n_samples):\n",
        "            if np.random.rand() < 0.5:\n",
        "                # Palindrome\n",
        "                half = np.random.randint(0, 10, size=(self.seq_len // 2,))\n",
        "                if self.seq_len % 2 == 1:\n",
        "                    middle = np.random.randint(0, 10, size=(1,))\n",
        "                    seq = np.concatenate([half, middle, half[::-1]])\n",
        "                else:\n",
        "                    seq = np.concatenate([half, half[::-1]])\n",
        "                label = 1\n",
        "            else:\n",
        "                # Non-palindrome\n",
        "                seq = np.random.randint(0, 10, size=(self.seq_len,))\n",
        "                # Ensure it's not accidentally a palindrome\n",
        "                if np.all(seq == seq[::-1]):\n",
        "                    seq[0] = (seq[0] + 1) % 10\n",
        "                label = 0\n",
        "            data.append(seq)\n",
        "            labels.append(label)\n",
        "        return np.array(data, dtype=np.int64), np.array(labels, dtype=np.int64)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "seq_len_pal = 7\n",
        "train_ds_p = PalindromeDataset(n_samples=3000, seq_len=seq_len_pal)\n",
        "test_ds_p = PalindromeDataset(n_samples=1000, seq_len=seq_len_pal)\n",
        "\n",
        "train_loader_p = DataLoader(train_ds_p, batch_size=64, shuffle=True)\n",
        "test_loader_p = DataLoader(test_ds_p, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "class UniRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=10, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out_last = out[:, -1, :]\n",
        "        y_hat = torch.sigmoid(self.fc(out_last))\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "class BiRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=10, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out_last = out[:, -1, :]  # concat of forward/backward hidden states\n",
        "        y_hat = torch.sigmoid(self.fc(out_last))\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "def train_palindrome(model, train_loader, test_loader, epochs=15, lr=1e-2):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    train_accs, test_accs = [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).unsqueeze(-1).float()\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(xb)\n",
        "            loss = criterion(y_hat, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = (y_hat > 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "        train_acc = correct / total\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device).unsqueeze(-1).float()\n",
        "                y_hat = model(xb)\n",
        "                preds = (y_hat > 0.5).float()\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        test_acc = correct / total\n",
        "        test_accs.append(test_acc)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - train_acc={train_acc:.3f}, test_acc={test_acc:.3f}\")\n",
        "    return train_accs, test_accs\n",
        "\n",
        "\n",
        "print(\"\\nTraining unidirectional RNN classifier...\")\n",
        "uni_model = UniRNNClassifier()\n",
        "train_uni_acc, test_uni_acc = train_palindrome(uni_model, train_loader_p, test_loader_p, epochs=15, lr=1e-2)\n",
        "\n",
        "print(\"\\nTraining bidirectional RNN classifier...\")\n",
        "bi_model = BiRNNClassifier()\n",
        "train_bi_acc, test_bi_acc = train_palindrome(bi_model, train_loader_p, test_loader_p, epochs=15, lr=1e-2)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(test_uni_acc, label=\"Unidirectional test acc\")\n",
        "plt.plot(test_bi_acc, label=\"Bidirectional test acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Palindrome Classification: Uni vs Bi RNN\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In many runs, the bidirectional RNN should achieve higher and more stable accuracy, because it can use information from **both ends** of the sequence, which is especially helpful for palindrome detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Hidden State Visualization (Optional)\n",
        "\n",
        "Finally, we visualize how hidden states **evolve over time** in the sine-wave LSTM.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Take a trained `SineLSTM`\n",
        "- Feed a sine segment through the network\n",
        "- Collect hidden states at each time step\n",
        "- Use PCA to project hidden states to 2D\n",
        "- Plot the trajectory of hidden states over time\n",
        "\n",
        "This gives some geometric intuition for how the RNN’s internal state changes as it processes a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Take a single long sequence segment from the sine series\n",
        "segment = series[split:split+seq_len_sine+100]  # some part from test region\n",
        "x_segment, _ = create_sequences(segment, seq_len=seq_len_sine)\n",
        "\n",
        "x0 = torch.tensor(x_segment[0]).unsqueeze(0).unsqueeze(-1).to(device)  # (1,T,1)\n",
        "\n",
        "sine_model.eval()\n",
        "hidden_states = []\n",
        "with torch.no_grad():\n",
        "    out, (h_n, c_n) = sine_model.lstm(x0)\n",
        "    # out: (1,T,hidden_dim)\n",
        "    hs = out.squeeze(0).cpu().numpy()  # (T,hidden_dim)\n",
        "    hidden_states = hs\n",
        "\n",
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "hs_2d = pca.fit_transform(hidden_states)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(hs_2d[:,0], hs_2d[:,1], c=np.arange(len(hs_2d)), cmap=\"viridis\")\n",
        "for i in range(0, len(hs_2d), 5):\n",
        "    plt.text(hs_2d[i,0], hs_2d[i,1], str(i), fontsize=8)\n",
        "plt.colorbar(label=\"Time step\")\n",
        "plt.title(\"Hidden State Trajectory (SineLSTM)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see the hidden state moving along a smooth trajectory as it tracks the phase of the sine wave. Different regions of the trajectory roughly correspond to different phases of the underlying periodic signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this RNN/LSTM lab we:\n",
        "\n",
        "- Built a **character-level LSTM** to generate text from a tiny corpus.\n",
        "- Trained an LSTM to **predict the next value** in a noisy sine wave.\n",
        "- Compared a vanilla RNN vs an LSTM on a synthetic **long-term dependency** task (echo), showing why gating helps.\n",
        "- Used a **bidirectional RNN** for a toy palindrome classification task and saw the benefit of looking both forward and backward.\n",
        "- Visualized the **hidden state trajectory** of an LSTM over time using PCA.\n",
        "\n",
        "These experiments complement Chapter 6 by giving concrete, runnable examples of sequence generation, prediction, long-term dependency handling, bidirectionality, and internal state dynamics."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
