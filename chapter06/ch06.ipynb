{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Recurrent Neural Networks (RNNs)\n",
    "\n",
    "This notebook covers **Chapter 6** of the Deep Learning in Hebrew book, focusing on Recurrent Neural Networks (RNNs). We'll learn how to build networks that can process sequential data and maintain memory of past information.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The advantage of convolutional layers over FC layers is the utilization of the spatial relationship between different elements in the data, such as the relationship between nearby pixels in an image. However, there are other types of data where the relationship is not spatial but **temporal** - the order of elements is important, such as in text, speech, DNA sequences, stock prices, etc.\n",
    "\n",
    "Unlike spatial data, temporal data has a **sequence** - the order of elements matters, and the same element cannot appear multiple times. For this reason, convolutional networks are not suitable for this type of data, and a different architecture is needed.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data. They receive a sequence of vectors and output a single vector, where the single vector encodes relationships over the original data. This vector can then be passed to an FC layer for the task at hand.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 6.1 Sequence Models\n",
    "- 6.1.1 [Vanilla Recurrent Neural Networks](#611-vanilla-recurrent-neural-networks)\n",
    "- 6.1.2 [Learning Parameters](#612-learning-parameters)\n",
    "\n",
    "### 6.2 RNN Architectures\n",
    "- 6.2.1 [Long Short-Term Memory (LSTM)](#621-long-short-term-memory-lstm)\n",
    "- 6.2.2 [Gated Recurrent Units (GRU)](#622-gated-recurrent-units-gru)\n",
    "- 6.2.3 [Deep RNN](#623-deep-rnn)\n",
    "- 6.2.4 [Bidirectional RNN](#624-bidirectional-rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Sequence Models\n",
    "\n",
    "## 6.1.1 Vanilla Recurrent Neural Networks\n",
    "\n",
    "Recurrent networks are a generalization of deep neural networks, where they contain weights that give meaning to the **order of input elements**. We can look at these weights as an internal memory component, where in addition to the input vector, there is an additional variable component that depends on past values.\n",
    "\n",
    "Given an input vector $x_t$, it is multiplied by weight $w_{xh}$ and enters the memory component $h_t$, where $h_t$ is a function of $x_t$ and $h_{t-1}$:\n",
    "\n",
    "$$h_t = f(h_{t-1}, x_t)$$\n",
    "\n",
    "In addition to the weights operating on the input vector, there are also weights operating on the internal weights (memory component) - $w_{hh}$, and weights operating on the output of this component - $w_{hy}$. The weights $w_{xh}, w_{hh}, w_{hy}$ are shared and updated together.\n",
    "\n",
    "Also, the function $f_w$ is constant for all elements, for example tanh, sigmoid, or ReLU. Formally, we can write:\n",
    "\n",
    "$$h_t = f_w(w_{hh}h_{t-1} + w_{xh}x_t), \\quad f_w = \\tanh/\\text{ReLU}/\\text{sigmoid}$$\n",
    "\n",
    "$$y_t = w_{hy}h_t$$\n",
    "\n",
    "### RNN Architectures\n",
    "\n",
    "There are several different RNN models, suitable for different problems:\n",
    "\n",
    "- **One to Many**: There is a single input and we want to output a sequence of outputs, for example - generating a description for an image (Image captioning).\n",
    "- **Many to One**: A sequence of inputs and a single output - for example, to classify a sentence and determine its sentiment - whether it is positive or negative.\n",
    "- **Many to Many**: For each input sequence there is an output sequence, for example translation from one language to another - input sequence and output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla RNN Implementation from Scratch\n",
    "class VanillaRNN:\n",
    "    \"\"\"\n",
    "    Simple Vanilla RNN implementation from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "        self.b_y = np.zeros((1, output_size))\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_seq : list of arrays, shape (T, input_size)\n",
    "            Input sequence\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_states : list of arrays\n",
    "            Hidden states at each time step\n",
    "        y_outputs : list of arrays\n",
    "            Outputs at each time step\n",
    "        \"\"\"\n",
    "        T = len(x_seq)\n",
    "        h_states = []\n",
    "        y_outputs = []\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h_prev = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Compute hidden state\n",
    "            h_t = np.tanh(np.dot(x_seq[t], self.W_xh) + \n",
    "                         np.dot(h_prev, self.W_hh) + self.b_h)\n",
    "            \n",
    "            # Compute output\n",
    "            y_t = np.dot(h_t, self.W_hy) + self.b_y\n",
    "            \n",
    "            h_states.append(h_t)\n",
    "            y_outputs.append(y_t)\n",
    "            h_prev = h_t\n",
    "        \n",
    "        return h_states, y_outputs\n",
    "\n",
    "# Example: Many-to-One RNN\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "rnn = VanillaRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Create a sequence of inputs\n",
    "T = 5\n",
    "x_sequence = [np.random.randn(1, input_size) for _ in range(T)]\n",
    "\n",
    "# Forward pass\n",
    "h_states, y_outputs = rnn.forward(x_sequence)\n",
    "\n",
    "print(\"Vanilla RNN - Many-to-One Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input sequence length: {T}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"\\nHidden states shape at each step: {h_states[0].shape}\")\n",
    "print(f\"Outputs shape at each step: {y_outputs[0].shape}\")\n",
    "print(f\"\\nFinal output (for classification): {y_outputs[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RNN Architecture\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Many-to-One\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-0.5, 5.5)\n",
    "ax1.set_ylim(-0.5, 3.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Many-to-One RNN', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "for i in range(4):\n",
    "    circle = plt.Circle((i, 2.5), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(i, 2.5, f'$x_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    if i < 3:\n",
    "        ax1.arrow(i+0.3, 2.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Hidden states\n",
    "for i in range(4):\n",
    "    circle = plt.Circle((i, 1.5), 0.3, color='lightgreen', ec='black', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(i, 1.5, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    if i < 3:\n",
    "        ax1.arrow(i+0.3, 1.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    # Recurrent connection\n",
    "    if i > 0:\n",
    "        ax1.plot([i-0.3, i-0.3, i-0.3], [1.8, 2.2, 1.8], 'r-', linewidth=1.5, alpha=0.5)\n",
    "        ax1.arrow(i-0.3, 1.8, 0, -0.2, head_width=0.08, head_length=0.08, fc='red', ec='red', alpha=0.5)\n",
    "\n",
    "# Output\n",
    "circle = plt.Circle((3, 0.5), 0.3, color='lightyellow', ec='black', linewidth=2)\n",
    "ax1.add_patch(circle)\n",
    "ax1.text(3, 0.5, '$y$', ha='center', va='center', fontsize=12, weight='bold')\n",
    "ax1.arrow(3, 1.2, 0, -0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Many-to-Many\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-0.5, 5.5)\n",
    "ax2.set_ylim(-0.5, 3.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Many-to-Many RNN', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "for i in range(4):\n",
    "    circle = plt.Circle((i, 2.5), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(i, 2.5, f'$x_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    if i < 3:\n",
    "        ax2.arrow(i+0.3, 2.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Hidden states\n",
    "for i in range(4):\n",
    "    circle = plt.Circle((i, 1.5), 0.3, color='lightgreen', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(i, 1.5, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    if i < 3:\n",
    "        ax2.arrow(i+0.3, 1.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    # Recurrent connection\n",
    "    if i > 0:\n",
    "        ax2.plot([i-0.3, i-0.3, i-0.3], [1.8, 2.2, 1.8], 'r-', linewidth=1.5, alpha=0.5)\n",
    "        ax2.arrow(i-0.3, 1.8, 0, -0.2, head_width=0.08, head_length=0.08, fc='red', ec='red', alpha=0.5)\n",
    "\n",
    "# Output sequence\n",
    "for i in range(4):\n",
    "    circle = plt.Circle((i, 0.5), 0.3, color='lightyellow', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(i, 0.5, f'$y_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    ax2.arrow(i, 1.2, 0, -0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RNN Architecture Types:\")\n",
    "print(\"  - Many-to-One: Sequence → Single output (e.g., sentiment classification)\")\n",
    "print(\"  - Many-to-Many: Sequence → Sequence (e.g., machine translation)\")\n",
    "print(\"  - One-to-Many: Single input → Sequence (e.g., image captioning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Learning Parameters\n",
    "\n",
    "The learning of the network is done in a similar way to networks in previous chapters. For data $x = (x_1, \\ldots, x_n), (y_1, \\ldots, y_n)$, we define the cost function:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{n}\\sum_i L(\\hat{y}_i, y_i, \\theta)$$\n",
    "\n",
    "where the function $L(\\hat{y}_i, y_i, \\theta)$ depends on the task - for classification tasks we use cross entropy, and for regression tasks we use the MSE criterion.\n",
    "\n",
    "Training is performed using GD, but we cannot use regular backpropagation because the weights operate on multiple inputs - $w_{xh}$ operates on all inputs and $w_{hh}$ operates on all memory components. To update the weights, we use **Backpropagation Through Time (BPTT)** - we look at the network as one large network, calculate the gradient for each weight, and then sum or average all the gradients.\n",
    "\n",
    "If the input has $n$ elements, then there are $n$ memory components, and $n-1$ uses of $w_{hh}$. The gradient for each use is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{hh}} = \\sum_{t=1}^{n-1} \\frac{\\partial L}{\\partial w_{hh}^{(t)}} \\quad \\text{or} \\quad \\frac{\\partial L}{\\partial w_{hh}} = \\frac{1}{n-1}\\sum_{t=1}^{n-1} \\frac{\\partial L}{\\partial w_{hh}^{(t)}}$$\n",
    "\n",
    "Since the weights are identical throughout the network, $w_{hh}^{(t)} = w_{hh}$, and the change in time will only be after performing BPTT and will be relevant to the entire vector.\n",
    "\n",
    "### The Vanishing/Exploding Gradient Problem\n",
    "\n",
    "The simple form of BPTT creates a problem with the gradient. Suppose we write the hidden state explicitly:\n",
    "\n",
    "$$h_t = f(z_t) = f(w_{hh}h_{t-1} + w_{xh}x_t + b_h)$$\n",
    "\n",
    "According to the chain rule:\n",
    "\n",
    "$$\\frac{\\partial h_n}{\\partial x_1} = \\frac{\\partial h_n}{\\partial h_{n-1}} \\times \\frac{\\partial h_{n-1}}{\\partial h_{n-2}} \\times \\cdots \\times \\frac{\\partial h_2}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial x_1}$$\n",
    "\n",
    "Since $w_{hh}$ is constant with respect to time for a single input vector, we get:\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_{t-1}} = f'(z_t) \\cdot w_{hh}$$\n",
    "\n",
    "If we substitute this in the chain rule, we get that for calculating the derivative $\\frac{\\partial h_n}{\\partial x_1}$, we multiply $n-1$ times by $w_{hh}$. Therefore, if $||w_{hh}|| > 1$, then the gradient will explode, and if $||w_{hh}|| < 1$, the gradient will vanish.\n",
    "\n",
    "For the problem of exploding gradient, we can perform **gradient clipping** - if the gradient is too large, we normalize it:\n",
    "\n",
    "$$\\text{if } ||g|| > c, \\text{ then } g = c\\frac{g}{||g||}$$\n",
    "\n",
    "The problem of vanishing gradient does not cause calculations of huge numbers, but it actually prevents learning long-term dependencies. If the sequence is long, then the gradient through BPTT will be very small, and there is almost no influence of the first word on the last word. In other words - the vanishing gradient causes a problem of **Long-term dependency**, meaning it is difficult to learn data that changes slowly or sequences that are long.\n",
    "\n",
    "Because of this problem, we do not use the classic RNN (also called **Vanilla RNN**), but rather more advanced architectures that will be detailed in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Vanishing/Exploding Gradient Problem\n",
    "def compute_gradient_through_time(w_hh, n_steps, activation='tanh'):\n",
    "    \"\"\"\n",
    "    Compute gradient through time for RNN.\n",
    "    \n",
    "    Shows how gradient changes as we backpropagate through time.\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    \n",
    "    if activation == 'tanh':\n",
    "        # For tanh, derivative is bounded by 1\n",
    "        # But we multiply by w_hh each step\n",
    "        grad = 1.0\n",
    "        for t in range(n_steps):\n",
    "            grad = grad * w_hh  # Simplified: assuming f'(z) ≈ 1\n",
    "            gradients.append(grad)\n",
    "    elif activation == 'sigmoid':\n",
    "        # For sigmoid, derivative is bounded by 0.25\n",
    "        grad = 1.0\n",
    "        for t in range(n_steps):\n",
    "            grad = grad * 0.25 * w_hh\n",
    "            gradients.append(grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Test different weight values\n",
    "n_steps = 20\n",
    "w_values = [0.5, 0.9, 1.0, 1.1, 1.5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Vanishing gradient (w < 1)\n",
    "ax1 = axes[0]\n",
    "for w in [0.5, 0.9]:\n",
    "    grads = compute_gradient_through_time(w, n_steps, 'tanh')\n",
    "    ax1.plot(range(1, n_steps+1), grads, label=f'$w_{{hh}} = {w}$', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Time Steps', fontsize=12)\n",
    "ax1.set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "ax1.set_title('Vanishing Gradient Problem ($w_{{hh}} < 1$)', fontsize=14, weight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Exploding gradient (w > 1)\n",
    "ax2 = axes[1]\n",
    "for w in [1.1, 1.5]:\n",
    "    grads = compute_gradient_through_time(w, n_steps, 'tanh')\n",
    "    ax2.plot(range(1, n_steps+1), grads, label=f'$w_{{hh}} = {w}$', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Time Steps', fontsize=12)\n",
    "ax2.set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "ax2.set_title('Exploding Gradient Problem ($w_{{hh}} > 1$)', fontsize=14, weight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient Problems in RNNs:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Vanishing Gradient ($w_{{hh}} < 1$):\")\n",
    "print(\"  - Gradient becomes exponentially small\")\n",
    "print(\"  - Cannot learn long-term dependencies\")\n",
    "print(\"  - Early time steps have negligible influence\")\n",
    "print(\"\\nExploding Gradient ($w_{{hh}} > 1$):\")\n",
    "print(\"  - Gradient becomes exponentially large\")\n",
    "print(\"  - Causes numerical instability\")\n",
    "print(\"  - Can be fixed with gradient clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 RNN Architectures\n",
    "\n",
    "## 6.2.1 Long Short-Term Memory (LSTM)\n",
    "\n",
    "To overcome the problem of vanishing gradient that prevents the network from using long-term memory, we need to modify the memory component so that it does not only contain information about the past, but also has control over how and how much to use the information.\n",
    "\n",
    "In the simple RNN, the memory component has two inputs - $h_{t-1}, x_t$, and with their help we compute the hidden state using $f_w(h_{t-1}, x_t)$. In fact, the memory component is fixed and learning is performed only in the weights.\n",
    "\n",
    "In **LSTM**, there are several important components - in addition to the regular inputs, there is another input called **cell state memory** and denoted by $c_{t-1}$, and in addition $h_t$ is calculated in a more complex way. In this way, the element $c_t$ takes care of long-term memory of things, and $h_t$ is responsible for short-term memory.\n",
    "\n",
    "### LSTM Architecture\n",
    "\n",
    "The pair $[x_t, h_{t-1}]$ enters the cell and is multiplied by weight $w$, and then passes separately through four gates (note that we do not perform operations between $x_t$ and $h_{t-1}$, but they remain separate and all operations are done on each element separately).\n",
    "\n",
    "The first gate $f_t = [\\sigma(x_t), \\sigma(h_{t-1})]$ is the **forget gate** and is responsible for erasing part of the memory. If, for example, there is a new topic, then this gate should erase the topic that was stored in memory.\n",
    "\n",
    "The second gate $i_t$ is the **input gate** (memory gate) and is responsible for how much to remember the new information for the long term. For example, if there is indeed a new topic in a certain sentence, then the gate will decide to remember this information.\n",
    "\n",
    "The fourth gate $o_t$ is the **output gate** and is responsible for how much of the information is relevant to the current data $x_t$. The output of the cell is calculated as a function of the cell state given past information.\n",
    "\n",
    "These three gates are called **masks**, and they receive values between 0 and 1 since they pass through sigmoid.\n",
    "\n",
    "There is another gate $\\tilde{c}_t$ (also called $g$ - the candidate for memory) that prepares the information to enter the next cell. This gate weighs the information together with $i_t$. The new information is written to memory only if it is relevant.\n",
    "\n",
    "In this way, we get both $h_t$ which is responsible for short-term memory like in Vanilla RNN, and $c_t$ which is responsible for memory of all the past.\n",
    "\n",
    "### LSTM Equations\n",
    "\n",
    "Formally, we can formulate the operations as follows:\n",
    "\n",
    "$$\\begin{pmatrix} i \\\\ f \\\\ o \\\\ \\tilde{c} \\end{pmatrix} = \\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{pmatrix} W \\begin{pmatrix} h_{t-1} \\\\ x_t \\end{pmatrix} = \\begin{pmatrix} \\sigma(w_i \\cdot [x_t, h_{t-1}] + b_i) \\\\ \\sigma(w_f \\cdot [x_t, h_{t-1}] + b_f) \\\\ \\sigma(w_o \\cdot [x_t, h_{t-1}] + b_o) \\\\ \\tanh(w_{\\tilde{c}} \\cdot [x_t, h_{t-1}] + b_{\\tilde{c}}) \\end{pmatrix}$$\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "where the operator $\\odot$ symbolizes element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Implementation using PyTorch\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM cell implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input gate\n",
    "        self.W_ii = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Forget gate\n",
    "        self.W_if = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hf = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_f = nn.Parameter(torch.ones(hidden_size))  # Initialize to 1 to remember\n",
    "        \n",
    "        # Output gate\n",
    "        self.W_io = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_ho = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Cell gate\n",
    "        self.W_ig = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hg = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM cell.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (batch, input_size)\n",
    "            Input at time t\n",
    "        hidden : tuple (h, c)\n",
    "            h: hidden state, shape (batch, hidden_size)\n",
    "            c: cell state, shape (batch, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_new : tensor\n",
    "            New hidden state\n",
    "        c_new : tensor\n",
    "            New cell state\n",
    "        \"\"\"\n",
    "        h_prev, c_prev = hidden\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = torch.sigmoid(self.W_ii(x) + self.W_hi(h_prev) + self.b_i)\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = torch.sigmoid(self.W_if(x) + self.W_hf(h_prev) + self.b_f)\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = torch.sigmoid(self.W_io(x) + self.W_ho(h_prev) + self.b_o)\n",
    "        \n",
    "        # Cell gate (candidate)\n",
    "        g_t = torch.tanh(self.W_ig(x) + self.W_hg(h_prev) + self.b_g)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_new = f_t * c_prev + i_t * g_t\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_new = o_t * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, c_new\n",
    "\n",
    "# Test LSTM\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "lstm = LSTM(input_size, hidden_size)\n",
    "\n",
    "# Create a sequence\n",
    "seq_length = 8\n",
    "batch_size = 2\n",
    "x_seq = torch.randn(seq_length, batch_size, input_size)\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "h = torch.zeros(batch_size, hidden_size)\n",
    "c = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "print(\"LSTM Forward Pass:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input sequence length: {seq_length}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "# Process sequence\n",
    "for t in range(seq_length):\n",
    "    h, c = lstm(x_seq[t], (h, c))\n",
    "    print(f\"Step {t+1}: h.shape={h.shape}, c.shape={c.shape}\")\n",
    "\n",
    "print(f\"\\nFinal hidden state shape: {h.shape}\")\n",
    "print(f\"Final cell state shape: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM Cell Architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(-1, 10)\n",
    "ax.set_ylim(-1, 8)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('LSTM Cell Architecture', fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "# Input\n",
    "ax.text(0, 6, '$x_t$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax.text(0, 4, '$h_{t-1}$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Concatenate\n",
    "ax.text(2, 5, 'Concat', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.arrow(0.5, 6, 0.8, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(0.5, 4, 0.8, 0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Gates\n",
    "gate_y = [6.5, 5, 3.5, 2]\n",
    "gate_names = ['$i_t$ (Input)', '$f_t$ (Forget)', '$o_t$ (Output)', '$\\\\tilde{c}_t$ (Candidate)']\n",
    "gate_colors = ['lightcoral', 'lightpink', 'lightcyan', 'wheat']\n",
    "\n",
    "for i, (y, name, color) in enumerate(zip(gate_y, gate_names, gate_colors)):\n",
    "    ax.text(4.5, y, name, fontsize=10, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor=color, edgecolor='black'))\n",
    "    ax.arrow(2.5, 5, 1.5, y-5, head_width=0.15, head_length=0.1, fc='black', ec='black', alpha=0.6)\n",
    "\n",
    "# Cell state\n",
    "ax.text(7, 6, '$c_{t-1}$', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Operations\n",
    "ax.text(7, 4, '$\\\\times$', fontsize=16, ha='center', va='center')\n",
    "ax.text(7, 2.5, '$+$', fontsize=16, ha='center', va='center')\n",
    "ax.text(7, 1, '$\\\\times$', fontsize=16, ha='center', va='center')\n",
    "\n",
    "# New cell state\n",
    "ax.text(9, 3.5, '$c_t$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "\n",
    "# New hidden state\n",
    "ax.text(9, 1, '$h_t$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrows\n",
    "ax.arrow(5.2, 6.5, 1.3, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.2, 5, 1.3, -1, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.2, 3.5, 1.3, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.2, 2, 1.3, 1.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "ax.arrow(7.5, 3.5, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 3.5, 0, -1, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 1, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# tanh\n",
    "ax.text(8.5, 2.5, 'tanh', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"LSTM Key Components:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Forget Gate ($f_t$): Decides what to forget from $c_{t-1}$\")\n",
    "print(\"2. Input Gate ($i_t$): Decides what new information to store\")\n",
    "print(\"3. Output Gate ($o_t$): Decides what parts of $c_t$ to output\")\n",
    "print(\"4. Cell State ($c_t$): Long-term memory\")\n",
    "print(\"5. Hidden State ($h_t$): Short-term memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Variants\n",
    "\n",
    "There are different variations of LSTM components - for example, we can connect $c_{t-1}$ not only to the output $h_t$ but also to the other gates. Such a connection is called **peepholes**, because it allows the gates to observe $c_{t-1}$ directly.\n",
    "\n",
    "There are architectures that connect $c_{t-1}$ to all gates, and there are architectures that connect it only to some gates. Connecting all gates to $c_{t-1}$ changes the gate equations from $\\sigma(w \\cdot [x_t, h_{t-1}] + b)$ to $\\sigma(w \\cdot [c_{t-1}, x_t, h_{t-1}] + b)$.\n",
    "\n",
    "Another variation of LSTM unifies the forget gate $f_t$ with the input gate $i_t$, so that the decision of how much to forget from memory is obtained together with the decision of how much new information to write. This change affects the memory cell, where instead of $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$, the update equation becomes $c_t = f_t \\odot c_{t-1} + (1 - f_t) \\odot \\tilde{c}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Gated Recurrent Units (GRU)\n",
    "\n",
    "There is another architecture of memory cell called **Gated Recurrent Units (GRU)**, which has fewer components than LSTM.\n",
    "\n",
    "### GRU Architecture\n",
    "\n",
    "The significant change from LSTM is the fact that there is no **cell state memory**, and all gates are based only on the input and output of the previous cell. To enable memory both for the long term and the short term, there are two gates - **Reset gate** and **Update gate**, and they are computed as follows:\n",
    "\n",
    "**Update gate**: $z_t = \\sigma(w_z \\cdot [x_t, h_{t-1}])$\n",
    "\n",
    "**Reset gate**: $r_t = \\sigma(w_r \\cdot [x_t, h_{t-1}])$\n",
    "\n",
    "Using the reset gate, we compute the **Candidate hidden state**:\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(w \\cdot [x_t, r_t \\odot h_{t-1}])$$\n",
    "\n",
    "First, note that $r_t \\in [0, 1]$ since it is the result of sigmoid. Now, if we compare $\\tilde{h}_t$ to the simple hidden state of Vanilla RNN: $h_t = f_w(w_{hh}h_{t-1} + w_{xh}x_t)$. If $r_t$ is close to 1:\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(w \\cdot [x_t, r_t \\odot h_{t-1}] \\approx \\tanh(w[x_t, h_{t-1}]) = \\tanh(w_{hx}x_t + w_{hh}h_{t-1}])$$\n",
    "\n",
    "The meaning is that if $r_t \\to 1$, we get the classic memory component. If $r_t \\to 0$, then $\\tilde{h}_t \\approx \\tanh(w \\cdot [x_t, 0 \\odot h_{t-1}] = \\tanh(w_{xh}x_t)$, and in fact the short-term memory is reset.\n",
    "\n",
    "To combine $\\tilde{h}_t$ with the previous hidden state, we use $z_t$, which also receives values between 0 and 1:\n",
    "\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "If $z_t \\to 0$, then $h_t \\approx h_{t-1}$, meaning we do not consider $\\tilde{h}_t$, and in fact we pass the previous state as is. If, on the other hand, $z_t \\to 1$, then $h_t \\approx \\tilde{h}_t$, meaning we ignore the previous state as is and take the Candidate hidden state, which is the weighting of the previous hidden state with the current input element. For any other value of $z_t$, we get a combination of the previous hidden and the Candidate hidden state.\n",
    "\n",
    "This architecture solves the gradient problem - the gradient is close to 1 and does not explode, and since we pass the state through time, the gradient is close to 1 all the time and does not vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Implementation using PyTorch\n",
    "class GRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU cell implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Update gate\n",
    "        self.W_iz = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hz = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Reset gate\n",
    "        self.W_ir = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hr = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.W_in = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hn = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.b_n = nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        Forward pass through GRU cell.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (batch, input_size)\n",
    "            Input at time t\n",
    "        h_prev : tensor, shape (batch, hidden_size)\n",
    "            Previous hidden state\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_new : tensor\n",
    "            New hidden state\n",
    "        \"\"\"\n",
    "        # Update gate\n",
    "        z_t = torch.sigmoid(self.W_iz(x) + self.W_hz(h_prev) + self.b_z)\n",
    "        \n",
    "        # Reset gate\n",
    "        r_t = torch.sigmoid(self.W_ir(x) + self.W_hr(h_prev) + self.b_r)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        n_t = torch.tanh(self.W_in(x) + r_t * (self.W_hn(h_prev) + self.b_n))\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_new = (1 - z_t) * h_prev + z_t * n_t\n",
    "        \n",
    "        return h_new\n",
    "\n",
    "# Test GRU\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "gru = GRU(input_size, hidden_size)\n",
    "\n",
    "# Create a sequence\n",
    "seq_length = 8\n",
    "batch_size = 2\n",
    "x_seq = torch.randn(seq_length, batch_size, input_size)\n",
    "\n",
    "# Initialize hidden state\n",
    "h = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "print(\"GRU Forward Pass:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input sequence length: {seq_length}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "# Process sequence\n",
    "for t in range(seq_length):\n",
    "    h = gru(x_seq[t], h)\n",
    "    print(f\"Step {t+1}: h.shape={h.shape}\")\n",
    "\n",
    "print(f\"\\nFinal hidden state shape: {h.shape}\")\n",
    "\n",
    "# Compare parameters\n",
    "lstm_params = sum(p.numel() for p in LSTM(input_size, hidden_size).parameters())\n",
    "gru_params = sum(p.numel() for p in GRU(input_size, hidden_size).parameters())\n",
    "\n",
    "print(f\"\\nParameter Comparison:\")\n",
    "print(f\"  LSTM parameters: {lstm_params}\")\n",
    "print(f\"  GRU parameters: {gru_params}\")\n",
    "print(f\"  GRU has {lstm_params - gru_params} fewer parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GRU Cell Architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(-1, 10)\n",
    "ax.set_ylim(-1, 8)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('GRU Cell Architecture', fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "# Input\n",
    "ax.text(0, 6, '$x_t$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax.text(0, 4, '$h_{t-1}$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Concatenate\n",
    "ax.text(2, 5, 'Concat', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.arrow(0.5, 6, 0.8, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(0.5, 4, 0.8, 0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Gates\n",
    "ax.text(4.5, 6.5, '$z_t$ (Update)', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black'))\n",
    "ax.text(4.5, 5, '$r_t$ (Reset)', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightpink', edgecolor='black'))\n",
    "\n",
    "ax.arrow(2.5, 5, 1.5, 1.5, head_width=0.15, head_length=0.1, fc='black', ec='black', alpha=0.6)\n",
    "ax.arrow(2.5, 5, 1.5, 0, head_width=0.15, head_length=0.1, fc='black', ec='black', alpha=0.6)\n",
    "\n",
    "# Candidate\n",
    "ax.text(4.5, 3, '$\\\\tilde{h}_t$', fontsize=12, weight='bold', ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', edgecolor='black'))\n",
    "ax.arrow(2.5, 5, 1.5, -2, head_width=0.15, head_length=0.1, fc='black', ec='black', alpha=0.6)\n",
    "\n",
    "# Operations\n",
    "ax.text(7, 6.5, '$\\\\times$', fontsize=16, ha='center', va='center')\n",
    "ax.text(7, 4.5, '$1-z_t$', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.text(7, 3, '$z_t$', fontsize=10, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.text(7, 1.5, '$+$', fontsize=16, ha='center', va='center')\n",
    "\n",
    "# New hidden state\n",
    "ax.text(9, 3.5, '$h_t$', fontsize=14, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrows\n",
    "ax.arrow(5.2, 6.5, 1.3, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.2, 5, 1.3, -0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(5.2, 3, 1.3, 0.5, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 3.5, 1, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Reset gate connection\n",
    "ax.plot([4.5, 4.5, 6.5], [5, 2.5, 2.5], 'r--', linewidth=1.5, alpha=0.5)\n",
    "ax.text(5.5, 3.5, '$r_t$', fontsize=9, color='red', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"GRU Key Components:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Reset Gate ($r_t$): Controls how much past information to forget\")\n",
    "print(\"2. Update Gate ($z_t$): Controls how much to update with new information\")\n",
    "print(\"3. Candidate Hidden State ($\\\\tilde{h}_t$): New information candidate\")\n",
    "print(\"4. Hidden State ($h_t$): Combination of old and new information\")\n",
    "print(\"\\nAdvantages over LSTM:\")\n",
    "print(\"  - Simpler architecture (no cell state)\")\n",
    "print(\"  - Fewer parameters\")\n",
    "print(\"  - Often performs similarly to LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 Deep RNN\n",
    "\n",
    "So far we have discussed single memory components, which can be chained together to get a layer that can analyze temporal data. We can also stack several layers to get a deep network.\n",
    "\n",
    "### Deep RNN Architecture\n",
    "\n",
    "Let us describe the network formally. At each time point $t$, there is an input vector $x_t \\in \\mathbb{R}^{n \\times d}$ (a vector with $n$ elements, where each element is of dimension $d$). The sequence elements enter a network with $L$ layers and $T$ elements in the input, where at each time point $t$ there are $L$ layers (and hidden states) - for each layer $\\ell$ at time point $t$, we denote $H_t^{(\\ell)} \\in \\mathbb{R}^{n \\times h}$.\n",
    "\n",
    "At each time point there is also an output vector of length $n$ - $o_t \\in \\mathbb{R}^{n \\times o}$. We denote: $H_t^{(0)} = x_t$, and we assume that between one layer and another there are no direct connections, only through function $\\phi$. Using these notations, we can write:\n",
    "\n",
    "$$H_t^{(\\ell)} = \\phi_\\ell(H_t^{(\\ell-1)}w_{xh}^{(\\ell)} + H_{t-1}^{(\\ell)}w_{hh}^{(\\ell)} + b_h^{(\\ell)})$$\n",
    "\n",
    "where $w_{xh}^{(\\ell)} \\in \\mathbb{R}^{h \\times h}, w_{hh}^{(\\ell)} \\in \\mathbb{R}^{h \\times h}, b_h^{(\\ell)} \\in \\mathbb{R}^{1 \\times h}$ are the parameters of the hidden layer $\\ell$.\n",
    "\n",
    "The output $o_t$ depends directly only on layer $L$, and is calculated by:\n",
    "\n",
    "$$o_t = H_t^{(L)}w_{hy}^{(L)} + b_y^{(L)}$$\n",
    "\n",
    "where $w_{hy}^{(L)} \\in \\mathbb{R}^{h \\times o}, b_y^{(L)} \\in \\mathbb{R}^{1 \\times o}$ are the parameters of the output layer.\n",
    "\n",
    "Of course, we can use hidden states in GRU or LSTM memory components, and thus get a Deep Gated RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep RNN Implementation using PyTorch\n",
    "class DeepRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep RNN with multiple stacked layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, cell_type='LSTM'):\n",
    "        super(DeepRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "        # Create stacked RNN layers\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through deep RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (seq_len, batch, input_size)\n",
    "            Input sequence\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : tensor, shape (seq_len, batch, output_size)\n",
    "            Output sequence\n",
    "        \"\"\"\n",
    "        # RNN forward pass\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        \n",
    "        # Apply output layer to each time step\n",
    "        output = self.fc(rnn_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test Deep RNN\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 3\n",
    "output_size = 5\n",
    "seq_length = 15\n",
    "batch_size = 4\n",
    "\n",
    "# Create model\n",
    "deep_lstm = DeepRNN(input_size, hidden_size, num_layers, output_size, cell_type='LSTM')\n",
    "deep_gru = DeepRNN(input_size, hidden_size, num_layers, output_size, cell_type='GRU')\n",
    "\n",
    "# Create input sequence\n",
    "x = torch.randn(seq_length, batch_size, input_size)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output_lstm = deep_lstm(x)\n",
    "    output_gru = deep_gru(x)\n",
    "\n",
    "print(\"Deep RNN Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"LSTM output shape: {output_lstm.shape}\")\n",
    "print(f\"GRU output shape: {output_gru.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "lstm_params = sum(p.numel() for p in deep_lstm.parameters())\n",
    "gru_params = sum(p.numel() for p in deep_gru.parameters())\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Deep LSTM: {lstm_params:,}\")\n",
    "print(f\"  Deep GRU: {gru_params:,}\")\n",
    "\n",
    "# Visualize architecture\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(-0.5, 6)\n",
    "ax.set_ylim(-0.5, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Deep RNN Architecture (3 Layers)', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "for i in range(3):\n",
    "    ax.text(0, 3-i*0.8, f'$x_t$', fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black'))\n",
    "\n",
    "# Layers\n",
    "for layer in range(3):\n",
    "    y_pos = 2.5 - layer * 0.8\n",
    "    ax.text(2, y_pos, f'Layer {layer+1}', fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black'))\n",
    "    ax.text(4, y_pos, f'$H_t^{{({layer+1})}}$', fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "\n",
    "# Output\n",
    "ax.text(5.5, 1.5, '$o_t$', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrows\n",
    "for i in range(3):\n",
    "    ax.arrow(0.5, 3-i*0.8, 1, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    ax.arrow(2.5, 2.5-i*0.8, 1, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    if i == 2:\n",
    "        ax.arrow(4.5, 1.5, 0.8, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Recurrent connections\n",
    "for layer in range(3):\n",
    "    y_pos = 2.5 - layer * 0.8\n",
    "    ax.plot([2, 2, 1.5, 1.5, 2], [y_pos, y_pos+0.3, y_pos+0.3, y_pos, y_pos], \n",
    "            'r--', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional RNN Implementation using PyTorch\n",
    "class BidirectionalRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional RNN implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, cell_type='LSTM'):\n",
    "        super(BidirectionalRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "        # Bidirectional RNN\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, \n",
    "                              batch_first=False, bidirectional=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers=1,\n",
    "                             batch_first=False, bidirectional=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers=1,\n",
    "                             batch_first=False, bidirectional=True)\n",
    "        \n",
    "        # Output layer (input is 2*hidden_size because of bidirectional)\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through bidirectional RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : tensor, shape (seq_len, batch, input_size)\n",
    "            Input sequence\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : tensor, shape (seq_len, batch, output_size)\n",
    "            Output sequence\n",
    "        \"\"\"\n",
    "        # Bidirectional RNN forward pass\n",
    "        rnn_out, _ = self.rnn(x)  # Shape: (seq_len, batch, 2*hidden_size)\n",
    "        \n",
    "        # Apply output layer\n",
    "        output = self.fc(rnn_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test Bidirectional RNN\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "seq_length = 8\n",
    "batch_size = 3\n",
    "\n",
    "# Create models\n",
    "bi_lstm = BidirectionalRNN(input_size, hidden_size, output_size, cell_type='LSTM')\n",
    "bi_gru = BidirectionalRNN(input_size, hidden_size, output_size, cell_type='GRU')\n",
    "\n",
    "# Create input sequence\n",
    "x = torch.randn(seq_length, batch_size, input_size)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output_lstm = bi_lstm(x)\n",
    "    output_gru = bi_gru(x)\n",
    "\n",
    "print(\"Bidirectional RNN Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size (per direction): {hidden_size}\")\n",
    "print(f\"Total hidden size: {2 * hidden_size} (forward + backward)\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Bidirectional LSTM output shape: {output_lstm.shape}\")\n",
    "print(f\"Bidirectional GRU output shape: {output_gru.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "lstm_params = sum(p.numel() for p in bi_lstm.parameters())\n",
    "gru_params = sum(p.numel() for p in bi_gru.parameters())\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Bidirectional LSTM: {lstm_params:,}\")\n",
    "print(f\"  Bidirectional GRU: {gru_params:,}\")\n",
    "\n",
    "# Visualize architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(-0.5, 7)\n",
    "ax.set_ylim(-0.5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Bidirectional RNN Architecture', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "# Input sequence\n",
    "for i in range(3):\n",
    "    ax.text(0, 4-i*1.2, f'$x_{i+1}$', fontsize=11, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Forward RNN\n",
    "ax.text(2, 3.5, '$\\\\overrightarrow{H}_t$', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "ax.arrow(0.5, 4, 1, -0.5, head_width=0.15, head_length=0.1, fc='green', ec='green')\n",
    "ax.arrow(0.5, 2.8, 1, 0.5, head_width=0.15, head_length=0.1, fc='green', ec='green')\n",
    "ax.arrow(0.5, 1.6, 1, 0.5, head_width=0.15, head_length=0.1, fc='green', ec='green')\n",
    "\n",
    "# Backward RNN\n",
    "ax.text(4, 3.5, '$\\\\overleftarrow{H}_t$', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "ax.arrow(5.5, 1.6, -1, 0.5, head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "ax.arrow(5.5, 2.8, -1, 0.5, head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "ax.arrow(5.5, 4, -1, -0.5, head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "\n",
    "# Concatenate\n",
    "ax.text(3, 1.5, 'Concat', fontsize=11, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.arrow(2.3, 3.5, 0.4, -1.8, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(4.3, 3.5, -0.4, -1.8, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Output\n",
    "ax.text(3, 0.5, '$o_t$', fontsize=12, weight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', edgecolor='black', linewidth=2))\n",
    "ax.arrow(3, 1.3, 0, -0.6, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Labels\n",
    "ax.text(2, 4.5, 'Forward', fontsize=10, ha='center', color='green', weight='bold')\n",
    "ax.text(4, 4.5, 'Backward', fontsize=10, ha='center', color='red', weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBidirectional RNN Benefits:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"  - Can use information from both past and future\")\n",
    "print(\"  - Better for tasks like:\")\n",
    "print(\"    * Machine translation\")\n",
    "print(\"    * Named entity recognition\")\n",
    "print(\"    * Sentiment analysis\")\n",
    "print(\"    * Any task where context matters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
